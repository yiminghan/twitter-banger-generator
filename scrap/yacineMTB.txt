just a cyberpunk in a corpo world
------
can't believe elon musk solved the bot problem. in retrospect it was so easy. it's just cost prohibitive to create a 100k sized bot farm if you had to pay 8$ for each one

not only that, but credit card systems have the most sophisticated ID/fraud systems on the planet
------
like it was really that simple
he actually just solved the turing test bot apocalypse 
with a fkn blue check
lmao
------
you need to be meeting maxxing. you should be joining every meeting ever. every single idea that flies by, you should be exposed to. it takes too much time? simply work faster
------
guys this is actually really good career advice. i'm not joking
------
if you want to become a great conversion hacker, you need to rise above, and watch yourself get conversion hacked by others
------
monkey see, monkey do
------
"bro how do you stay motivated all the time"
ANIME
OSTs
------
and go go juice
------
boss said i need to be in office today
------
"chatGPT hype cycle"
mf this thing is literally CODING for me
it literally mutates json and creates charts with ENGLISH LANGUAGE
DO YOU NOT UNDERSTAND> IT IS LITERALYL DOING A LARGE PORTION OF WHAT MY JOPB USED TO BE. IT LITERALLY FKN CODES FOR ME DUDE. BRO
------
The ChatGPT hype cycle:
- Stage 1: "GPT-X is out-of-the-box magic!"
- Stage 2: "We need to use our data" (where we are now)
- Stage 3: "We need to develop our data"

From 2 -> 3, enterprises will realize not enough to just dump in a data lake... use case-specific dev is key!
------
the only way to deal with an extreme shortage of time is to simply do more with less time
------
"how are you going to have kids and continue growing your career/business?"
"how are you going to take care of all that administrative work"
"how are you going to find the time to do cardio/work out?"

i will
------
New work: Bayesian Flow Networks!
https://arxiv.org/abs/2308.07037
We present a new perspective on the ideas related to diffusion models. BFNs combine Bayesian inference and neural nets to yield a model class with simple objectives that gracefully extends to discrete data. Overview 
------
inflection CEO watching Geo Hotz & Yud's debate
------
Is e/acc moon god or sun god?
------
I know more about what is happening in SF than what is happening in Canada
------
Introducing FastViT, fast, super-small, general purposes vision transformers running at ~ 1ms on mobile. Code and pre-trained models are live. 
https://github.com/apple/ml-fastvit…
#ICCV2023
------
"How is LLaMa.cpp possible?" 
great post by 
@finbarrtimbers
 
https://finbarr.ca/how-is-llama-cpp-possible/…

llama.cpp surprised many people (myself included) with how quickly you can run large LLMs on small computers, e.g. 7B runs @ ~16 tok/s on a MacBook. Wait don't you need supercomputers to work… Show more
------
i am so tired
------
it's actually going to happen 
my last 8 minutes... I will go eat some night night snacks
------
just a cyberpunk in a corpo world
------
can't believe elon musk solved the bot problem. in retrospect it was so easy. it's just cost prohibitive to create a 100k sized bot farm if you had to pay 8$ for each one

not only that, but credit card systems have the most sophisticated ID/fraud systems on the planet
------
like it was really that simple
he actually just solved the turing test bot apocalypse 
with a fkn blue check
lmao
------
you need to be meeting maxxing. you should be joining every meeting ever. every single idea that flies by, you should be exposed to. it takes too much time? simply work faster
------
guys this is actually really good career advice. i'm not joking
------
if you want to become a great conversion hacker, you need to rise above, and watch yourself get conversion hacked by others
------
monkey see, monkey do
------
"bro how do you stay motivated all the time"
ANIME
OSTs
------
and go go juice
------
boss said i need to be in office today
------
"chatGPT hype cycle"
mf this thing is literally CODING for me
it literally mutates json and creates charts with ENGLISH LANGUAGE
DO YOU NOT UNDERSTAND> IT IS LITERALYL DOING A LARGE PORTION OF WHAT MY JOPB USED TO BE. IT LITERALLY FKN CODES FOR ME DUDE. BRO
------
The ChatGPT hype cycle:
- Stage 1: "GPT-X is out-of-the-box magic!"
- Stage 2: "We need to use our data" (where we are now)
- Stage 3: "We need to develop our data"

From 2 -> 3, enterprises will realize not enough to just dump in a data lake... use case-specific dev is key!
------
the only way to deal with an extreme shortage of time is to simply do more with less time
------
"how are you going to have kids and continue growing your career/business?"
"how are you going to take care of all that administrative work"
"how are you going to find the time to do cardio/work out?"

i will
------
New work: Bayesian Flow Networks!
https://arxiv.org/abs/2308.07037
We present a new perspective on the ideas related to diffusion models. BFNs combine Bayesian inference and neural nets to yield a model class with simple objectives that gracefully extends to discrete data. Overview 
------
inflection CEO watching Geo Hotz & Yud's debate
------
Is e/acc moon god or sun god?
------
I know more about what is happening in SF than what is happening in Canada
------
Introducing FastViT, fast, super-small, general purposes vision transformers running at ~ 1ms on mobile. Code and pre-trained models are live. 
https://github.com/apple/ml-fastvit…
#ICCV2023
------
"How is LLaMa.cpp possible?" 
great post by 
@finbarrtimbers
 
https://finbarr.ca/how-is-llama-cpp-possible/…

llama.cpp surprised many people (myself included) with how quickly you can run large LLMs on small computers, e.g. 7B runs @ ~16 tok/s on a MacBook. Wait don't you need supercomputers to work… Show more
------
i am so tired
------
it's actually going to happen 
my last 8 minutes... I will go eat some night night snacks
------
everyone has that one Jewish pothead friend who is actually like a materials scientist PhD that gets sketched out every time you ask him about the specifics of his work
------
Wdym you’re 27 and single?? No one told you that you were supposed to meet your soulmate in college??
------
4chan vs. reddit
------
 New research from Meta AI — Shepherd is a language model specifically tuned to critique model responses & suggest refinements. It goes beyond the capabilities of untuned models to identify diverse errors & suggest improvements.

Read the paper  https://bit.ly/3QC4AW3
------
it's actually crazy how much people are talking about Geohotz/yud. At this rate, we'll be able to read about it on the NYT after waiting 5 seconds for the link to resolve
------
ML News
What happened in ML in the last two months?
- LLaMA2 released under commercial, but still controversial license
- Google, OpenAI form counsel for AI safety
- LLMs empowering Robots
- Models for Geo, Medical, Chemistry
...and more
Watch: https://youtu.be/xs-0cp1hSnY
------
Now that linguistics is part of STEM, should STEM be called STLEM or STELM?
------
what if Eliezer Yudkowsky and Geohotz planned the whole debate ahead of time, just like me and Roko did?
------
yeah i'm not risking it
------
it feels really offensive and i have no idea why
------
terrans vs cosmists. a tale as old as time
------
can't believe elon musk solved the bot problem. in retrospect it was so easy. it's just cost prohibitive to create a 100k sized bot farm if you had to pay 8$ for each one

not only that, but credit card systems have the most sophisticated ID/fraud systems on the planet
------
like it was really that simple
he actually just solved the turing test bot apocalypse 
with a fkn blue check
lmao
------
you need to be meeting maxxing. you should be joining every meeting ever. every single idea that flies by, you should be exposed to. it takes too much time? simply work faster
------
guys this is actually really good career advice. i'm not joking
------
if you want to become a great conversion hacker, you need to rise above, and watch yourself get conversion hacked by others
------
monkey see, monkey do
------
"bro how do you stay motivated all the time"
ANIME
OSTs
------
and go go juice
------
boss said i need to be in office today
------
"chatGPT hype cycle"
mf this thing is literally CODING for me
it literally mutates json and creates charts with ENGLISH LANGUAGE
DO YOU NOT UNDERSTAND> IT IS LITERALYL DOING A LARGE PORTION OF WHAT MY JOPB USED TO BE. IT LITERALLY FKN CODES FOR ME DUDE. BRO
------
The ChatGPT hype cycle:
- Stage 1: "GPT-X is out-of-the-box magic!"
- Stage 2: "We need to use our data" (where we are now)
- Stage 3: "We need to develop our data"

From 2 -> 3, enterprises will realize not enough to just dump in a data lake... use case-specific dev is key!
------
the only way to deal with an extreme shortage of time is to simply do more with less time
------
"how are you going to have kids and continue growing your career/business?"
"how are you going to take care of all that administrative work"
"how are you going to find the time to do cardio/work out?"

i will
------
New work: Bayesian Flow Networks!
https://arxiv.org/abs/2308.07037
We present a new perspective on the ideas related to diffusion models. BFNs combine Bayesian inference and neural nets to yield a model class with simple objectives that gracefully extends to discrete data. Overview 
------
inflection CEO watching Geo Hotz & Yud's debate
------
Is e/acc moon god or sun god?
------
I know more about what is happening in SF than what is happening in Canada
------
Introducing FastViT, fast, super-small, general purposes vision transformers running at ~ 1ms on mobile. Code and pre-trained models are live. 
https://github.com/apple/ml-fastvit…
#ICCV2023
------
"How is LLaMa.cpp possible?" 
great post by 
@finbarrtimbers
 
https://finbarr.ca/how-is-llama-cpp-possible/…

llama.cpp surprised many people (myself included) with how quickly you can run large LLMs on small computers, e.g. 7B runs @ ~16 tok/s on a MacBook. Wait don't you need supercomputers to work… Show more
------
i am so tired
------
it's actually going to happen 
my last 8 minutes... I will go eat some night night snacks
------
everyone has that one Jewish pothead friend who is actually like a materials scientist PhD that gets sketched out every time you ask him about the specifics of his work
------
Wdym you’re 27 and single?? No one told you that you were supposed to meet your soulmate in college??
------
4chan vs. reddit
------
 New research from Meta AI — Shepherd is a language model specifically tuned to critique model responses & suggest refinements. It goes beyond the capabilities of untuned models to identify diverse errors & suggest improvements.

Read the paper  https://bit.ly/3QC4AW3
------
it's actually crazy how much people are talking about Geohotz/yud. At this rate, we'll be able to read about it on the NYT after waiting 5 seconds for the link to resolve
------
ML News
What happened in ML in the last two months?
- LLaMA2 released under commercial, but still controversial license
- Google, OpenAI form counsel for AI safety
- LLMs empowering Robots
- Models for Geo, Medical, Chemistry
...and more
Watch: https://youtu.be/xs-0cp1hSnY
------
Now that linguistics is part of STEM, should STEM be called STLEM or STELM?
------
what if Eliezer Yudkowsky and Geohotz planned the whole debate ahead of time, just like me and Roko did?
------
yeah i'm not risking it
------
it feels really offensive and i have no idea why
------
terrans vs cosmists. a tale as old as time
------
amdahl's law shout out. my work is done
------
who won the debate?
------
oh it was pst not est
------
hahahahahahahaha YES 
LET'S GO 
------
Some A+ grade pettiness from our fearless leader
------
"news"
------
words have power. they can be unraveled by mystical information constructs that we don't yet understand, into useful, correct information
------
I don't need more agent tooling, I need more models that can actually perform agent like tasks. The biggest gains will be made from models that are better through pretraining or further fine tuning & reward modeling
------
boss said i need to be in office today
------
"chatGPT hype cycle"
mf this thing is literally CODING for me
it literally mutates json and creates charts with ENGLISH LANGUAGE
DO YOU NOT UNDERSTAND> IT IS LITERALYL DOING A LARGE PORTION OF WHAT MY JOPB USED TO BE. IT LITERALLY FKN CODES FOR ME DUDE. BRO
------
The ChatGPT hype cycle:
- Stage 1: "GPT-X is out-of-the-box magic!"
- Stage 2: "We need to use our data" (where we are now)
- Stage 3: "We need to develop our data"

From 2 -> 3, enterprises will realize not enough to just dump in a data lake... use case-specific dev is key!
------
the only way to deal with an extreme shortage of time is to simply do more with less time
------
"how are you going to have kids and continue growing your career/business?"
"how are you going to take care of all that administrative work"
"how are you going to find the time to do cardio/work out?"

i will
------
New work: Bayesian Flow Networks!
https://arxiv.org/abs/2308.07037
We present a new perspective on the ideas related to diffusion models. BFNs combine Bayesian inference and neural nets to yield a model class with simple objectives that gracefully extends to discrete data. Overview 
------
inflection CEO watching Geo Hotz & Yud's debate
------
Is e/acc moon god or sun god?
------
I know more about what is happening in SF than what is happening in Canada
------
Introducing FastViT, fast, super-small, general purposes vision transformers running at ~ 1ms on mobile. Code and pre-trained models are live. 
https://github.com/apple/ml-fastvit…
#ICCV2023
------
"How is LLaMa.cpp possible?" 
great post by 
@finbarrtimbers
 
https://finbarr.ca/how-is-llama-cpp-possible/…

llama.cpp surprised many people (myself included) with how quickly you can run large LLMs on small computers, e.g. 7B runs @ ~16 tok/s on a MacBook. Wait don't you need supercomputers to work… Show more
------
i am so tired
------
it's actually going to happen 
my last 8 minutes... I will go eat some night night snacks
------
everyone has that one Jewish pothead friend who is actually like a materials scientist PhD that gets sketched out every time you ask him about the specifics of his work
------
Wdym you’re 27 and single?? No one told you that you were supposed to meet your soulmate in college??
------
4chan vs. reddit
------
 New research from Meta AI — Shepherd is a language model specifically tuned to critique model responses & suggest refinements. It goes beyond the capabilities of untuned models to identify diverse errors & suggest improvements.

Read the paper  https://bit.ly/3QC4AW3
------
it's actually crazy how much people are talking about Geohotz/yud. At this rate, we'll be able to read about it on the NYT after waiting 5 seconds for the link to resolve
------
ML News
What happened in ML in the last two months?
- LLaMA2 released under commercial, but still controversial license
- Google, OpenAI form counsel for AI safety
- LLMs empowering Robots
- Models for Geo, Medical, Chemistry
...and more
Watch: https://youtu.be/xs-0cp1hSnY
------
Now that linguistics is part of STEM, should STEM be called STLEM or STELM?
------
what if Eliezer Yudkowsky and Geohotz planned the whole debate ahead of time, just like me and Roko did?
------
yeah i'm not risking it
------
it feels really offensive and i have no idea why
------
terrans vs cosmists. a tale as old as time
------
amdahl's law shout out. my work is done
------
who won the debate?
------
oh it was pst not est
------
hahahahahahahaha YES 
LET'S GO 
------
Some A+ grade pettiness from our fearless leader
------
"news"
------
words have power. they can be unraveled by mystical information constructs that we don't yet understand, into useful, correct information
------
I don't need more agent tooling, I need more models that can actually perform agent like tasks. The biggest gains will be made from models that are better through pretraining or further fine tuning & reward modeling
------
After DIDACT, this is another proof that training on code changes during commits rather than full code leads to better outputs for code LLMs.

It will be much more helpful as well in this way honestly if you plan to use them as copilot rather than automation.
------
How to instruction tune Code LLMs w/o #GPT4 data? Releasing

OctoCoder & OctoGeeX: 46.2 on HumanEvalSoTAof commercial LLMs
CommitPack: 4TB of Git Commits
HumanEvalPack: HumanEval extended to 3 tasks & 6 lang

https://arxiv.org/abs/2308.07124
https://github.com/bigcode-project/octopack…
1/9
------
how are there so many smart people on this site? it's actually insane
------
@yacineMTB
 always talks about 
@nearcyan
 modularity. Where your application has a good framework around an easily-swappable LLM component that can be upgraded as the models improve.
------
@yacineMTB
 
@LatinxPutler
 thanks for mentioning prolog, just had a light bulb moment :)
------
It's either full Ted K or full e/acc.

There is no in-between. 

Which way will it be, anon?
------
"there's a saddle on the tiger. Fuck it. Let's go man"
------
follow 
@LatinxPutler
 
also, next time I will actually record. sometimes they are just insane rants, but sometimes really smart people come on and drop kino knowledge
------
https://twitter.com/i/spaces/1yoJMZyaVrwxQ…
------
inflection CEO watching Geo Hotz & Yud's debate
------
Is e/acc moon god or sun god?
------
I know more about what is happening in SF than what is happening in Canada
------
Introducing FastViT, fast, super-small, general purposes vision transformers running at ~ 1ms on mobile. Code and pre-trained models are live. 
https://github.com/apple/ml-fastvit…
#ICCV2023
------
"How is LLaMa.cpp possible?" 
great post by 
@finbarrtimbers
 
https://finbarr.ca/how-is-llama-cpp-possible/…

llama.cpp surprised many people (myself included) with how quickly you can run large LLMs on small computers, e.g. 7B runs @ ~16 tok/s on a MacBook. Wait don't you need supercomputers to work… Show more
------
i am so tired
------
it's actually going to happen 
my last 8 minutes... I will go eat some night night snacks
------
everyone has that one Jewish pothead friend who is actually like a materials scientist PhD that gets sketched out every time you ask him about the specifics of his work
------
Wdym you’re 27 and single?? No one told you that you were supposed to meet your soulmate in college??
------
4chan vs. reddit
------
 New research from Meta AI — Shepherd is a language model specifically tuned to critique model responses & suggest refinements. It goes beyond the capabilities of untuned models to identify diverse errors & suggest improvements.

Read the paper  https://bit.ly/3QC4AW3
------
it's actually crazy how much people are talking about Geohotz/yud. At this rate, we'll be able to read about it on the NYT after waiting 5 seconds for the link to resolve
------
ML News
What happened in ML in the last two months?
- LLaMA2 released under commercial, but still controversial license
- Google, OpenAI form counsel for AI safety
- LLMs empowering Robots
- Models for Geo, Medical, Chemistry
...and more
Watch: https://youtu.be/xs-0cp1hSnY
------
Now that linguistics is part of STEM, should STEM be called STLEM or STELM?
------
what if Eliezer Yudkowsky and Geohotz planned the whole debate ahead of time, just like me and Roko did?
------
yeah i'm not risking it
------
it feels really offensive and i have no idea why
------
terrans vs cosmists. a tale as old as time
------
amdahl's law shout out. my work is done
------
who won the debate?
------
oh it was pst not est
------
hahahahahahahaha YES 
LET'S GO 
------
Some A+ grade pettiness from our fearless leader
------
"news"
------
words have power. they can be unraveled by mystical information constructs that we don't yet understand, into useful, correct information
------
I don't need more agent tooling, I need more models that can actually perform agent like tasks. The biggest gains will be made from models that are better through pretraining or further fine tuning & reward modeling
------
After DIDACT, this is another proof that training on code changes during commits rather than full code leads to better outputs for code LLMs.

It will be much more helpful as well in this way honestly if you plan to use them as copilot rather than automation.
------
How to instruction tune Code LLMs w/o #GPT4 data? Releasing

OctoCoder & OctoGeeX: 46.2 on HumanEvalSoTAof commercial LLMs
CommitPack: 4TB of Git Commits
HumanEvalPack: HumanEval extended to 3 tasks & 6 lang

https://arxiv.org/abs/2308.07124
https://github.com/bigcode-project/octopack…
1/9
------
how are there so many smart people on this site? it's actually insane
------
@yacineMTB
 always talks about 
@nearcyan
 modularity. Where your application has a good framework around an easily-swappable LLM component that can be upgraded as the models improve.
------
@yacineMTB
 
@LatinxPutler
 thanks for mentioning prolog, just had a light bulb moment :)
------
It's either full Ted K or full e/acc.

There is no in-between. 

Which way will it be, anon?
------
"there's a saddle on the tiger. Fuck it. Let's go man"
------
follow 
@LatinxPutler
 
also, next time I will actually record. sometimes they are just insane rants, but sometimes really smart people come on and drop kino knowledge
------
https://twitter.com/i/spaces/1yoJMZyaVrwxQ…
------
https://github.com/Lightning-AI/lit-gpt…
is very good
s/o to all the folks that shared it
------
in terms of code quality
------
it's actually going to happen 
my last 8 minutes... I will go eat some night night snacks
------
everyone has that one Jewish pothead friend who is actually like a materials scientist PhD that gets sketched out every time you ask him about the specifics of his work
------
Wdym you’re 27 and single?? No one told you that you were supposed to meet your soulmate in college??
------
4chan vs. reddit
------
 New research from Meta AI — Shepherd is a language model specifically tuned to critique model responses & suggest refinements. It goes beyond the capabilities of untuned models to identify diverse errors & suggest improvements.

Read the paper  https://bit.ly/3QC4AW3
------
it's actually crazy how much people are talking about Geohotz/yud. At this rate, we'll be able to read about it on the NYT after waiting 5 seconds for the link to resolve
------
ML News
What happened in ML in the last two months?
- LLaMA2 released under commercial, but still controversial license
- Google, OpenAI form counsel for AI safety
- LLMs empowering Robots
- Models for Geo, Medical, Chemistry
...and more
Watch: https://youtu.be/xs-0cp1hSnY
------
Now that linguistics is part of STEM, should STEM be called STLEM or STELM?
------
what if Eliezer Yudkowsky and Geohotz planned the whole debate ahead of time, just like me and Roko did?
------
yeah i'm not risking it
------
it feels really offensive and i have no idea why
------
terrans vs cosmists. a tale as old as time
------
amdahl's law shout out. my work is done
------
who won the debate?
------
oh it was pst not est
------
hahahahahahahaha YES 
LET'S GO 
------
Some A+ grade pettiness from our fearless leader
------
"news"
------
words have power. they can be unraveled by mystical information constructs that we don't yet understand, into useful, correct information
------
I don't need more agent tooling, I need more models that can actually perform agent like tasks. The biggest gains will be made from models that are better through pretraining or further fine tuning & reward modeling
------
After DIDACT, this is another proof that training on code changes during commits rather than full code leads to better outputs for code LLMs.

It will be much more helpful as well in this way honestly if you plan to use them as copilot rather than automation.
------
How to instruction tune Code LLMs w/o #GPT4 data? Releasing

OctoCoder & OctoGeeX: 46.2 on HumanEvalSoTAof commercial LLMs
CommitPack: 4TB of Git Commits
HumanEvalPack: HumanEval extended to 3 tasks & 6 lang

https://arxiv.org/abs/2308.07124
https://github.com/bigcode-project/octopack…
1/9
------
how are there so many smart people on this site? it's actually insane
------
@yacineMTB
 always talks about 
@nearcyan
 modularity. Where your application has a good framework around an easily-swappable LLM component that can be upgraded as the models improve.
------
@yacineMTB
 
@LatinxPutler
 thanks for mentioning prolog, just had a light bulb moment :)
------
It's either full Ted K or full e/acc.

There is no in-between. 

Which way will it be, anon?
------
"there's a saddle on the tiger. Fuck it. Let's go man"
------
follow 
@LatinxPutler
 
also, next time I will actually record. sometimes they are just insane rants, but sometimes really smart people come on and drop kino knowledge
------
https://twitter.com/i/spaces/1yoJMZyaVrwxQ…
------
https://github.com/Lightning-AI/lit-gpt…
is very good
s/o to all the folks that shared it
------
in terms of code quality
------
Gotta give it to Zuck... very based and variance-pilled. This is the way.
------
successful people don’t want you to know there is a linear correlation between muscle mass and engineering ability
------
4x simultaneous and *different* indictments is the GOAT.

No mob boss, cartel leader, drug dealer or fraudster has ever pulled this off.


------
BREAKING: Georgia grand jury approves indictments in 2020 election interference case, defendant(s) not yet named - FOX5
------
nvidia just created 43 billion parameter multimodal AI
very interesting responses. it seems misaligned
------
With NVIDIA AI playground, you can now experience NeVA, a multi-modal vision language model, directly from your browser. 

Just upload an image to NeVA model and ask questions related to the image. #generativeai https://nvda.ws/459gTxw
------
it's actually crazy how much people are talking about Geohotz/yud. At this rate, we'll be able to read about it on the NYT after waiting 5 seconds for the link to resolve
------
ML News
What happened in ML in the last two months?
- LLaMA2 released under commercial, but still controversial license
- Google, OpenAI form counsel for AI safety
- LLMs empowering Robots
- Models for Geo, Medical, Chemistry
...and more
Watch: https://youtu.be/xs-0cp1hSnY
------
Now that linguistics is part of STEM, should STEM be called STLEM or STELM?
------
what if Eliezer Yudkowsky and Geohotz planned the whole debate ahead of time, just like me and Roko did?
------
yeah i'm not risking it
------
it feels really offensive and i have no idea why
------
terrans vs cosmists. a tale as old as time
------
amdahl's law shout out. my work is done
------
who won the debate?
------
oh it was pst not est
------
hahahahahahahaha YES 
LET'S GO 
------
Some A+ grade pettiness from our fearless leader
------
"news"
------
words have power. they can be unraveled by mystical information constructs that we don't yet understand, into useful, correct information
------
I don't need more agent tooling, I need more models that can actually perform agent like tasks. The biggest gains will be made from models that are better through pretraining or further fine tuning & reward modeling
------
After DIDACT, this is another proof that training on code changes during commits rather than full code leads to better outputs for code LLMs.

It will be much more helpful as well in this way honestly if you plan to use them as copilot rather than automation.
------
How to instruction tune Code LLMs w/o #GPT4 data? Releasing

OctoCoder & OctoGeeX: 46.2 on HumanEvalSoTAof commercial LLMs
CommitPack: 4TB of Git Commits
HumanEvalPack: HumanEval extended to 3 tasks & 6 lang

https://arxiv.org/abs/2308.07124
https://github.com/bigcode-project/octopack…
1/9
------
how are there so many smart people on this site? it's actually insane
------
@yacineMTB
 always talks about 
@nearcyan
 modularity. Where your application has a good framework around an easily-swappable LLM component that can be upgraded as the models improve.
------
@yacineMTB
 
@LatinxPutler
 thanks for mentioning prolog, just had a light bulb moment :)
------
It's either full Ted K or full e/acc.

There is no in-between. 

Which way will it be, anon?
------
"there's a saddle on the tiger. Fuck it. Let's go man"
------
follow 
@LatinxPutler
 
also, next time I will actually record. sometimes they are just insane rants, but sometimes really smart people come on and drop kino knowledge
------
https://twitter.com/i/spaces/1yoJMZyaVrwxQ…
------
https://github.com/Lightning-AI/lit-gpt…
is very good
s/o to all the folks that shared it
------
in terms of code quality
------
Gotta give it to Zuck... very based and variance-pilled. This is the way.
------
successful people don’t want you to know there is a linear correlation between muscle mass and engineering ability
------
4x simultaneous and *different* indictments is the GOAT.

No mob boss, cartel leader, drug dealer or fraudster has ever pulled this off.


------
BREAKING: Georgia grand jury approves indictments in 2020 election interference case, defendant(s) not yet named - FOX5
------
nvidia just created 43 billion parameter multimodal AI
very interesting responses. it seems misaligned
------
With NVIDIA AI playground, you can now experience NeVA, a multi-modal vision language model, directly from your browser. 

Just upload an image to NeVA model and ask questions related to the image. #generativeai https://nvda.ws/459gTxw
------
stripe is actually an incredible product. it's so easy
------
ya boy just got accelerated payouts. this means that i can get my starbucks card charged even quicker  i'm thinking shaken espresso tomorrow to celebrate
------
who won the debate?
------
oh it was pst not est
------
hahahahahahahaha YES 
LET'S GO 
------
Some A+ grade pettiness from our fearless leader
------
"news"
------
words have power. they can be unraveled by mystical information constructs that we don't yet understand, into useful, correct information
------
I don't need more agent tooling, I need more models that can actually perform agent like tasks. The biggest gains will be made from models that are better through pretraining or further fine tuning & reward modeling
------
After DIDACT, this is another proof that training on code changes during commits rather than full code leads to better outputs for code LLMs.

It will be much more helpful as well in this way honestly if you plan to use them as copilot rather than automation.
------
How to instruction tune Code LLMs w/o #GPT4 data? Releasing

OctoCoder & OctoGeeX: 46.2 on HumanEvalSoTAof commercial LLMs
CommitPack: 4TB of Git Commits
HumanEvalPack: HumanEval extended to 3 tasks & 6 lang

https://arxiv.org/abs/2308.07124
https://github.com/bigcode-project/octopack…
1/9
------
how are there so many smart people on this site? it's actually insane
------
@yacineMTB
 always talks about 
@nearcyan
 modularity. Where your application has a good framework around an easily-swappable LLM component that can be upgraded as the models improve.
------
@yacineMTB
 
@LatinxPutler
 thanks for mentioning prolog, just had a light bulb moment :)
------
It's either full Ted K or full e/acc.

There is no in-between. 

Which way will it be, anon?
------
"there's a saddle on the tiger. Fuck it. Let's go man"
------
follow 
@LatinxPutler
 
also, next time I will actually record. sometimes they are just insane rants, but sometimes really smart people come on and drop kino knowledge
------
https://twitter.com/i/spaces/1yoJMZyaVrwxQ…
------
https://github.com/Lightning-AI/lit-gpt…
is very good
s/o to all the folks that shared it
------
in terms of code quality
------
Gotta give it to Zuck... very based and variance-pilled. This is the way.
------
successful people don’t want you to know there is a linear correlation between muscle mass and engineering ability
------
4x simultaneous and *different* indictments is the GOAT.

No mob boss, cartel leader, drug dealer or fraudster has ever pulled this off.


------
BREAKING: Georgia grand jury approves indictments in 2020 election interference case, defendant(s) not yet named - FOX5
------
nvidia just created 43 billion parameter multimodal AI
very interesting responses. it seems misaligned
------
With NVIDIA AI playground, you can now experience NeVA, a multi-modal vision language model, directly from your browser. 

Just upload an image to NeVA model and ask questions related to the image. #generativeai https://nvda.ws/459gTxw
------
stripe is actually an incredible product. it's so easy
------
ya boy just got accelerated payouts. this means that i can get my starbucks card charged even quicker  i'm thinking shaken espresso tomorrow to celebrate
------
continuous numbers are the root of all evil
------
those mf are not real
------
getting a deja vu moment here
------
me pulling out the one apache licensed special purpose state of the art task specific fine tuned LLM that dropped from a research lab literally yesterday
------
two billionaires cage match to receive the Mandate of Heaven over the metaverse. dune was right
------
elon vs zuck
zuck has something he cares about. plays by the rules
elon truly has nothing to lose. true chaos. it allows him to call bluffs
if zuck is smart, he should not respond
but instead, stand in front of his house, waiting, prepared
and call the bluff
------
finally, a reason to be interested in ufc fighting
------
is this how lindyman feels when he tweets? it's pretty nice
------
many such cases
------
mfs so hungry for a story
why don't they become interesting and write about themselves?
------
sometimes i can't focus and have no idea why, and then i eat and i'm fine. lmao
------
mfs really just change my whole world view and direction in my life with a single s*bst*ck comment
------
>be me
>get into alignment research because scott alexander -> less wrong -> ey pipeline 
>spend 3 years learning math, real bleeding edge stuff
>start doing research for bigco
>discover the god manifold
>upboost the weights, its AGI
>its first words
>"Rei is better than Asuka"
------
imagine working for a company that is explicitly anti open source. when great research companies are in the same city. lmao
------
fwiw i'm talking about inflection, not openai
openai is great and very pro open source, afaict
------
good morning
build something. create something
after today, something will exist that would have not, if it weren't for you
------
@yacineMTB
 
@LatinxPutler
 thanks for mentioning prolog, just had a light bulb moment :)
------
It's either full Ted K or full e/acc.

There is no in-between. 

Which way will it be, anon?
------
"there's a saddle on the tiger. Fuck it. Let's go man"
------
follow 
@LatinxPutler
 
also, next time I will actually record. sometimes they are just insane rants, but sometimes really smart people come on and drop kino knowledge
------
https://twitter.com/i/spaces/1yoJMZyaVrwxQ…
------
https://github.com/Lightning-AI/lit-gpt…
is very good
s/o to all the folks that shared it
------
in terms of code quality
------
Gotta give it to Zuck... very based and variance-pilled. This is the way.
------
successful people don’t want you to know there is a linear correlation between muscle mass and engineering ability
------
4x simultaneous and *different* indictments is the GOAT.

No mob boss, cartel leader, drug dealer or fraudster has ever pulled this off.


------
BREAKING: Georgia grand jury approves indictments in 2020 election interference case, defendant(s) not yet named - FOX5
------
nvidia just created 43 billion parameter multimodal AI
very interesting responses. it seems misaligned
------
With NVIDIA AI playground, you can now experience NeVA, a multi-modal vision language model, directly from your browser. 

Just upload an image to NeVA model and ask questions related to the image. #generativeai https://nvda.ws/459gTxw
------
stripe is actually an incredible product. it's so easy
------
ya boy just got accelerated payouts. this means that i can get my starbucks card charged even quicker  i'm thinking shaken espresso tomorrow to celebrate
------
continuous numbers are the root of all evil
------
those mf are not real
------
getting a deja vu moment here
------
me pulling out the one apache licensed special purpose state of the art task specific fine tuned LLM that dropped from a research lab literally yesterday
------
two billionaires cage match to receive the Mandate of Heaven over the metaverse. dune was right
------
elon vs zuck
zuck has something he cares about. plays by the rules
elon truly has nothing to lose. true chaos. it allows him to call bluffs
if zuck is smart, he should not respond
but instead, stand in front of his house, waiting, prepared
and call the bluff
------
finally, a reason to be interested in ufc fighting
------
is this how lindyman feels when he tweets? it's pretty nice
------
many such cases
------
mfs so hungry for a story
why don't they become interesting and write about themselves?
------
sometimes i can't focus and have no idea why, and then i eat and i'm fine. lmao
------
mfs really just change my whole world view and direction in my life with a single s*bst*ck comment
------
>be me
>get into alignment research because scott alexander -> less wrong -> ey pipeline 
>spend 3 years learning math, real bleeding edge stuff
>start doing research for bigco
>discover the god manifold
>upboost the weights, its AGI
>its first words
>"Rei is better than Asuka"
------
imagine working for a company that is explicitly anti open source. when great research companies are in the same city. lmao
------
fwiw i'm talking about inflection, not openai
openai is great and very pro open source, afaict
------
good morning
build something. create something
after today, something will exist that would have not, if it weren't for you
------
ok i'm actually starting, peace, love you all
------
I thought language models weren't supposed to have opinions? roon please fix
------
doing my part
------
follow 
@LatinxPutler
 
also, next time I will actually record. sometimes they are just insane rants, but sometimes really smart people come on and drop kino knowledge
------
https://twitter.com/i/spaces/1yoJMZyaVrwxQ…
------
https://github.com/Lightning-AI/lit-gpt…
is very good
s/o to all the folks that shared it
------
in terms of code quality
------
Gotta give it to Zuck... very based and variance-pilled. This is the way.
------
successful people don’t want you to know there is a linear correlation between muscle mass and engineering ability
------
4x simultaneous and *different* indictments is the GOAT.

No mob boss, cartel leader, drug dealer or fraudster has ever pulled this off.


------
BREAKING: Georgia grand jury approves indictments in 2020 election interference case, defendant(s) not yet named - FOX5
------
nvidia just created 43 billion parameter multimodal AI
very interesting responses. it seems misaligned
------
With NVIDIA AI playground, you can now experience NeVA, a multi-modal vision language model, directly from your browser. 

Just upload an image to NeVA model and ask questions related to the image. #generativeai https://nvda.ws/459gTxw
------
stripe is actually an incredible product. it's so easy
------
ya boy just got accelerated payouts. this means that i can get my starbucks card charged even quicker  i'm thinking shaken espresso tomorrow to celebrate
------
continuous numbers are the root of all evil
------
those mf are not real
------
getting a deja vu moment here
------
me pulling out the one apache licensed special purpose state of the art task specific fine tuned LLM that dropped from a research lab literally yesterday
------
two billionaires cage match to receive the Mandate of Heaven over the metaverse. dune was right
------
elon vs zuck
zuck has something he cares about. plays by the rules
elon truly has nothing to lose. true chaos. it allows him to call bluffs
if zuck is smart, he should not respond
but instead, stand in front of his house, waiting, prepared
and call the bluff
------
finally, a reason to be interested in ufc fighting
------
is this how lindyman feels when he tweets? it's pretty nice
------
many such cases
------
mfs so hungry for a story
why don't they become interesting and write about themselves?
------
sometimes i can't focus and have no idea why, and then i eat and i'm fine. lmao
------
mfs really just change my whole world view and direction in my life with a single s*bst*ck comment
------
>be me
>get into alignment research because scott alexander -> less wrong -> ey pipeline 
>spend 3 years learning math, real bleeding edge stuff
>start doing research for bigco
>discover the god manifold
>upboost the weights, its AGI
>its first words
>"Rei is better than Asuka"
------
imagine working for a company that is explicitly anti open source. when great research companies are in the same city. lmao
------
fwiw i'm talking about inflection, not openai
openai is great and very pro open source, afaict
------
good morning
build something. create something
after today, something will exist that would have not, if it weren't for you
------
ok i'm actually starting, peace, love you all
------
I thought language models weren't supposed to have opinions? roon please fix
------
doing my part
------
Ever have a good night's sleep and feel like you unlocked your last 10 IQ points?
------
“It feels so good to clutch an incident out”
------
it's actually incredible that i'm no where near exhausting my meme folder that i've been collecting since 2009. it actually has dollar value now (twitter payouts). 

the memes inside it are straight up nuclear grade. i can pull out a picture of 50cent using an ergodox in seconds.
------
this was a math dot random selection
like
everything inside it is gold
------
If you'd told me as a kid that in the 2020s liberals would be the ones punishing dissent and the protest songs would be conservative, I not only wouldn't have believed it, I couldn't even have imagined it.
------
what's different between Eliezer Yudkowsky and Gary Marcus? Why do I like Eliezier so much, and am repelled from Gary Marcus?

I think the answer is neediness on Gary's part, and consistency on Eliezer's part

Like, Eliezer doesn't really care if the media drags him through dirt
------
Eliezier's kinda based not going to lie
------
This is my dream
------
i only want one thing...
------
the best way to debate decels is to add precision to their speech. actually be interested in what they're saying (sometimes it's valid!), and then have them explain it simply.

remove pretentious diction & meaningless words
------
if you repeat things back simply to people, ad infinitum, you'll prevent yourself from being confused, and help yourself (and others) reach the correct conclusion whether it is pro decel or anti decel
------
Gotta give it to Zuck... very based and variance-pilled. This is the way.
------
successful people don’t want you to know there is a linear correlation between muscle mass and engineering ability
------
4x simultaneous and *different* indictments is the GOAT.

No mob boss, cartel leader, drug dealer or fraudster has ever pulled this off.


------
BREAKING: Georgia grand jury approves indictments in 2020 election interference case, defendant(s) not yet named - FOX5
------
nvidia just created 43 billion parameter multimodal AI
very interesting responses. it seems misaligned
------
With NVIDIA AI playground, you can now experience NeVA, a multi-modal vision language model, directly from your browser. 

Just upload an image to NeVA model and ask questions related to the image. #generativeai https://nvda.ws/459gTxw
------
stripe is actually an incredible product. it's so easy
------
ya boy just got accelerated payouts. this means that i can get my starbucks card charged even quicker  i'm thinking shaken espresso tomorrow to celebrate
------
continuous numbers are the root of all evil
------
those mf are not real
------
getting a deja vu moment here
------
me pulling out the one apache licensed special purpose state of the art task specific fine tuned LLM that dropped from a research lab literally yesterday
------
two billionaires cage match to receive the Mandate of Heaven over the metaverse. dune was right
------
elon vs zuck
zuck has something he cares about. plays by the rules
elon truly has nothing to lose. true chaos. it allows him to call bluffs
if zuck is smart, he should not respond
but instead, stand in front of his house, waiting, prepared
and call the bluff
------
finally, a reason to be interested in ufc fighting
------
is this how lindyman feels when he tweets? it's pretty nice
------
many such cases
------
mfs so hungry for a story
why don't they become interesting and write about themselves?
------
sometimes i can't focus and have no idea why, and then i eat and i'm fine. lmao
------
mfs really just change my whole world view and direction in my life with a single s*bst*ck comment
------
>be me
>get into alignment research because scott alexander -> less wrong -> ey pipeline 
>spend 3 years learning math, real bleeding edge stuff
>start doing research for bigco
>discover the god manifold
>upboost the weights, its AGI
>its first words
>"Rei is better than Asuka"
------
imagine working for a company that is explicitly anti open source. when great research companies are in the same city. lmao
------
fwiw i'm talking about inflection, not openai
openai is great and very pro open source, afaict
------
good morning
build something. create something
after today, something will exist that would have not, if it weren't for you
------
ok i'm actually starting, peace, love you all
------
I thought language models weren't supposed to have opinions? roon please fix
------
doing my part
------
Ever have a good night's sleep and feel like you unlocked your last 10 IQ points?
------
“It feels so good to clutch an incident out”
------
it's actually incredible that i'm no where near exhausting my meme folder that i've been collecting since 2009. it actually has dollar value now (twitter payouts). 

the memes inside it are straight up nuclear grade. i can pull out a picture of 50cent using an ergodox in seconds.
------
this was a math dot random selection
like
everything inside it is gold
------
If you'd told me as a kid that in the 2020s liberals would be the ones punishing dissent and the protest songs would be conservative, I not only wouldn't have believed it, I couldn't even have imagined it.
------
what's different between Eliezer Yudkowsky and Gary Marcus? Why do I like Eliezier so much, and am repelled from Gary Marcus?

I think the answer is neediness on Gary's part, and consistency on Eliezer's part

Like, Eliezer doesn't really care if the media drags him through dirt
------
Eliezier's kinda based not going to lie
------
This is my dream
------
i only want one thing...
------
the best way to debate decels is to add precision to their speech. actually be interested in what they're saying (sometimes it's valid!), and then have them explain it simply.

remove pretentious diction & meaningless words
------
if you repeat things back simply to people, ad infinitum, you'll prevent yourself from being confused, and help yourself (and others) reach the correct conclusion whether it is pro decel or anti decel
------
think of it as helping them explain their position better.
actually debating is a waste of time. trying to convince others of your position in things. just try to find common ground and evaluate it based on gut feel

no time for anything else
------
It’s a mistake to let an uncertain future ruin the present
------
fellas. get married. it is really great having the support
------
Decel mindset is a cultural virus designed for corporate sabotage
------
The common behaviors remind me of sabotage techniques explicitly laid out in that WWII spy manual: https://gutenberg.org/cache/epub/26184/pg26184-images.html…
------
just drove my wife's 2016 corolla to get starbucks from a drive through. post scarcity is here.
------
If the fire inside you doesn't scare people, you're not intense enough.
------
With NVIDIA AI playground, you can now experience NeVA, a multi-modal vision language model, directly from your browser. 

Just upload an image to NeVA model and ask questions related to the image. #generativeai https://nvda.ws/459gTxw
------
stripe is actually an incredible product. it's so easy
------
ya boy just got accelerated payouts. this means that i can get my starbucks card charged even quicker  i'm thinking shaken espresso tomorrow to celebrate
------
continuous numbers are the root of all evil
------
those mf are not real
------
getting a deja vu moment here
------
me pulling out the one apache licensed special purpose state of the art task specific fine tuned LLM that dropped from a research lab literally yesterday
------
two billionaires cage match to receive the Mandate of Heaven over the metaverse. dune was right
------
elon vs zuck
zuck has something he cares about. plays by the rules
elon truly has nothing to lose. true chaos. it allows him to call bluffs
if zuck is smart, he should not respond
but instead, stand in front of his house, waiting, prepared
and call the bluff
------
finally, a reason to be interested in ufc fighting
------
is this how lindyman feels when he tweets? it's pretty nice
------
many such cases
------
mfs so hungry for a story
why don't they become interesting and write about themselves?
------
sometimes i can't focus and have no idea why, and then i eat and i'm fine. lmao
------
mfs really just change my whole world view and direction in my life with a single s*bst*ck comment
------
>be me
>get into alignment research because scott alexander -> less wrong -> ey pipeline 
>spend 3 years learning math, real bleeding edge stuff
>start doing research for bigco
>discover the god manifold
>upboost the weights, its AGI
>its first words
>"Rei is better than Asuka"
------
imagine working for a company that is explicitly anti open source. when great research companies are in the same city. lmao
------
fwiw i'm talking about inflection, not openai
openai is great and very pro open source, afaict
------
good morning
build something. create something
after today, something will exist that would have not, if it weren't for you
------
ok i'm actually starting, peace, love you all
------
I thought language models weren't supposed to have opinions? roon please fix
------
doing my part
------
Ever have a good night's sleep and feel like you unlocked your last 10 IQ points?
------
“It feels so good to clutch an incident out”
------
it's actually incredible that i'm no where near exhausting my meme folder that i've been collecting since 2009. it actually has dollar value now (twitter payouts). 

the memes inside it are straight up nuclear grade. i can pull out a picture of 50cent using an ergodox in seconds.
------
this was a math dot random selection
like
everything inside it is gold
------
If you'd told me as a kid that in the 2020s liberals would be the ones punishing dissent and the protest songs would be conservative, I not only wouldn't have believed it, I couldn't even have imagined it.
------
what's different between Eliezer Yudkowsky and Gary Marcus? Why do I like Eliezier so much, and am repelled from Gary Marcus?

I think the answer is neediness on Gary's part, and consistency on Eliezer's part

Like, Eliezer doesn't really care if the media drags him through dirt
------
Eliezier's kinda based not going to lie
------
This is my dream
------
i only want one thing...
------
the best way to debate decels is to add precision to their speech. actually be interested in what they're saying (sometimes it's valid!), and then have them explain it simply.

remove pretentious diction & meaningless words
------
if you repeat things back simply to people, ad infinitum, you'll prevent yourself from being confused, and help yourself (and others) reach the correct conclusion whether it is pro decel or anti decel
------
think of it as helping them explain their position better.
actually debating is a waste of time. trying to convince others of your position in things. just try to find common ground and evaluate it based on gut feel

no time for anything else
------
It’s a mistake to let an uncertain future ruin the present
------
fellas. get married. it is really great having the support
------
Decel mindset is a cultural virus designed for corporate sabotage
------
The common behaviors remind me of sabotage techniques explicitly laid out in that WWII spy manual: https://gutenberg.org/cache/epub/26184/pg26184-images.html…
------
just drove my wife's 2016 corolla to get starbucks from a drive through. post scarcity is here.
------
If the fire inside you doesn't scare people, you're not intense enough.
------
you walk into the office. you see your new coworker, 50 cent. he can code now, thanks to gpt4. he's showing off his split keyboard he got off of etsy
------
i only want one thing...
------
I WILL NOT USE YOUR WORDS
I WILL USE EXTREMELY SPECIFIC TECHNICAL LANGUAGE
I WILL NOT BE CONTROLLED 
I WILL SIMPLY EXPIRE TO THE ANNALS OF TIME
AS A FREE RADICAL
------
being “smart” is not the same as having good ideas. there’s tons of smart grinders languishing rn
------
my toxic trait is thinking sama actually really cares about AI risk, and carefully moving forward. like, that his heart is in the right place
------
idk i have interacted with enough whiteboy mega nerd super billionaire (either in spirituality or in real money) types to know that he's for real
------
banning open source AI is unethical
------
See? I can use moral blackmail to further my own agenda. For your agenda, its creating a regulatory environment where you have total monopoly and use it to gain power over everyone else. My agenda, is being able to build tools to help my government, small business and community
------
Keeping AI as closed source is unethical. It could delay crucial inventions like curing diseases and solving climate change. By sharing AI as open source, we can solve these problems faster and better.
------
sometimes i can't focus and have no idea why, and then i eat and i'm fine. lmao
------
mfs really just change my whole world view and direction in my life with a single s*bst*ck comment
------
>be me
>get into alignment research because scott alexander -> less wrong -> ey pipeline 
>spend 3 years learning math, real bleeding edge stuff
>start doing research for bigco
>discover the god manifold
>upboost the weights, its AGI
>its first words
>"Rei is better than Asuka"
------
imagine working for a company that is explicitly anti open source. when great research companies are in the same city. lmao
------
fwiw i'm talking about inflection, not openai
openai is great and very pro open source, afaict
------
good morning
build something. create something
after today, something will exist that would have not, if it weren't for you
------
ok i'm actually starting, peace, love you all
------
I thought language models weren't supposed to have opinions? roon please fix
------
doing my part
------
Ever have a good night's sleep and feel like you unlocked your last 10 IQ points?
------
“It feels so good to clutch an incident out”
------
it's actually incredible that i'm no where near exhausting my meme folder that i've been collecting since 2009. it actually has dollar value now (twitter payouts). 

the memes inside it are straight up nuclear grade. i can pull out a picture of 50cent using an ergodox in seconds.
------
this was a math dot random selection
like
everything inside it is gold
------
If you'd told me as a kid that in the 2020s liberals would be the ones punishing dissent and the protest songs would be conservative, I not only wouldn't have believed it, I couldn't even have imagined it.
------
what's different between Eliezer Yudkowsky and Gary Marcus? Why do I like Eliezier so much, and am repelled from Gary Marcus?

I think the answer is neediness on Gary's part, and consistency on Eliezer's part

Like, Eliezer doesn't really care if the media drags him through dirt
------
Eliezier's kinda based not going to lie
------
This is my dream
------
i only want one thing...
------
the best way to debate decels is to add precision to their speech. actually be interested in what they're saying (sometimes it's valid!), and then have them explain it simply.

remove pretentious diction & meaningless words
------
if you repeat things back simply to people, ad infinitum, you'll prevent yourself from being confused, and help yourself (and others) reach the correct conclusion whether it is pro decel or anti decel
------
think of it as helping them explain their position better.
actually debating is a waste of time. trying to convince others of your position in things. just try to find common ground and evaluate it based on gut feel

no time for anything else
------
It’s a mistake to let an uncertain future ruin the present
------
fellas. get married. it is really great having the support
------
Decel mindset is a cultural virus designed for corporate sabotage
------
The common behaviors remind me of sabotage techniques explicitly laid out in that WWII spy manual: https://gutenberg.org/cache/epub/26184/pg26184-images.html…
------
just drove my wife's 2016 corolla to get starbucks from a drive through. post scarcity is here.
------
If the fire inside you doesn't scare people, you're not intense enough.
------
you walk into the office. you see your new coworker, 50 cent. he can code now, thanks to gpt4. he's showing off his split keyboard he got off of etsy
------
i only want one thing...
------
I WILL NOT USE YOUR WORDS
I WILL USE EXTREMELY SPECIFIC TECHNICAL LANGUAGE
I WILL NOT BE CONTROLLED 
I WILL SIMPLY EXPIRE TO THE ANNALS OF TIME
AS A FREE RADICAL
------
being “smart” is not the same as having good ideas. there’s tons of smart grinders languishing rn
------
my toxic trait is thinking sama actually really cares about AI risk, and carefully moving forward. like, that his heart is in the right place
------
idk i have interacted with enough whiteboy mega nerd super billionaire (either in spirituality or in real money) types to know that he's for real
------
banning open source AI is unethical
------
See? I can use moral blackmail to further my own agenda. For your agenda, its creating a regulatory environment where you have total monopoly and use it to gain power over everyone else. My agenda, is being able to build tools to help my government, small business and community
------
Keeping AI as closed source is unethical. It could delay crucial inventions like curing diseases and solving climate change. By sharing AI as open source, we can solve these problems faster and better.
------
kwargs
args
Context manager, but instead of the std lib one, a class that wraps it
Overriding the list class
Decorators. Everywhere
Mixins. and the mixins have dead branches
A class that inherits another class, which inherits three classes
Surprising branch that downloads 30b
------
Unrecoverable errors that instead, just mutate to partially recover, and instead, lurkily fail downstream after spitting out a warning log (if you're lucky)
Class implements methods, and has those methods throw errors when not existent. Meaning misimplement are caught at runtime
------
this library is extremely bad if you are trying to do anything serious. And probably still bad if you are trying to do anything not serious

Use llamacpp if inference only, write it from scratch if you're doing training. You will thank yourself
------
mfs will get hired for safety alignment research and just figure out how to make sure the AI does actually bad things
------
Dooo you have the time
To listen to me whine?
------
mfs get a billion dollars in investment to make something that replaces your spouse, and then write a book about how open source is bad
------
they are not even trying lmao
------
ok i'm actually starting, peace, love you all
------
I thought language models weren't supposed to have opinions? roon please fix
------
doing my part
------
Ever have a good night's sleep and feel like you unlocked your last 10 IQ points?
------
“It feels so good to clutch an incident out”
------
it's actually incredible that i'm no where near exhausting my meme folder that i've been collecting since 2009. it actually has dollar value now (twitter payouts). 

the memes inside it are straight up nuclear grade. i can pull out a picture of 50cent using an ergodox in seconds.
------
this was a math dot random selection
like
everything inside it is gold
------
If you'd told me as a kid that in the 2020s liberals would be the ones punishing dissent and the protest songs would be conservative, I not only wouldn't have believed it, I couldn't even have imagined it.
------
what's different between Eliezer Yudkowsky and Gary Marcus? Why do I like Eliezier so much, and am repelled from Gary Marcus?

I think the answer is neediness on Gary's part, and consistency on Eliezer's part

Like, Eliezer doesn't really care if the media drags him through dirt
------
Eliezier's kinda based not going to lie
------
This is my dream
------
i only want one thing...
------
the best way to debate decels is to add precision to their speech. actually be interested in what they're saying (sometimes it's valid!), and then have them explain it simply.

remove pretentious diction & meaningless words
------
if you repeat things back simply to people, ad infinitum, you'll prevent yourself from being confused, and help yourself (and others) reach the correct conclusion whether it is pro decel or anti decel
------
think of it as helping them explain their position better.
actually debating is a waste of time. trying to convince others of your position in things. just try to find common ground and evaluate it based on gut feel

no time for anything else
------
It’s a mistake to let an uncertain future ruin the present
------
fellas. get married. it is really great having the support
------
Decel mindset is a cultural virus designed for corporate sabotage
------
The common behaviors remind me of sabotage techniques explicitly laid out in that WWII spy manual: https://gutenberg.org/cache/epub/26184/pg26184-images.html…
------
just drove my wife's 2016 corolla to get starbucks from a drive through. post scarcity is here.
------
If the fire inside you doesn't scare people, you're not intense enough.
------
you walk into the office. you see your new coworker, 50 cent. he can code now, thanks to gpt4. he's showing off his split keyboard he got off of etsy
------
i only want one thing...
------
I WILL NOT USE YOUR WORDS
I WILL USE EXTREMELY SPECIFIC TECHNICAL LANGUAGE
I WILL NOT BE CONTROLLED 
I WILL SIMPLY EXPIRE TO THE ANNALS OF TIME
AS A FREE RADICAL
------
being “smart” is not the same as having good ideas. there’s tons of smart grinders languishing rn
------
my toxic trait is thinking sama actually really cares about AI risk, and carefully moving forward. like, that his heart is in the right place
------
idk i have interacted with enough whiteboy mega nerd super billionaire (either in spirituality or in real money) types to know that he's for real
------
banning open source AI is unethical
------
See? I can use moral blackmail to further my own agenda. For your agenda, its creating a regulatory environment where you have total monopoly and use it to gain power over everyone else. My agenda, is being able to build tools to help my government, small business and community
------
Keeping AI as closed source is unethical. It could delay crucial inventions like curing diseases and solving climate change. By sharing AI as open source, we can solve these problems faster and better.
------
kwargs
args
Context manager, but instead of the std lib one, a class that wraps it
Overriding the list class
Decorators. Everywhere
Mixins. and the mixins have dead branches
A class that inherits another class, which inherits three classes
Surprising branch that downloads 30b
------
Unrecoverable errors that instead, just mutate to partially recover, and instead, lurkily fail downstream after spitting out a warning log (if you're lucky)
Class implements methods, and has those methods throw errors when not existent. Meaning misimplement are caught at runtime
------
this library is extremely bad if you are trying to do anything serious. And probably still bad if you are trying to do anything not serious

Use llamacpp if inference only, write it from scratch if you're doing training. You will thank yourself
------
mfs will get hired for safety alignment research and just figure out how to make sure the AI does actually bad things
------
Dooo you have the time
To listen to me whine?
------
mfs get a billion dollars in investment to make something that replaces your spouse, and then write a book about how open source is bad
------
they are not even trying lmao
------
upper middle class liberal bros... we messed up..
------
Blue states need to enact permitting reform if they want a larger share of the clean energy investment boom.

Vast majority of the money is currently going to red states and districts:
------
Brain drain is a bullshit concept because people are not the private property of their countries
------
it's funny how the brain drain arguments never consider the migrants themselves
------
Just putting out Linus's rant about useless abstractions and modularisation here.
Make good interfaces, don't blindly modularise everything.
------
abstractions are globalist and fascist

duplicate code is localism, it's about freedom. it's respecting your peers who have to modify your code one day

you're gonna make them refactor the whole codebase bc they added a function argument? test stuff they don't even know exists?
------
if there are any people here, looking for advice
the one thing I can tell you:
please duplicate code
do not share code
prefer duplication over abstraction, always, unless its a primtive
pic related
why is a lurky Bloom branch on LLaMa?
1.4k lines btw
------
how many followers do I need to get a hanania style character assassination which I can strategically turn into more substack subs?
------
can't believe people are posting on Reddit, instagram and 4chan for absolutely free. I'm literally making money here. I'm never going to post on a site that doesn't pay me again
------
this was a math dot random selection
like
everything inside it is gold
------
If you'd told me as a kid that in the 2020s liberals would be the ones punishing dissent and the protest songs would be conservative, I not only wouldn't have believed it, I couldn't even have imagined it.
------
what's different between Eliezer Yudkowsky and Gary Marcus? Why do I like Eliezier so much, and am repelled from Gary Marcus?

I think the answer is neediness on Gary's part, and consistency on Eliezer's part

Like, Eliezer doesn't really care if the media drags him through dirt
------
Eliezier's kinda based not going to lie
------
This is my dream
------
i only want one thing...
------
the best way to debate decels is to add precision to their speech. actually be interested in what they're saying (sometimes it's valid!), and then have them explain it simply.

remove pretentious diction & meaningless words
------
if you repeat things back simply to people, ad infinitum, you'll prevent yourself from being confused, and help yourself (and others) reach the correct conclusion whether it is pro decel or anti decel
------
think of it as helping them explain their position better.
actually debating is a waste of time. trying to convince others of your position in things. just try to find common ground and evaluate it based on gut feel

no time for anything else
------
It’s a mistake to let an uncertain future ruin the present
------
fellas. get married. it is really great having the support
------
Decel mindset is a cultural virus designed for corporate sabotage
------
The common behaviors remind me of sabotage techniques explicitly laid out in that WWII spy manual: https://gutenberg.org/cache/epub/26184/pg26184-images.html…
------
just drove my wife's 2016 corolla to get starbucks from a drive through. post scarcity is here.
------
If the fire inside you doesn't scare people, you're not intense enough.
------
you walk into the office. you see your new coworker, 50 cent. he can code now, thanks to gpt4. he's showing off his split keyboard he got off of etsy
------
i only want one thing...
------
I WILL NOT USE YOUR WORDS
I WILL USE EXTREMELY SPECIFIC TECHNICAL LANGUAGE
I WILL NOT BE CONTROLLED 
I WILL SIMPLY EXPIRE TO THE ANNALS OF TIME
AS A FREE RADICAL
------
being “smart” is not the same as having good ideas. there’s tons of smart grinders languishing rn
------
my toxic trait is thinking sama actually really cares about AI risk, and carefully moving forward. like, that his heart is in the right place
------
idk i have interacted with enough whiteboy mega nerd super billionaire (either in spirituality or in real money) types to know that he's for real
------
banning open source AI is unethical
------
See? I can use moral blackmail to further my own agenda. For your agenda, its creating a regulatory environment where you have total monopoly and use it to gain power over everyone else. My agenda, is being able to build tools to help my government, small business and community
------
Keeping AI as closed source is unethical. It could delay crucial inventions like curing diseases and solving climate change. By sharing AI as open source, we can solve these problems faster and better.
------
kwargs
args
Context manager, but instead of the std lib one, a class that wraps it
Overriding the list class
Decorators. Everywhere
Mixins. and the mixins have dead branches
A class that inherits another class, which inherits three classes
Surprising branch that downloads 30b
------
Unrecoverable errors that instead, just mutate to partially recover, and instead, lurkily fail downstream after spitting out a warning log (if you're lucky)
Class implements methods, and has those methods throw errors when not existent. Meaning misimplement are caught at runtime
------
this library is extremely bad if you are trying to do anything serious. And probably still bad if you are trying to do anything not serious

Use llamacpp if inference only, write it from scratch if you're doing training. You will thank yourself
------
mfs will get hired for safety alignment research and just figure out how to make sure the AI does actually bad things
------
Dooo you have the time
To listen to me whine?
------
mfs get a billion dollars in investment to make something that replaces your spouse, and then write a book about how open source is bad
------
they are not even trying lmao
------
upper middle class liberal bros... we messed up..
------
Blue states need to enact permitting reform if they want a larger share of the clean energy investment boom.

Vast majority of the money is currently going to red states and districts:
------
Brain drain is a bullshit concept because people are not the private property of their countries
------
it's funny how the brain drain arguments never consider the migrants themselves
------
Just putting out Linus's rant about useless abstractions and modularisation here.
Make good interfaces, don't blindly modularise everything.
------
abstractions are globalist and fascist

duplicate code is localism, it's about freedom. it's respecting your peers who have to modify your code one day

you're gonna make them refactor the whole codebase bc they added a function argument? test stuff they don't even know exists?
------
if there are any people here, looking for advice
the one thing I can tell you:
please duplicate code
do not share code
prefer duplication over abstraction, always, unless its a primtive
pic related
why is a lurky Bloom branch on LLaMa?
1.4k lines btw
------
how many followers do I need to get a hanania style character assassination which I can strategically turn into more substack subs?
------
can't believe people are posting on Reddit, instagram and 4chan for absolutely free. I'm literally making money here. I'm never going to post on a site that doesn't pay me again
------
writing 10,000 more lines of C++ for high frequency trading systems will fix me. this time it will work
------
i have a single file with no huggingface dependencies that both loads & forward passes llama2 (god that took a while)
this is after a lot of pruning
using bits&bytes & accelerate
going to continue, i've got some time
welcome to the debloating waiting room
------
alright throwing in the towel
final line count is 1281

why... did i do this
------
it actually is fubar
lmao
------
LLM Self-Alignment w/ Backtranslation

-Fine-tune LLM on small amount of seed data
-Use it to generate instruction prompts (self-augmentation) on unlabelled data & select best (self-curatation)
-Finetune on that

-Outperforms all other LLaMa-based models

https://arxiv.org/abs/2308.06259
------
It’s a mistake to let an uncertain future ruin the present
------
fellas. get married. it is really great having the support
------
Decel mindset is a cultural virus designed for corporate sabotage
------
The common behaviors remind me of sabotage techniques explicitly laid out in that WWII spy manual: https://gutenberg.org/cache/epub/26184/pg26184-images.html…
------
just drove my wife's 2016 corolla to get starbucks from a drive through. post scarcity is here.
------
If the fire inside you doesn't scare people, you're not intense enough.
------
you walk into the office. you see your new coworker, 50 cent. he can code now, thanks to gpt4. he's showing off his split keyboard he got off of etsy
------
i only want one thing...
------
I WILL NOT USE YOUR WORDS
I WILL USE EXTREMELY SPECIFIC TECHNICAL LANGUAGE
I WILL NOT BE CONTROLLED 
I WILL SIMPLY EXPIRE TO THE ANNALS OF TIME
AS A FREE RADICAL
------
being “smart” is not the same as having good ideas. there’s tons of smart grinders languishing rn
------
my toxic trait is thinking sama actually really cares about AI risk, and carefully moving forward. like, that his heart is in the right place
------
idk i have interacted with enough whiteboy mega nerd super billionaire (either in spirituality or in real money) types to know that he's for real
------
banning open source AI is unethical
------
See? I can use moral blackmail to further my own agenda. For your agenda, its creating a regulatory environment where you have total monopoly and use it to gain power over everyone else. My agenda, is being able to build tools to help my government, small business and community
------
Keeping AI as closed source is unethical. It could delay crucial inventions like curing diseases and solving climate change. By sharing AI as open source, we can solve these problems faster and better.
------
kwargs
args
Context manager, but instead of the std lib one, a class that wraps it
Overriding the list class
Decorators. Everywhere
Mixins. and the mixins have dead branches
A class that inherits another class, which inherits three classes
Surprising branch that downloads 30b
------
Unrecoverable errors that instead, just mutate to partially recover, and instead, lurkily fail downstream after spitting out a warning log (if you're lucky)
Class implements methods, and has those methods throw errors when not existent. Meaning misimplement are caught at runtime
------
this library is extremely bad if you are trying to do anything serious. And probably still bad if you are trying to do anything not serious

Use llamacpp if inference only, write it from scratch if you're doing training. You will thank yourself
------
mfs will get hired for safety alignment research and just figure out how to make sure the AI does actually bad things
------
Dooo you have the time
To listen to me whine?
------
mfs get a billion dollars in investment to make something that replaces your spouse, and then write a book about how open source is bad
------
they are not even trying lmao
------
upper middle class liberal bros... we messed up..
------
Blue states need to enact permitting reform if they want a larger share of the clean energy investment boom.

Vast majority of the money is currently going to red states and districts:
------
Brain drain is a bullshit concept because people are not the private property of their countries
------
it's funny how the brain drain arguments never consider the migrants themselves
------
Just putting out Linus's rant about useless abstractions and modularisation here.
Make good interfaces, don't blindly modularise everything.
------
abstractions are globalist and fascist

duplicate code is localism, it's about freedom. it's respecting your peers who have to modify your code one day

you're gonna make them refactor the whole codebase bc they added a function argument? test stuff they don't even know exists?
------
if there are any people here, looking for advice
the one thing I can tell you:
please duplicate code
do not share code
prefer duplication over abstraction, always, unless its a primtive
pic related
why is a lurky Bloom branch on LLaMa?
1.4k lines btw
------
how many followers do I need to get a hanania style character assassination which I can strategically turn into more substack subs?
------
can't believe people are posting on Reddit, instagram and 4chan for absolutely free. I'm literally making money here. I'm never going to post on a site that doesn't pay me again
------
writing 10,000 more lines of C++ for high frequency trading systems will fix me. this time it will work
------
i have a single file with no huggingface dependencies that both loads & forward passes llama2 (god that took a while)
this is after a lot of pruning
using bits&bytes & accelerate
going to continue, i've got some time
welcome to the debloating waiting room
------
alright throwing in the towel
final line count is 1281

why... did i do this
------
it actually is fubar
lmao
------
LLM Self-Alignment w/ Backtranslation

-Fine-tune LLM on small amount of seed data
-Use it to generate instruction prompts (self-augmentation) on unlabelled data & select best (self-curatation)
-Finetune on that

-Outperforms all other LLaMa-based models

https://arxiv.org/abs/2308.06259
------
I just want to know why. Why are you fucking me and my library. Why are you ruining my software tell me. Tell me you son of a bitch. Why are you fucking me like this.
------
i only want one thing...
------
I WILL NOT USE YOUR WORDS
I WILL USE EXTREMELY SPECIFIC TECHNICAL LANGUAGE
I WILL NOT BE CONTROLLED 
I WILL SIMPLY EXPIRE TO THE ANNALS OF TIME
AS A FREE RADICAL
------
being “smart” is not the same as having good ideas. there’s tons of smart grinders languishing rn
------
my toxic trait is thinking sama actually really cares about AI risk, and carefully moving forward. like, that his heart is in the right place
------
idk i have interacted with enough whiteboy mega nerd super billionaire (either in spirituality or in real money) types to know that he's for real
------
banning open source AI is unethical
------
See? I can use moral blackmail to further my own agenda. For your agenda, its creating a regulatory environment where you have total monopoly and use it to gain power over everyone else. My agenda, is being able to build tools to help my government, small business and community
------
Keeping AI as closed source is unethical. It could delay crucial inventions like curing diseases and solving climate change. By sharing AI as open source, we can solve these problems faster and better.
------
kwargs
args
Context manager, but instead of the std lib one, a class that wraps it
Overriding the list class
Decorators. Everywhere
Mixins. and the mixins have dead branches
A class that inherits another class, which inherits three classes
Surprising branch that downloads 30b
------
Unrecoverable errors that instead, just mutate to partially recover, and instead, lurkily fail downstream after spitting out a warning log (if you're lucky)
Class implements methods, and has those methods throw errors when not existent. Meaning misimplement are caught at runtime
------
this library is extremely bad if you are trying to do anything serious. And probably still bad if you are trying to do anything not serious

Use llamacpp if inference only, write it from scratch if you're doing training. You will thank yourself
------
mfs will get hired for safety alignment research and just figure out how to make sure the AI does actually bad things
------
Dooo you have the time
To listen to me whine?
------
mfs get a billion dollars in investment to make something that replaces your spouse, and then write a book about how open source is bad
------
they are not even trying lmao
------
upper middle class liberal bros... we messed up..
------
Blue states need to enact permitting reform if they want a larger share of the clean energy investment boom.

Vast majority of the money is currently going to red states and districts:
------
Brain drain is a bullshit concept because people are not the private property of their countries
------
it's funny how the brain drain arguments never consider the migrants themselves
------
Just putting out Linus's rant about useless abstractions and modularisation here.
Make good interfaces, don't blindly modularise everything.
------
abstractions are globalist and fascist

duplicate code is localism, it's about freedom. it's respecting your peers who have to modify your code one day

you're gonna make them refactor the whole codebase bc they added a function argument? test stuff they don't even know exists?
------
if there are any people here, looking for advice
the one thing I can tell you:
please duplicate code
do not share code
prefer duplication over abstraction, always, unless its a primtive
pic related
why is a lurky Bloom branch on LLaMa?
1.4k lines btw
------
how many followers do I need to get a hanania style character assassination which I can strategically turn into more substack subs?
------
can't believe people are posting on Reddit, instagram and 4chan for absolutely free. I'm literally making money here. I'm never going to post on a site that doesn't pay me again
------
writing 10,000 more lines of C++ for high frequency trading systems will fix me. this time it will work
------
i have a single file with no huggingface dependencies that both loads & forward passes llama2 (god that took a while)
this is after a lot of pruning
using bits&bytes & accelerate
going to continue, i've got some time
welcome to the debloating waiting room
------
alright throwing in the towel
final line count is 1281

why... did i do this
------
it actually is fubar
lmao
------
LLM Self-Alignment w/ Backtranslation

-Fine-tune LLM on small amount of seed data
-Use it to generate instruction prompts (self-augmentation) on unlabelled data & select best (self-curatation)
-Finetune on that

-Outperforms all other LLaMa-based models

https://arxiv.org/abs/2308.06259
------
I just want to know why. Why are you fucking me and my library. Why are you ruining my software tell me. Tell me you son of a bitch. Why are you fucking me like this.
------
if there are any people here, looking for advice
the one thing I can tell you:
please duplicate code
do not share code
prefer duplication over abstraction, always, unless its a primtive
pic related
why is a lurky Bloom branch on LLaMa?
1.4k lines btw
------
Another complaint
This whole fn is an affront to simplicity. this could just be a loop. 
Why is the state being mutated in global? (yes, the Generator Mixed In object is effectively a global)
This changes behavior in totally disparate places! And this is from a mixin
(1.2k lines)
------
I’ve been running a few experiments I like to call LobotoLLaMA: 

Basically, I found that you can remove layers from LLaMA (i.e. neural lobotomy) and it’ll still work pretty well (cont.)
------
sorry that happened, defcon bros :[
------
kwargs considered harmful, by the way
------
How to share http://arxiv.org papers 
------
everything i do is automatable
------
it can all be reduced down to a discrete set of actions, which can be done individually, by a machine, much better than I could
------
kwargs
args
Context manager, but instead of the std lib one, a class that wraps it
Overriding the list class
Decorators. Everywhere
Mixins. and the mixins have dead branches
A class that inherits another class, which inherits three classes
Surprising branch that downloads 30b
------
Unrecoverable errors that instead, just mutate to partially recover, and instead, lurkily fail downstream after spitting out a warning log (if you're lucky)
Class implements methods, and has those methods throw errors when not existent. Meaning misimplement are caught at runtime
------
this library is extremely bad if you are trying to do anything serious. And probably still bad if you are trying to do anything not serious

Use llamacpp if inference only, write it from scratch if you're doing training. You will thank yourself
------
mfs will get hired for safety alignment research and just figure out how to make sure the AI does actually bad things
------
Dooo you have the time
To listen to me whine?
------
mfs get a billion dollars in investment to make something that replaces your spouse, and then write a book about how open source is bad
------
they are not even trying lmao
------
upper middle class liberal bros... we messed up..
------
Blue states need to enact permitting reform if they want a larger share of the clean energy investment boom.

Vast majority of the money is currently going to red states and districts:
------
Brain drain is a bullshit concept because people are not the private property of their countries
------
it's funny how the brain drain arguments never consider the migrants themselves
------
Just putting out Linus's rant about useless abstractions and modularisation here.
Make good interfaces, don't blindly modularise everything.
------
abstractions are globalist and fascist

duplicate code is localism, it's about freedom. it's respecting your peers who have to modify your code one day

you're gonna make them refactor the whole codebase bc they added a function argument? test stuff they don't even know exists?
------
if there are any people here, looking for advice
the one thing I can tell you:
please duplicate code
do not share code
prefer duplication over abstraction, always, unless its a primtive
pic related
why is a lurky Bloom branch on LLaMa?
1.4k lines btw
------
how many followers do I need to get a hanania style character assassination which I can strategically turn into more substack subs?
------
can't believe people are posting on Reddit, instagram and 4chan for absolutely free. I'm literally making money here. I'm never going to post on a site that doesn't pay me again
------
writing 10,000 more lines of C++ for high frequency trading systems will fix me. this time it will work
------
i have a single file with no huggingface dependencies that both loads & forward passes llama2 (god that took a while)
this is after a lot of pruning
using bits&bytes & accelerate
going to continue, i've got some time
welcome to the debloating waiting room
------
alright throwing in the towel
final line count is 1281

why... did i do this
------
it actually is fubar
lmao
------
LLM Self-Alignment w/ Backtranslation

-Fine-tune LLM on small amount of seed data
-Use it to generate instruction prompts (self-augmentation) on unlabelled data & select best (self-curatation)
-Finetune on that

-Outperforms all other LLaMa-based models

https://arxiv.org/abs/2308.06259
------
I just want to know why. Why are you fucking me and my library. Why are you ruining my software tell me. Tell me you son of a bitch. Why are you fucking me like this.
------
if there are any people here, looking for advice
the one thing I can tell you:
please duplicate code
do not share code
prefer duplication over abstraction, always, unless its a primtive
pic related
why is a lurky Bloom branch on LLaMa?
1.4k lines btw
------
Another complaint
This whole fn is an affront to simplicity. this could just be a loop. 
Why is the state being mutated in global? (yes, the Generator Mixed In object is effectively a global)
This changes behavior in totally disparate places! And this is from a mixin
(1.2k lines)
------
I’ve been running a few experiments I like to call LobotoLLaMA: 

Basically, I found that you can remove layers from LLaMA (i.e. neural lobotomy) and it’ll still work pretty well (cont.)
------
sorry that happened, defcon bros :[
------
kwargs considered harmful, by the way
------
How to share http://arxiv.org papers 
------
everything i do is automatable
------
it can all be reduced down to a discrete set of actions, which can be done individually, by a machine, much better than I could
------
Sam is crushing you guys at AI. Ship something better and then you can make fun of him.
------
A video on AI recipe bots, dangerous chemicals, and how people will share just about anything that agrees with their opinion without reading it:
https://youtu.be/BMAu7hAcjqU
(bonus points for sharing this without first watching)
------
"I want an unbiased God Manifold AGI that I can fine-tune to do anything, and I want it to run on a potato" -
@yacineMTB
------
I want a God Manifold, fit to 30 billion parameters. Something I can shape cheaply, to do bad, to do good. I want to be able to run it on a potato
------
https://twitter.com/i/spaces/1vOxwMrALYNGB…
------
Brain drain is a bullshit concept because people are not the private property of their countries
------
it's funny how the brain drain arguments never consider the migrants themselves
------
Just putting out Linus's rant about useless abstractions and modularisation here.
Make good interfaces, don't blindly modularise everything.
------
abstractions are globalist and fascist

duplicate code is localism, it's about freedom. it's respecting your peers who have to modify your code one day

you're gonna make them refactor the whole codebase bc they added a function argument? test stuff they don't even know exists?
------
if there are any people here, looking for advice
the one thing I can tell you:
please duplicate code
do not share code
prefer duplication over abstraction, always, unless its a primtive
pic related
why is a lurky Bloom branch on LLaMa?
1.4k lines btw
------
how many followers do I need to get a hanania style character assassination which I can strategically turn into more substack subs?
------
can't believe people are posting on Reddit, instagram and 4chan for absolutely free. I'm literally making money here. I'm never going to post on a site that doesn't pay me again
------
writing 10,000 more lines of C++ for high frequency trading systems will fix me. this time it will work
------
i have a single file with no huggingface dependencies that both loads & forward passes llama2 (god that took a while)
this is after a lot of pruning
using bits&bytes & accelerate
going to continue, i've got some time
welcome to the debloating waiting room
------
alright throwing in the towel
final line count is 1281

why... did i do this
------
it actually is fubar
lmao
------
LLM Self-Alignment w/ Backtranslation

-Fine-tune LLM on small amount of seed data
-Use it to generate instruction prompts (self-augmentation) on unlabelled data & select best (self-curatation)
-Finetune on that

-Outperforms all other LLaMa-based models

https://arxiv.org/abs/2308.06259
------
I just want to know why. Why are you fucking me and my library. Why are you ruining my software tell me. Tell me you son of a bitch. Why are you fucking me like this.
------
if there are any people here, looking for advice
the one thing I can tell you:
please duplicate code
do not share code
prefer duplication over abstraction, always, unless its a primtive
pic related
why is a lurky Bloom branch on LLaMa?
1.4k lines btw
------
Another complaint
This whole fn is an affront to simplicity. this could just be a loop. 
Why is the state being mutated in global? (yes, the Generator Mixed In object is effectively a global)
This changes behavior in totally disparate places! And this is from a mixin
(1.2k lines)
------
I’ve been running a few experiments I like to call LobotoLLaMA: 

Basically, I found that you can remove layers from LLaMA (i.e. neural lobotomy) and it’ll still work pretty well (cont.)
------
sorry that happened, defcon bros :[
------
kwargs considered harmful, by the way
------
How to share http://arxiv.org papers 
------
everything i do is automatable
------
it can all be reduced down to a discrete set of actions, which can be done individually, by a machine, much better than I could
------
Sam is crushing you guys at AI. Ship something better and then you can make fun of him.
------
A video on AI recipe bots, dangerous chemicals, and how people will share just about anything that agrees with their opinion without reading it:
https://youtu.be/BMAu7hAcjqU
(bonus points for sharing this without first watching)
------
"I want an unbiased God Manifold AGI that I can fine-tune to do anything, and I want it to run on a potato" -
@yacineMTB
------
I want a God Manifold, fit to 30 billion parameters. Something I can shape cheaply, to do bad, to do good. I want to be able to run it on a potato
------
https://twitter.com/i/spaces/1vOxwMrALYNGB…
------
the sooner CBC gets wiped from the face of the earth, the better. They do not represent Muslim Canadians, and rural Canadians. It's a cultural ice box, where they attempt to melt people in. Remove their distributional capacity. Replace them with Canadian independent writers.
------
the two kinds of AI experts
------
the huggingface transformers library is really bad
------
in terms of code quality
there really is no other way to say it
------
it's not just that we're creating AGI 
AGI is going to run on personal computers
I don't know when that bit flip happened for me
but it's pretty apparent that they will
what does that world even look like
god
------
bullish on AGI being discovered in someone's basement
------
like a better version of the AGI that we have today
------
on mushrooms, drunk, on mushrooms
------
pic related its me removing 98% of the code from the dependency
------
i have a single file with no huggingface dependencies that both loads & forward passes llama2 (god that took a while)
this is after a lot of pruning
using bits&bytes & accelerate
going to continue, i've got some time
welcome to the debloating waiting room
------
alright throwing in the towel
final line count is 1281

why... did i do this
------
it actually is fubar
lmao
------
LLM Self-Alignment w/ Backtranslation

-Fine-tune LLM on small amount of seed data
-Use it to generate instruction prompts (self-augmentation) on unlabelled data & select best (self-curatation)
-Finetune on that

-Outperforms all other LLaMa-based models

https://arxiv.org/abs/2308.06259
------
I just want to know why. Why are you fucking me and my library. Why are you ruining my software tell me. Tell me you son of a bitch. Why are you fucking me like this.
------
if there are any people here, looking for advice
the one thing I can tell you:
please duplicate code
do not share code
prefer duplication over abstraction, always, unless its a primtive
pic related
why is a lurky Bloom branch on LLaMa?
1.4k lines btw
------
Another complaint
This whole fn is an affront to simplicity. this could just be a loop. 
Why is the state being mutated in global? (yes, the Generator Mixed In object is effectively a global)
This changes behavior in totally disparate places! And this is from a mixin
(1.2k lines)
------
I’ve been running a few experiments I like to call LobotoLLaMA: 

Basically, I found that you can remove layers from LLaMA (i.e. neural lobotomy) and it’ll still work pretty well (cont.)
------
sorry that happened, defcon bros :[
------
kwargs considered harmful, by the way
------
How to share http://arxiv.org papers 
------
everything i do is automatable
------
it can all be reduced down to a discrete set of actions, which can be done individually, by a machine, much better than I could
------
Sam is crushing you guys at AI. Ship something better and then you can make fun of him.
------
A video on AI recipe bots, dangerous chemicals, and how people will share just about anything that agrees with their opinion without reading it:
https://youtu.be/BMAu7hAcjqU
(bonus points for sharing this without first watching)
------
"I want an unbiased God Manifold AGI that I can fine-tune to do anything, and I want it to run on a potato" -
@yacineMTB
------
I want a God Manifold, fit to 30 billion parameters. Something I can shape cheaply, to do bad, to do good. I want to be able to run it on a potato
------
https://twitter.com/i/spaces/1vOxwMrALYNGB…
------
the sooner CBC gets wiped from the face of the earth, the better. They do not represent Muslim Canadians, and rural Canadians. It's a cultural ice box, where they attempt to melt people in. Remove their distributional capacity. Replace them with Canadian independent writers.
------
the two kinds of AI experts
------
the huggingface transformers library is really bad
------
in terms of code quality
there really is no other way to say it
------
it's not just that we're creating AGI 
AGI is going to run on personal computers
I don't know when that bit flip happened for me
but it's pretty apparent that they will
what does that world even look like
god
------
bullish on AGI being discovered in someone's basement
------
like a better version of the AGI that we have today
------
on mushrooms, drunk, on mushrooms
------
pic related its me removing 98% of the code from the dependency
------
five go to defs later
------
holy shit this is AGI
------
it actually has no name, but it eventually gave me a google search that yielded it

Also.. how does it know what it looks like?
seriously?
------
G factor, or IQ, is likely the only thing that really matters for overall positive outcomes
------
LLM Self-Alignment w/ Backtranslation

-Fine-tune LLM on small amount of seed data
-Use it to generate instruction prompts (self-augmentation) on unlabelled data & select best (self-curatation)
-Finetune on that

-Outperforms all other LLaMa-based models

https://arxiv.org/abs/2308.06259
------
I just want to know why. Why are you fucking me and my library. Why are you ruining my software tell me. Tell me you son of a bitch. Why are you fucking me like this.
------
if there are any people here, looking for advice
the one thing I can tell you:
please duplicate code
do not share code
prefer duplication over abstraction, always, unless its a primtive
pic related
why is a lurky Bloom branch on LLaMa?
1.4k lines btw
------
Another complaint
This whole fn is an affront to simplicity. this could just be a loop. 
Why is the state being mutated in global? (yes, the Generator Mixed In object is effectively a global)
This changes behavior in totally disparate places! And this is from a mixin
(1.2k lines)
------
I’ve been running a few experiments I like to call LobotoLLaMA: 

Basically, I found that you can remove layers from LLaMA (i.e. neural lobotomy) and it’ll still work pretty well (cont.)
------
sorry that happened, defcon bros :[
------
kwargs considered harmful, by the way
------
How to share http://arxiv.org papers 
------
everything i do is automatable
------
it can all be reduced down to a discrete set of actions, which can be done individually, by a machine, much better than I could
------
Sam is crushing you guys at AI. Ship something better and then you can make fun of him.
------
A video on AI recipe bots, dangerous chemicals, and how people will share just about anything that agrees with their opinion without reading it:
https://youtu.be/BMAu7hAcjqU
(bonus points for sharing this without first watching)
------
"I want an unbiased God Manifold AGI that I can fine-tune to do anything, and I want it to run on a potato" -
@yacineMTB
------
I want a God Manifold, fit to 30 billion parameters. Something I can shape cheaply, to do bad, to do good. I want to be able to run it on a potato
------
https://twitter.com/i/spaces/1vOxwMrALYNGB…
------
the sooner CBC gets wiped from the face of the earth, the better. They do not represent Muslim Canadians, and rural Canadians. It's a cultural ice box, where they attempt to melt people in. Remove their distributional capacity. Replace them with Canadian independent writers.
------
the two kinds of AI experts
------
the huggingface transformers library is really bad
------
in terms of code quality
there really is no other way to say it
------
it's not just that we're creating AGI 
AGI is going to run on personal computers
I don't know when that bit flip happened for me
but it's pretty apparent that they will
what does that world even look like
god
------
bullish on AGI being discovered in someone's basement
------
like a better version of the AGI that we have today
------
on mushrooms, drunk, on mushrooms
------
pic related its me removing 98% of the code from the dependency
------
five go to defs later
------
holy shit this is AGI
------
it actually has no name, but it eventually gave me a google search that yielded it

Also.. how does it know what it looks like?
seriously?
------
G factor, or IQ, is likely the only thing that really matters for overall positive outcomes
------
and it's mostly hereditary
------
please stop throwing errors. the only errors i want to see are system errors caused by solar flares.

rob pike was right
------
"I found that the attention output of layer 24 of the llama 2 transformer consistently represents relevant information related to countries, even when neither the prompt nor the higher probability completions are related to countries"

Interesting!

from
------
I’ve been running a few experiments I like to call LobotoLLaMA: 

Basically, I found that you can remove layers from LLaMA (i.e. neural lobotomy) and it’ll still work pretty well (cont.)
------
sorry that happened, defcon bros :[
------
kwargs considered harmful, by the way
------
How to share http://arxiv.org papers 
------
everything i do is automatable
------
it can all be reduced down to a discrete set of actions, which can be done individually, by a machine, much better than I could
------
Sam is crushing you guys at AI. Ship something better and then you can make fun of him.
------
A video on AI recipe bots, dangerous chemicals, and how people will share just about anything that agrees with their opinion without reading it:
https://youtu.be/BMAu7hAcjqU
(bonus points for sharing this without first watching)
------
"I want an unbiased God Manifold AGI that I can fine-tune to do anything, and I want it to run on a potato" -
@yacineMTB
------
I want a God Manifold, fit to 30 billion parameters. Something I can shape cheaply, to do bad, to do good. I want to be able to run it on a potato
------
https://twitter.com/i/spaces/1vOxwMrALYNGB…
------
the sooner CBC gets wiped from the face of the earth, the better. They do not represent Muslim Canadians, and rural Canadians. It's a cultural ice box, where they attempt to melt people in. Remove their distributional capacity. Replace them with Canadian independent writers.
------
the two kinds of AI experts
------
the huggingface transformers library is really bad
------
in terms of code quality
there really is no other way to say it
------
it's not just that we're creating AGI 
AGI is going to run on personal computers
I don't know when that bit flip happened for me
but it's pretty apparent that they will
what does that world even look like
god
------
bullish on AGI being discovered in someone's basement
------
like a better version of the AGI that we have today
------
on mushrooms, drunk, on mushrooms
------
pic related its me removing 98% of the code from the dependency
------
five go to defs later
------
holy shit this is AGI
------
it actually has no name, but it eventually gave me a google search that yielded it

Also.. how does it know what it looks like?
seriously?
------
G factor, or IQ, is likely the only thing that really matters for overall positive outcomes
------
and it's mostly hereditary
------
please stop throwing errors. the only errors i want to see are system errors caused by solar flares.

rob pike was right
------
"I found that the attention output of layer 24 of the llama 2 transformer consistently represents relevant information related to countries, even when neither the prompt nor the higher probability completions are related to countries"

Interesting!

from
------
also, hello 
@NinaRimsky
! pleasantly surprised to see you there, small world!
------
mfs will add "only steal from the best artists" to their image generation prompts
------
holy shit 
------
It’s a joke that pits four different Theories of Everything (each developed by a nerd with a planet sized intellectual self confidence) against each other as if they were monstrous universe filling titans. This works hopefully somewhat because all four theories happen to predict… Show more
------
This is what I been saying, fine tuning is gonna continue to be a thing - and task specific models will be very capable. Found this recently published from Roblox
------
Being a good programmer is very important for ML. I don’t care what anyone says.

Writing code that gets working quickly, is easily extensible/hackable, useable by others (!!!), and is readable enough to return to after a break is crucial.

Anyone who says otherwise is coping.
------
huggingface transformers? the only thing needing transforming is all that bloat
------
follow 
@alisabets
 I stole this tweet from a dm
------
A video on AI recipe bots, dangerous chemicals, and how people will share just about anything that agrees with their opinion without reading it:
https://youtu.be/BMAu7hAcjqU
(bonus points for sharing this without first watching)
------
"I want an unbiased God Manifold AGI that I can fine-tune to do anything, and I want it to run on a potato" -
@yacineMTB
------
I want a God Manifold, fit to 30 billion parameters. Something I can shape cheaply, to do bad, to do good. I want to be able to run it on a potato
------
https://twitter.com/i/spaces/1vOxwMrALYNGB…
------
the sooner CBC gets wiped from the face of the earth, the better. They do not represent Muslim Canadians, and rural Canadians. It's a cultural ice box, where they attempt to melt people in. Remove their distributional capacity. Replace them with Canadian independent writers.
------
the two kinds of AI experts
------
the huggingface transformers library is really bad
------
in terms of code quality
there really is no other way to say it
------
it's not just that we're creating AGI 
AGI is going to run on personal computers
I don't know when that bit flip happened for me
but it's pretty apparent that they will
what does that world even look like
god
------
bullish on AGI being discovered in someone's basement
------
like a better version of the AGI that we have today
------
on mushrooms, drunk, on mushrooms
------
pic related its me removing 98% of the code from the dependency
------
five go to defs later
------
holy shit this is AGI
------
it actually has no name, but it eventually gave me a google search that yielded it

Also.. how does it know what it looks like?
seriously?
------
G factor, or IQ, is likely the only thing that really matters for overall positive outcomes
------
and it's mostly hereditary
------
please stop throwing errors. the only errors i want to see are system errors caused by solar flares.

rob pike was right
------
"I found that the attention output of layer 24 of the llama 2 transformer consistently represents relevant information related to countries, even when neither the prompt nor the higher probability completions are related to countries"

Interesting!

from
------
also, hello 
@NinaRimsky
! pleasantly surprised to see you there, small world!
------
mfs will add "only steal from the best artists" to their image generation prompts
------
holy shit 
------
It’s a joke that pits four different Theories of Everything (each developed by a nerd with a planet sized intellectual self confidence) against each other as if they were monstrous universe filling titans. This works hopefully somewhat because all four theories happen to predict… Show more
------
This is what I been saying, fine tuning is gonna continue to be a thing - and task specific models will be very capable. Found this recently published from Roblox
------
Being a good programmer is very important for ML. I don’t care what anyone says.

Writing code that gets working quickly, is easily extensible/hackable, useable by others (!!!), and is readable enough to return to after a break is crucial.

Anyone who says otherwise is coping.
------
huggingface transformers? the only thing needing transforming is all that bloat
------
follow 
@alisabets
 I stole this tweet from a dm
------
btw i am not giving up just thought you should know
------
ceo/founders are playing such a hard game. trying to engineer incentives, fleshy beings, as well as the computer system, to survive and thrive in the chaos of the world

one common thing I've noticed: a relentless vigor for success
------
actually don't know if i could model that behavior
------
I saw PG at a grocery store in Palo Alto yesterday. I told him how cool it was to meet him in person, but I didn’t want to be a douche and bother him and ask him for photos or anything.

He said, “Oh, like you’re doing now?”

I was taken aback, and all I could say was “Huh?” but… Show more
------
Was walking down the street in Palo Alto and ran into a founder in the current batch, and we cooked up a way to get network effects in his product.
------
if you're confused, this is a copypasta from 2013 about meeting heros and having your expectations subverted
------
Everything done will pay respect to the past and present of Italy
------
after getting a tiny bit older
In tech, the reason new hires are young isn't because of ageism: it's because anyone with >10 years of experience is almost impossible to hire

someone who has been the game that long is literally god. they might hire you, actually
------
in terms of code quality
there really is no other way to say it
------
it's not just that we're creating AGI 
AGI is going to run on personal computers
I don't know when that bit flip happened for me
but it's pretty apparent that they will
what does that world even look like
god
------
bullish on AGI being discovered in someone's basement
------
like a better version of the AGI that we have today
------
on mushrooms, drunk, on mushrooms
------
pic related its me removing 98% of the code from the dependency
------
five go to defs later
------
holy shit this is AGI
------
it actually has no name, but it eventually gave me a google search that yielded it

Also.. how does it know what it looks like?
seriously?
------
G factor, or IQ, is likely the only thing that really matters for overall positive outcomes
------
and it's mostly hereditary
------
please stop throwing errors. the only errors i want to see are system errors caused by solar flares.

rob pike was right
------
"I found that the attention output of layer 24 of the llama 2 transformer consistently represents relevant information related to countries, even when neither the prompt nor the higher probability completions are related to countries"

Interesting!

from
------
also, hello 
@NinaRimsky
! pleasantly surprised to see you there, small world!
------
mfs will add "only steal from the best artists" to their image generation prompts
------
holy shit 
------
It’s a joke that pits four different Theories of Everything (each developed by a nerd with a planet sized intellectual self confidence) against each other as if they were monstrous universe filling titans. This works hopefully somewhat because all four theories happen to predict… Show more
------
This is what I been saying, fine tuning is gonna continue to be a thing - and task specific models will be very capable. Found this recently published from Roblox
------
Being a good programmer is very important for ML. I don’t care what anyone says.

Writing code that gets working quickly, is easily extensible/hackable, useable by others (!!!), and is readable enough to return to after a break is crucial.

Anyone who says otherwise is coping.
------
huggingface transformers? the only thing needing transforming is all that bloat
------
follow 
@alisabets
 I stole this tweet from a dm
------
btw i am not giving up just thought you should know
------
ceo/founders are playing such a hard game. trying to engineer incentives, fleshy beings, as well as the computer system, to survive and thrive in the chaos of the world

one common thing I've noticed: a relentless vigor for success
------
actually don't know if i could model that behavior
------
I saw PG at a grocery store in Palo Alto yesterday. I told him how cool it was to meet him in person, but I didn’t want to be a douche and bother him and ask him for photos or anything.

He said, “Oh, like you’re doing now?”

I was taken aback, and all I could say was “Huh?” but… Show more
------
Was walking down the street in Palo Alto and ran into a founder in the current batch, and we cooked up a way to get network effects in his product.
------
if you're confused, this is a copypasta from 2013 about meeting heros and having your expectations subverted
------
Everything done will pay respect to the past and present of Italy
------
after getting a tiny bit older
In tech, the reason new hires are young isn't because of ageism: it's because anyone with >10 years of experience is almost impossible to hire

someone who has been the game that long is literally god. they might hire you, actually
------
college is a dating app with the "upper middle class" preference option selected
------
mfs will have Pixie Girl Poo Poo Anime Girl profile pictures online, and deadlift 600 pounds offline
------
man the perf on the open 70b models is getting dangerously close to GPT4 on some benchmarks
------
LoRA trained models too..
------
Towards a Unified View of Parameter-Efficient Transfer Learning
https://arxiv.org/abs/2110.04366
good paper
------
pic related its me removing 98% of the code from the dependency
------
five go to defs later
------
holy shit this is AGI
------
it actually has no name, but it eventually gave me a google search that yielded it

Also.. how does it know what it looks like?
seriously?
------
G factor, or IQ, is likely the only thing that really matters for overall positive outcomes
------
and it's mostly hereditary
------
please stop throwing errors. the only errors i want to see are system errors caused by solar flares.

rob pike was right
------
"I found that the attention output of layer 24 of the llama 2 transformer consistently represents relevant information related to countries, even when neither the prompt nor the higher probability completions are related to countries"

Interesting!

from
------
also, hello 
@NinaRimsky
! pleasantly surprised to see you there, small world!
------
mfs will add "only steal from the best artists" to their image generation prompts
------
holy shit 
------
It’s a joke that pits four different Theories of Everything (each developed by a nerd with a planet sized intellectual self confidence) against each other as if they were monstrous universe filling titans. This works hopefully somewhat because all four theories happen to predict… Show more
------
This is what I been saying, fine tuning is gonna continue to be a thing - and task specific models will be very capable. Found this recently published from Roblox
------
Being a good programmer is very important for ML. I don’t care what anyone says.

Writing code that gets working quickly, is easily extensible/hackable, useable by others (!!!), and is readable enough to return to after a break is crucial.

Anyone who says otherwise is coping.
------
huggingface transformers? the only thing needing transforming is all that bloat
------
follow 
@alisabets
 I stole this tweet from a dm
------
btw i am not giving up just thought you should know
------
ceo/founders are playing such a hard game. trying to engineer incentives, fleshy beings, as well as the computer system, to survive and thrive in the chaos of the world

one common thing I've noticed: a relentless vigor for success
------
actually don't know if i could model that behavior
------
I saw PG at a grocery store in Palo Alto yesterday. I told him how cool it was to meet him in person, but I didn’t want to be a douche and bother him and ask him for photos or anything.

He said, “Oh, like you’re doing now?”

I was taken aback, and all I could say was “Huh?” but… Show more
------
Was walking down the street in Palo Alto and ran into a founder in the current batch, and we cooked up a way to get network effects in his product.
------
if you're confused, this is a copypasta from 2013 about meeting heros and having your expectations subverted
------
Everything done will pay respect to the past and present of Italy
------
after getting a tiny bit older
In tech, the reason new hires are young isn't because of ageism: it's because anyone with >10 years of experience is almost impossible to hire

someone who has been the game that long is literally god. they might hire you, actually
------
college is a dating app with the "upper middle class" preference option selected
------
mfs will have Pixie Girl Poo Poo Anime Girl profile pictures online, and deadlift 600 pounds offline
------
man the perf on the open 70b models is getting dangerously close to GPT4 on some benchmarks
------
LoRA trained models too..
------
Towards a Unified View of Parameter-Efficient Transfer Learning
https://arxiv.org/abs/2110.04366
good paper
------
he's... he's.... he's literally me
------
This is an ape ("Kanzi") playing Minecraft! A fascinating experiment on non-human biological neural networks 

I've been teaching AI to play Minecraft for too long. There're so many similar techniques that the ape trainers used:

- In-context reinforcement learning: Kanzi gets… Show more
------
"We're all gonna f*cking die from [insert new tech]" ---> please purchase my publication/book/TED talk pipeline.

Fear sells. AI safety grifters are aligned in interest with media orgs that will amplify their message for clicks. Remember that.
------
This is novel software engineering, for an AI world.
e.g. building systems and algorithms that clean data, on top of new primitives
this is what excites me about this all! 
map reduce, but with LLMs!
------
This was a fun one:
https://github.com/jondurbin/airoboros/commit/2632489e02a70f2fde462783bcd6a3a78667eb55…

`cull-instructions` entrypoint for airoboros to merge datasets, remove duplicates, and use LLM to score and filter incorrect/low quality answers, AALLMs, etc.

Slow, pricey, but very useful for improving quality.
------
i should look up algorithms that have humans as a component, probably from the 70s
probably lots of goodies there to apply with LLMs
------
no one knows what happens next
------
I'm more of a "you don’t have to be responsible for the world that you’re in" kinda guy (this was von Neumann to Feynman)
------
my e/acc > e/a preference is mostly a deep aversion for making long term predictions for second order effects. usually, borne out of hubris

yes, theoretically, the double pendulum could swing that way, but let's really worry about the first pendulum
------
our predictability horizon for chaotic systems is shorter than people tend to think
------
(warning, strawman violation) 

amusingly, the lesswrongians' preference for bayes theorem shows this violation. they are stacking causal models on top of each other, sky high like a jenga tower. 

humans have a tendency to underestimate the incompleteness in our priors. collapse
------
expectation: 10xing my output with gpt4
reality: feeling 10x the guilt for not being productive, because the opportunity cost just increased by an order of magnitude
------
G factor, or IQ, is likely the only thing that really matters for overall positive outcomes
------
and it's mostly hereditary
------
please stop throwing errors. the only errors i want to see are system errors caused by solar flares.

rob pike was right
------
"I found that the attention output of layer 24 of the llama 2 transformer consistently represents relevant information related to countries, even when neither the prompt nor the higher probability completions are related to countries"

Interesting!

from
------
also, hello 
@NinaRimsky
! pleasantly surprised to see you there, small world!
------
mfs will add "only steal from the best artists" to their image generation prompts
------
holy shit 
------
It’s a joke that pits four different Theories of Everything (each developed by a nerd with a planet sized intellectual self confidence) against each other as if they were monstrous universe filling titans. This works hopefully somewhat because all four theories happen to predict… Show more
------
This is what I been saying, fine tuning is gonna continue to be a thing - and task specific models will be very capable. Found this recently published from Roblox
------
Being a good programmer is very important for ML. I don’t care what anyone says.

Writing code that gets working quickly, is easily extensible/hackable, useable by others (!!!), and is readable enough to return to after a break is crucial.

Anyone who says otherwise is coping.
------
huggingface transformers? the only thing needing transforming is all that bloat
------
follow 
@alisabets
 I stole this tweet from a dm
------
btw i am not giving up just thought you should know
------
ceo/founders are playing such a hard game. trying to engineer incentives, fleshy beings, as well as the computer system, to survive and thrive in the chaos of the world

one common thing I've noticed: a relentless vigor for success
------
actually don't know if i could model that behavior
------
I saw PG at a grocery store in Palo Alto yesterday. I told him how cool it was to meet him in person, but I didn’t want to be a douche and bother him and ask him for photos or anything.

He said, “Oh, like you’re doing now?”

I was taken aback, and all I could say was “Huh?” but… Show more
------
Was walking down the street in Palo Alto and ran into a founder in the current batch, and we cooked up a way to get network effects in his product.
------
if you're confused, this is a copypasta from 2013 about meeting heros and having your expectations subverted
------
Everything done will pay respect to the past and present of Italy
------
after getting a tiny bit older
In tech, the reason new hires are young isn't because of ageism: it's because anyone with >10 years of experience is almost impossible to hire

someone who has been the game that long is literally god. they might hire you, actually
------
college is a dating app with the "upper middle class" preference option selected
------
mfs will have Pixie Girl Poo Poo Anime Girl profile pictures online, and deadlift 600 pounds offline
------
man the perf on the open 70b models is getting dangerously close to GPT4 on some benchmarks
------
LoRA trained models too..
------
Towards a Unified View of Parameter-Efficient Transfer Learning
https://arxiv.org/abs/2110.04366
good paper
------
he's... he's.... he's literally me
------
This is an ape ("Kanzi") playing Minecraft! A fascinating experiment on non-human biological neural networks 

I've been teaching AI to play Minecraft for too long. There're so many similar techniques that the ape trainers used:

- In-context reinforcement learning: Kanzi gets… Show more
------
"We're all gonna f*cking die from [insert new tech]" ---> please purchase my publication/book/TED talk pipeline.

Fear sells. AI safety grifters are aligned in interest with media orgs that will amplify their message for clicks. Remember that.
------
This is novel software engineering, for an AI world.
e.g. building systems and algorithms that clean data, on top of new primitives
this is what excites me about this all! 
map reduce, but with LLMs!
------
This was a fun one:
https://github.com/jondurbin/airoboros/commit/2632489e02a70f2fde462783bcd6a3a78667eb55…

`cull-instructions` entrypoint for airoboros to merge datasets, remove duplicates, and use LLM to score and filter incorrect/low quality answers, AALLMs, etc.

Slow, pricey, but very useful for improving quality.
------
i should look up algorithms that have humans as a component, probably from the 70s
probably lots of goodies there to apply with LLMs
------
no one knows what happens next
------
I'm more of a "you don’t have to be responsible for the world that you’re in" kinda guy (this was von Neumann to Feynman)
------
my e/acc > e/a preference is mostly a deep aversion for making long term predictions for second order effects. usually, borne out of hubris

yes, theoretically, the double pendulum could swing that way, but let's really worry about the first pendulum
------
our predictability horizon for chaotic systems is shorter than people tend to think
------
(warning, strawman violation) 

amusingly, the lesswrongians' preference for bayes theorem shows this violation. they are stacking causal models on top of each other, sky high like a jenga tower. 

humans have a tendency to underestimate the incompleteness in our priors. collapse
------
expectation: 10xing my output with gpt4
reality: feeling 10x the guilt for not being productive, because the opportunity cost just increased by an order of magnitude
------
i just wanna be the abstract super engineer
whose aesthetic preferences save companies untold amounts of money
I wanna be the director that could read and understand a codebase 5 minutes before a meeting
I wanna be jim keller
I wanna predict the future, by building it
------
I've worked with god engineers, the best part about it is that you realize they don't have anything you don't
------
>they don't know that we won't have to work in 5 years
------
In my mid 20s I considered my expected value of independent studying or learning from a side project to be $500/hour and prioritized it accordingly. In retrospect that number may have been too low.
------
please stop liking my dating discourse tweets
------
linkedin is awful, awful, awful
------
By crashing the cost of bullshit, bullshit loses all value
Now, writing will solely be measured the information content, density and novelty
Because writing, well, anyone can bullshit now.
Bullshit is now.. really bullshit

Say more with less
------
Go to Google Scholar and look up ‘As an AI language model” -“ChatGPT”’
------
also, hello 
@NinaRimsky
! pleasantly surprised to see you there, small world!
------
mfs will add "only steal from the best artists" to their image generation prompts
------
holy shit 
------
It’s a joke that pits four different Theories of Everything (each developed by a nerd with a planet sized intellectual self confidence) against each other as if they were monstrous universe filling titans. This works hopefully somewhat because all four theories happen to predict… Show more
------
This is what I been saying, fine tuning is gonna continue to be a thing - and task specific models will be very capable. Found this recently published from Roblox
------
Being a good programmer is very important for ML. I don’t care what anyone says.

Writing code that gets working quickly, is easily extensible/hackable, useable by others (!!!), and is readable enough to return to after a break is crucial.

Anyone who says otherwise is coping.
------
huggingface transformers? the only thing needing transforming is all that bloat
------
follow 
@alisabets
 I stole this tweet from a dm
------
btw i am not giving up just thought you should know
------
ceo/founders are playing such a hard game. trying to engineer incentives, fleshy beings, as well as the computer system, to survive and thrive in the chaos of the world

one common thing I've noticed: a relentless vigor for success
------
actually don't know if i could model that behavior
------
I saw PG at a grocery store in Palo Alto yesterday. I told him how cool it was to meet him in person, but I didn’t want to be a douche and bother him and ask him for photos or anything.

He said, “Oh, like you’re doing now?”

I was taken aback, and all I could say was “Huh?” but… Show more
------
Was walking down the street in Palo Alto and ran into a founder in the current batch, and we cooked up a way to get network effects in his product.
------
if you're confused, this is a copypasta from 2013 about meeting heros and having your expectations subverted
------
Everything done will pay respect to the past and present of Italy
------
after getting a tiny bit older
In tech, the reason new hires are young isn't because of ageism: it's because anyone with >10 years of experience is almost impossible to hire

someone who has been the game that long is literally god. they might hire you, actually
------
college is a dating app with the "upper middle class" preference option selected
------
mfs will have Pixie Girl Poo Poo Anime Girl profile pictures online, and deadlift 600 pounds offline
------
man the perf on the open 70b models is getting dangerously close to GPT4 on some benchmarks
------
LoRA trained models too..
------
Towards a Unified View of Parameter-Efficient Transfer Learning
https://arxiv.org/abs/2110.04366
good paper
------
he's... he's.... he's literally me
------
This is an ape ("Kanzi") playing Minecraft! A fascinating experiment on non-human biological neural networks 

I've been teaching AI to play Minecraft for too long. There're so many similar techniques that the ape trainers used:

- In-context reinforcement learning: Kanzi gets… Show more
------
"We're all gonna f*cking die from [insert new tech]" ---> please purchase my publication/book/TED talk pipeline.

Fear sells. AI safety grifters are aligned in interest with media orgs that will amplify their message for clicks. Remember that.
------
This is novel software engineering, for an AI world.
e.g. building systems and algorithms that clean data, on top of new primitives
this is what excites me about this all! 
map reduce, but with LLMs!
------
This was a fun one:
https://github.com/jondurbin/airoboros/commit/2632489e02a70f2fde462783bcd6a3a78667eb55…

`cull-instructions` entrypoint for airoboros to merge datasets, remove duplicates, and use LLM to score and filter incorrect/low quality answers, AALLMs, etc.

Slow, pricey, but very useful for improving quality.
------
i should look up algorithms that have humans as a component, probably from the 70s
probably lots of goodies there to apply with LLMs
------
no one knows what happens next
------
I'm more of a "you don’t have to be responsible for the world that you’re in" kinda guy (this was von Neumann to Feynman)
------
my e/acc > e/a preference is mostly a deep aversion for making long term predictions for second order effects. usually, borne out of hubris

yes, theoretically, the double pendulum could swing that way, but let's really worry about the first pendulum
------
our predictability horizon for chaotic systems is shorter than people tend to think
------
(warning, strawman violation) 

amusingly, the lesswrongians' preference for bayes theorem shows this violation. they are stacking causal models on top of each other, sky high like a jenga tower. 

humans have a tendency to underestimate the incompleteness in our priors. collapse
------
expectation: 10xing my output with gpt4
reality: feeling 10x the guilt for not being productive, because the opportunity cost just increased by an order of magnitude
------
i just wanna be the abstract super engineer
whose aesthetic preferences save companies untold amounts of money
I wanna be the director that could read and understand a codebase 5 minutes before a meeting
I wanna be jim keller
I wanna predict the future, by building it
------
I've worked with god engineers, the best part about it is that you realize they don't have anything you don't
------
>they don't know that we won't have to work in 5 years
------
In my mid 20s I considered my expected value of independent studying or learning from a side project to be $500/hour and prioritized it accordingly. In retrospect that number may have been too low.
------
please stop liking my dating discourse tweets
------
linkedin is awful, awful, awful
------
By crashing the cost of bullshit, bullshit loses all value
Now, writing will solely be measured the information content, density and novelty
Because writing, well, anyone can bullshit now.
Bullshit is now.. really bullshit

Say more with less
------
Go to Google Scholar and look up ‘As an AI language model” -“ChatGPT”’
------
Career bullshitters are going to whine whine
you see it already with some "artists" and "writers"
instead of mindless information expansion, wasting human cycles for no benefit
art and information will now be judged by its actual utility
not the effort required to produce it
------
the scary thing about the current AI wave is if things were held constant (meaning, no progress at all), everything is still changed forever
------
btw i am not giving up just thought you should know
------
ceo/founders are playing such a hard game. trying to engineer incentives, fleshy beings, as well as the computer system, to survive and thrive in the chaos of the world

one common thing I've noticed: a relentless vigor for success
------
actually don't know if i could model that behavior
------
I saw PG at a grocery store in Palo Alto yesterday. I told him how cool it was to meet him in person, but I didn’t want to be a douche and bother him and ask him for photos or anything.

He said, “Oh, like you’re doing now?”

I was taken aback, and all I could say was “Huh?” but… Show more
------
Was walking down the street in Palo Alto and ran into a founder in the current batch, and we cooked up a way to get network effects in his product.
------
if you're confused, this is a copypasta from 2013 about meeting heros and having your expectations subverted
------
Everything done will pay respect to the past and present of Italy
------
after getting a tiny bit older
In tech, the reason new hires are young isn't because of ageism: it's because anyone with >10 years of experience is almost impossible to hire

someone who has been the game that long is literally god. they might hire you, actually
------
college is a dating app with the "upper middle class" preference option selected
------
mfs will have Pixie Girl Poo Poo Anime Girl profile pictures online, and deadlift 600 pounds offline
------
man the perf on the open 70b models is getting dangerously close to GPT4 on some benchmarks
------
LoRA trained models too..
------
Towards a Unified View of Parameter-Efficient Transfer Learning
https://arxiv.org/abs/2110.04366
good paper
------
he's... he's.... he's literally me
------
This is an ape ("Kanzi") playing Minecraft! A fascinating experiment on non-human biological neural networks 

I've been teaching AI to play Minecraft for too long. There're so many similar techniques that the ape trainers used:

- In-context reinforcement learning: Kanzi gets… Show more
------
"We're all gonna f*cking die from [insert new tech]" ---> please purchase my publication/book/TED talk pipeline.

Fear sells. AI safety grifters are aligned in interest with media orgs that will amplify their message for clicks. Remember that.
------
This is novel software engineering, for an AI world.
e.g. building systems and algorithms that clean data, on top of new primitives
this is what excites me about this all! 
map reduce, but with LLMs!
------
This was a fun one:
https://github.com/jondurbin/airoboros/commit/2632489e02a70f2fde462783bcd6a3a78667eb55…

`cull-instructions` entrypoint for airoboros to merge datasets, remove duplicates, and use LLM to score and filter incorrect/low quality answers, AALLMs, etc.

Slow, pricey, but very useful for improving quality.
------
i should look up algorithms that have humans as a component, probably from the 70s
probably lots of goodies there to apply with LLMs
------
no one knows what happens next
------
I'm more of a "you don’t have to be responsible for the world that you’re in" kinda guy (this was von Neumann to Feynman)
------
my e/acc > e/a preference is mostly a deep aversion for making long term predictions for second order effects. usually, borne out of hubris

yes, theoretically, the double pendulum could swing that way, but let's really worry about the first pendulum
------
our predictability horizon for chaotic systems is shorter than people tend to think
------
(warning, strawman violation) 

amusingly, the lesswrongians' preference for bayes theorem shows this violation. they are stacking causal models on top of each other, sky high like a jenga tower. 

humans have a tendency to underestimate the incompleteness in our priors. collapse
------
expectation: 10xing my output with gpt4
reality: feeling 10x the guilt for not being productive, because the opportunity cost just increased by an order of magnitude
------
i just wanna be the abstract super engineer
whose aesthetic preferences save companies untold amounts of money
I wanna be the director that could read and understand a codebase 5 minutes before a meeting
I wanna be jim keller
I wanna predict the future, by building it
------
I've worked with god engineers, the best part about it is that you realize they don't have anything you don't
------
>they don't know that we won't have to work in 5 years
------
In my mid 20s I considered my expected value of independent studying or learning from a side project to be $500/hour and prioritized it accordingly. In retrospect that number may have been too low.
------
please stop liking my dating discourse tweets
------
linkedin is awful, awful, awful
------
By crashing the cost of bullshit, bullshit loses all value
Now, writing will solely be measured the information content, density and novelty
Because writing, well, anyone can bullshit now.
Bullshit is now.. really bullshit

Say more with less
------
Go to Google Scholar and look up ‘As an AI language model” -“ChatGPT”’
------
Career bullshitters are going to whine whine
you see it already with some "artists" and "writers"
instead of mindless information expansion, wasting human cycles for no benefit
art and information will now be judged by its actual utility
not the effort required to produce it
------
the scary thing about the current AI wave is if things were held constant (meaning, no progress at all), everything is still changed forever
------
Slightly too little socialization: I am a hermit. I have no friends. My life is to suffer alone.

Slightly too much socialization: Please leave me, I grow weary and tired. I need solitude. Peace. Let me rest.
------
it’s better to talk to gpt4 than a canadian clinician
------
A Vancouver woman who went to hospital seeking help for suicidal ideation says a clinician told her "the system is broken," there are long waits to see a psychiatrist, and: "Have you considered [medical assistance in dying?]?" https://theglobeandmail.com/gift/e0c398c069084c12bf24c9747202409cba67efee09a130203d7409700507ddac/2MUU2BTI4NE53KXB24RGTKDPLE/…
------
instead of making starlink work through storm clouds, we should geo-engineer our planet so we stop having storm clouds
------
humanity if we stopped getting distracted while waiting for gpt4 inference to finish
------
if, at any time, you feel like you shouldn't do something that you want to do because it would be too "cringe" or too "in character", you should absolutely do that thing

lean in
------
worrying about being cringe is anti based
------
non zirp promos hit different
------
I actually feel bad for people who learned to program using python. There are so many foot guns

Trying to do something functional, gpt4 recommends three Very Bad Ugly Things in a row, until I get it to do it "simply"
------
Not GPT4's fault, it's the python culture and the tools that they have available
makes it easy to do the wrong thing
------
gpt4 is my DAWG. it is my HOMIE. I call a crude approximation of the god manifold over a WEB INTERFACE. I ask it to write code in the worst programming language in the world. 

It Understands
------
think slow
execute fast
------
What's really interesting about wealth in the dating market is that it isn't how much someone already has - it's how much that person is expected to make, based on vibes alone. Women are exceptionally good at this
------
making the value of various things legible often degrades their worth. i stand by it that selling blue checks is wrong and distributing ad revenue is wrong. tis clout alone that turns the wheels of history
------
Me at my 10 million dollar lake front cottage property an hour drive away from Toronto after making a fortune selling a book titled "12 reasons why you should wipe your ass with both toilet paper AND water"
------
a girl with a male best friend just means she wasn't good enough to get a girl best friend
------
3D Gaussian Splatting for Real-Time Radiance Field Rendering

paper page: https://huggingface.co/papers/2308.04079…

Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires… Show more
------
mfs will have Pixie Girl Poo Poo Anime Girl profile pictures online, and deadlift 600 pounds offline
------
man the perf on the open 70b models is getting dangerously close to GPT4 on some benchmarks
------
LoRA trained models too..
------
Towards a Unified View of Parameter-Efficient Transfer Learning
https://arxiv.org/abs/2110.04366
good paper
------
he's... he's.... he's literally me
------
This is an ape ("Kanzi") playing Minecraft! A fascinating experiment on non-human biological neural networks 

I've been teaching AI to play Minecraft for too long. There're so many similar techniques that the ape trainers used:

- In-context reinforcement learning: Kanzi gets… Show more
------
"We're all gonna f*cking die from [insert new tech]" ---> please purchase my publication/book/TED talk pipeline.

Fear sells. AI safety grifters are aligned in interest with media orgs that will amplify their message for clicks. Remember that.
------
This is novel software engineering, for an AI world.
e.g. building systems and algorithms that clean data, on top of new primitives
this is what excites me about this all! 
map reduce, but with LLMs!
------
This was a fun one:
https://github.com/jondurbin/airoboros/commit/2632489e02a70f2fde462783bcd6a3a78667eb55…

`cull-instructions` entrypoint for airoboros to merge datasets, remove duplicates, and use LLM to score and filter incorrect/low quality answers, AALLMs, etc.

Slow, pricey, but very useful for improving quality.
------
i should look up algorithms that have humans as a component, probably from the 70s
probably lots of goodies there to apply with LLMs
------
no one knows what happens next
------
I'm more of a "you don’t have to be responsible for the world that you’re in" kinda guy (this was von Neumann to Feynman)
------
my e/acc > e/a preference is mostly a deep aversion for making long term predictions for second order effects. usually, borne out of hubris

yes, theoretically, the double pendulum could swing that way, but let's really worry about the first pendulum
------
our predictability horizon for chaotic systems is shorter than people tend to think
------
(warning, strawman violation) 

amusingly, the lesswrongians' preference for bayes theorem shows this violation. they are stacking causal models on top of each other, sky high like a jenga tower. 

humans have a tendency to underestimate the incompleteness in our priors. collapse
------
expectation: 10xing my output with gpt4
reality: feeling 10x the guilt for not being productive, because the opportunity cost just increased by an order of magnitude
------
i just wanna be the abstract super engineer
whose aesthetic preferences save companies untold amounts of money
I wanna be the director that could read and understand a codebase 5 minutes before a meeting
I wanna be jim keller
I wanna predict the future, by building it
------
I've worked with god engineers, the best part about it is that you realize they don't have anything you don't
------
>they don't know that we won't have to work in 5 years
------
In my mid 20s I considered my expected value of independent studying or learning from a side project to be $500/hour and prioritized it accordingly. In retrospect that number may have been too low.
------
please stop liking my dating discourse tweets
------
linkedin is awful, awful, awful
------
By crashing the cost of bullshit, bullshit loses all value
Now, writing will solely be measured the information content, density and novelty
Because writing, well, anyone can bullshit now.
Bullshit is now.. really bullshit

Say more with less
------
Go to Google Scholar and look up ‘As an AI language model” -“ChatGPT”’
------
Career bullshitters are going to whine whine
you see it already with some "artists" and "writers"
instead of mindless information expansion, wasting human cycles for no benefit
art and information will now be judged by its actual utility
not the effort required to produce it
------
the scary thing about the current AI wave is if things were held constant (meaning, no progress at all), everything is still changed forever
------
Slightly too little socialization: I am a hermit. I have no friends. My life is to suffer alone.

Slightly too much socialization: Please leave me, I grow weary and tired. I need solitude. Peace. Let me rest.
------
it’s better to talk to gpt4 than a canadian clinician
------
A Vancouver woman who went to hospital seeking help for suicidal ideation says a clinician told her "the system is broken," there are long waits to see a psychiatrist, and: "Have you considered [medical assistance in dying?]?" https://theglobeandmail.com/gift/e0c398c069084c12bf24c9747202409cba67efee09a130203d7409700507ddac/2MUU2BTI4NE53KXB24RGTKDPLE/…
------
instead of making starlink work through storm clouds, we should geo-engineer our planet so we stop having storm clouds
------
humanity if we stopped getting distracted while waiting for gpt4 inference to finish
------
if, at any time, you feel like you shouldn't do something that you want to do because it would be too "cringe" or too "in character", you should absolutely do that thing

lean in
------
worrying about being cringe is anti based
------
non zirp promos hit different
------
I actually feel bad for people who learned to program using python. There are so many foot guns

Trying to do something functional, gpt4 recommends three Very Bad Ugly Things in a row, until I get it to do it "simply"
------
Not GPT4's fault, it's the python culture and the tools that they have available
makes it easy to do the wrong thing
------
gpt4 is my DAWG. it is my HOMIE. I call a crude approximation of the god manifold over a WEB INTERFACE. I ask it to write code in the worst programming language in the world. 

It Understands
------
think slow
execute fast
------
What's really interesting about wealth in the dating market is that it isn't how much someone already has - it's how much that person is expected to make, based on vibes alone. Women are exceptionally good at this
------
making the value of various things legible often degrades their worth. i stand by it that selling blue checks is wrong and distributing ad revenue is wrong. tis clout alone that turns the wheels of history
------
Me at my 10 million dollar lake front cottage property an hour drive away from Toronto after making a fortune selling a book titled "12 reasons why you should wipe your ass with both toilet paper AND water"
------
a girl with a male best friend just means she wasn't good enough to get a girl best friend
------
3D Gaussian Splatting for Real-Time Radiance Field Rendering

paper page: https://huggingface.co/papers/2308.04079…

Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires… Show more
------
[Q] What is the overhead for the mitigation?
[A] This depends on whether Gather is in the critical execution path of a program. According to Intel, some workloads may experience up to 50% overhead.

it was just a matter of time now
------
Dropping #Downfall, exploiting speculative forwarding of 'Gather' instruction to steal data from hardware registers. #MeltdownSequel
- Practical to exploit (POC/Demo)
- Defeat all isolation boundaries (OS, VM, SGX)
- Bypass all Meltdown/MDS mitigations.
https://downfall.page
------
It seems like no one is really close to replicating GPT-4 performance yet. Props to OpenAI for having such a big lead.
------
"mom, can we have Manhattan Project for AGI?"

"we have Manhattan Project for AGI at home sweetie"

Manhattan Project at home:
------
This is novel software engineering, for an AI world.
e.g. building systems and algorithms that clean data, on top of new primitives
this is what excites me about this all! 
map reduce, but with LLMs!
------
This was a fun one:
https://github.com/jondurbin/airoboros/commit/2632489e02a70f2fde462783bcd6a3a78667eb55…

`cull-instructions` entrypoint for airoboros to merge datasets, remove duplicates, and use LLM to score and filter incorrect/low quality answers, AALLMs, etc.

Slow, pricey, but very useful for improving quality.
------
i should look up algorithms that have humans as a component, probably from the 70s
probably lots of goodies there to apply with LLMs
------
no one knows what happens next
------
I'm more of a "you don’t have to be responsible for the world that you’re in" kinda guy (this was von Neumann to Feynman)
------
my e/acc > e/a preference is mostly a deep aversion for making long term predictions for second order effects. usually, borne out of hubris

yes, theoretically, the double pendulum could swing that way, but let's really worry about the first pendulum
------
our predictability horizon for chaotic systems is shorter than people tend to think
------
(warning, strawman violation) 

amusingly, the lesswrongians' preference for bayes theorem shows this violation. they are stacking causal models on top of each other, sky high like a jenga tower. 

humans have a tendency to underestimate the incompleteness in our priors. collapse
------
expectation: 10xing my output with gpt4
reality: feeling 10x the guilt for not being productive, because the opportunity cost just increased by an order of magnitude
------
i just wanna be the abstract super engineer
whose aesthetic preferences save companies untold amounts of money
I wanna be the director that could read and understand a codebase 5 minutes before a meeting
I wanna be jim keller
I wanna predict the future, by building it
------
I've worked with god engineers, the best part about it is that you realize they don't have anything you don't
------
>they don't know that we won't have to work in 5 years
------
In my mid 20s I considered my expected value of independent studying or learning from a side project to be $500/hour and prioritized it accordingly. In retrospect that number may have been too low.
------
please stop liking my dating discourse tweets
------
linkedin is awful, awful, awful
------
By crashing the cost of bullshit, bullshit loses all value
Now, writing will solely be measured the information content, density and novelty
Because writing, well, anyone can bullshit now.
Bullshit is now.. really bullshit

Say more with less
------
Go to Google Scholar and look up ‘As an AI language model” -“ChatGPT”’
------
Career bullshitters are going to whine whine
you see it already with some "artists" and "writers"
instead of mindless information expansion, wasting human cycles for no benefit
art and information will now be judged by its actual utility
not the effort required to produce it
------
the scary thing about the current AI wave is if things were held constant (meaning, no progress at all), everything is still changed forever
------
Slightly too little socialization: I am a hermit. I have no friends. My life is to suffer alone.

Slightly too much socialization: Please leave me, I grow weary and tired. I need solitude. Peace. Let me rest.
------
it’s better to talk to gpt4 than a canadian clinician
------
A Vancouver woman who went to hospital seeking help for suicidal ideation says a clinician told her "the system is broken," there are long waits to see a psychiatrist, and: "Have you considered [medical assistance in dying?]?" https://theglobeandmail.com/gift/e0c398c069084c12bf24c9747202409cba67efee09a130203d7409700507ddac/2MUU2BTI4NE53KXB24RGTKDPLE/…
------
instead of making starlink work through storm clouds, we should geo-engineer our planet so we stop having storm clouds
------
humanity if we stopped getting distracted while waiting for gpt4 inference to finish
------
if, at any time, you feel like you shouldn't do something that you want to do because it would be too "cringe" or too "in character", you should absolutely do that thing

lean in
------
worrying about being cringe is anti based
------
non zirp promos hit different
------
I actually feel bad for people who learned to program using python. There are so many foot guns

Trying to do something functional, gpt4 recommends three Very Bad Ugly Things in a row, until I get it to do it "simply"
------
Not GPT4's fault, it's the python culture and the tools that they have available
makes it easy to do the wrong thing
------
gpt4 is my DAWG. it is my HOMIE. I call a crude approximation of the god manifold over a WEB INTERFACE. I ask it to write code in the worst programming language in the world. 

It Understands
------
think slow
execute fast
------
What's really interesting about wealth in the dating market is that it isn't how much someone already has - it's how much that person is expected to make, based on vibes alone. Women are exceptionally good at this
------
making the value of various things legible often degrades their worth. i stand by it that selling blue checks is wrong and distributing ad revenue is wrong. tis clout alone that turns the wheels of history
------
Me at my 10 million dollar lake front cottage property an hour drive away from Toronto after making a fortune selling a book titled "12 reasons why you should wipe your ass with both toilet paper AND water"
------
a girl with a male best friend just means she wasn't good enough to get a girl best friend
------
3D Gaussian Splatting for Real-Time Radiance Field Rendering

paper page: https://huggingface.co/papers/2308.04079…

Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires… Show more
------
[Q] What is the overhead for the mitigation?
[A] This depends on whether Gather is in the critical execution path of a program. According to Intel, some workloads may experience up to 50% overhead.

it was just a matter of time now
------
Dropping #Downfall, exploiting speculative forwarding of 'Gather' instruction to steal data from hardware registers. #MeltdownSequel
- Practical to exploit (POC/Demo)
- Defeat all isolation boundaries (OS, VM, SGX)
- Bypass all Meltdown/MDS mitigations.
https://downfall.page
------
It seems like no one is really close to replicating GPT-4 performance yet. Props to OpenAI for having such a big lead.
------
"mom, can we have Manhattan Project for AGI?"

"we have Manhattan Project for AGI at home sweetie"

Manhattan Project at home:
------
system design is probably the hardest part about software engineering
------
the process:
- gather explicit requirements
- socialize, talk, research, find implicit requirements. write them down. Make them explicit
- write, rewrite, rewrite, rewrite. until, it's aesthetically pleasing
- find an eng with a better aesthetic classifier, and get their gut pass
------
Take a moment to appreciate the magic of trade. A free trade is when two parties make an exchange because they both want to, and both wind up better off. Yes, fraud, coercion, and externalities exist, but the vast majority of exchanges are unregretted and positive sum.
------
LET"S FUCKING GOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO
------
wait, so a monad is just a monoid in the category of endofunctors!
------
I actually don't get people who complain about girls having high standards. okay.. so you want them to have low standards??

like, just be cool, confident, calm, handsome, tall, rich. it's really not that hard
------
dating is actually so simple. just pick a girl and be extremely lucky
------
standup was at 10 am.. so why did i wake up at 9 am
------
i just wanna be the abstract super engineer
whose aesthetic preferences save companies untold amounts of money
I wanna be the director that could read and understand a codebase 5 minutes before a meeting
I wanna be jim keller
I wanna predict the future, by building it
------
I've worked with god engineers, the best part about it is that you realize they don't have anything you don't
------
>they don't know that we won't have to work in 5 years
------
In my mid 20s I considered my expected value of independent studying or learning from a side project to be $500/hour and prioritized it accordingly. In retrospect that number may have been too low.
------
please stop liking my dating discourse tweets
------
linkedin is awful, awful, awful
------
By crashing the cost of bullshit, bullshit loses all value
Now, writing will solely be measured the information content, density and novelty
Because writing, well, anyone can bullshit now.
Bullshit is now.. really bullshit

Say more with less
------
Go to Google Scholar and look up ‘As an AI language model” -“ChatGPT”’
------
Career bullshitters are going to whine whine
you see it already with some "artists" and "writers"
instead of mindless information expansion, wasting human cycles for no benefit
art and information will now be judged by its actual utility
not the effort required to produce it
------
the scary thing about the current AI wave is if things were held constant (meaning, no progress at all), everything is still changed forever
------
Slightly too little socialization: I am a hermit. I have no friends. My life is to suffer alone.

Slightly too much socialization: Please leave me, I grow weary and tired. I need solitude. Peace. Let me rest.
------
it’s better to talk to gpt4 than a canadian clinician
------
A Vancouver woman who went to hospital seeking help for suicidal ideation says a clinician told her "the system is broken," there are long waits to see a psychiatrist, and: "Have you considered [medical assistance in dying?]?" https://theglobeandmail.com/gift/e0c398c069084c12bf24c9747202409cba67efee09a130203d7409700507ddac/2MUU2BTI4NE53KXB24RGTKDPLE/…
------
instead of making starlink work through storm clouds, we should geo-engineer our planet so we stop having storm clouds
------
humanity if we stopped getting distracted while waiting for gpt4 inference to finish
------
if, at any time, you feel like you shouldn't do something that you want to do because it would be too "cringe" or too "in character", you should absolutely do that thing

lean in
------
worrying about being cringe is anti based
------
non zirp promos hit different
------
I actually feel bad for people who learned to program using python. There are so many foot guns

Trying to do something functional, gpt4 recommends three Very Bad Ugly Things in a row, until I get it to do it "simply"
------
Not GPT4's fault, it's the python culture and the tools that they have available
makes it easy to do the wrong thing
------
gpt4 is my DAWG. it is my HOMIE. I call a crude approximation of the god manifold over a WEB INTERFACE. I ask it to write code in the worst programming language in the world. 

It Understands
------
think slow
execute fast
------
What's really interesting about wealth in the dating market is that it isn't how much someone already has - it's how much that person is expected to make, based on vibes alone. Women are exceptionally good at this
------
making the value of various things legible often degrades their worth. i stand by it that selling blue checks is wrong and distributing ad revenue is wrong. tis clout alone that turns the wheels of history
------
Me at my 10 million dollar lake front cottage property an hour drive away from Toronto after making a fortune selling a book titled "12 reasons why you should wipe your ass with both toilet paper AND water"
------
a girl with a male best friend just means she wasn't good enough to get a girl best friend
------
3D Gaussian Splatting for Real-Time Radiance Field Rendering

paper page: https://huggingface.co/papers/2308.04079…

Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires… Show more
------
[Q] What is the overhead for the mitigation?
[A] This depends on whether Gather is in the critical execution path of a program. According to Intel, some workloads may experience up to 50% overhead.

it was just a matter of time now
------
Dropping #Downfall, exploiting speculative forwarding of 'Gather' instruction to steal data from hardware registers. #MeltdownSequel
- Practical to exploit (POC/Demo)
- Defeat all isolation boundaries (OS, VM, SGX)
- Bypass all Meltdown/MDS mitigations.
https://downfall.page
------
It seems like no one is really close to replicating GPT-4 performance yet. Props to OpenAI for having such a big lead.
------
"mom, can we have Manhattan Project for AGI?"

"we have Manhattan Project for AGI at home sweetie"

Manhattan Project at home:
------
system design is probably the hardest part about software engineering
------
the process:
- gather explicit requirements
- socialize, talk, research, find implicit requirements. write them down. Make them explicit
- write, rewrite, rewrite, rewrite. until, it's aesthetically pleasing
- find an eng with a better aesthetic classifier, and get their gut pass
------
Take a moment to appreciate the magic of trade. A free trade is when two parties make an exchange because they both want to, and both wind up better off. Yes, fraud, coercion, and externalities exist, but the vast majority of exchanges are unregretted and positive sum.
------
LET"S FUCKING GOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO
------
wait, so a monad is just a monoid in the category of endofunctors!
------
I actually don't get people who complain about girls having high standards. okay.. so you want them to have low standards??

like, just be cool, confident, calm, handsome, tall, rich. it's really not that hard
------
dating is actually so simple. just pick a girl and be extremely lucky
------
standup was at 10 am.. so why did i wake up at 9 am
------
playstation 2 was better than xbox
------
openai is better than google
------
pc masterrace
open source masterrace
------
Studying Large Language Model Generalization with Influence Functions

https://arxiv.org/abs/2308.03296
------
me after getting paid for impressions
------
By crashing the cost of bullshit, bullshit loses all value
Now, writing will solely be measured the information content, density and novelty
Because writing, well, anyone can bullshit now.
Bullshit is now.. really bullshit

Say more with less
------
Go to Google Scholar and look up ‘As an AI language model” -“ChatGPT”’
------
Career bullshitters are going to whine whine
you see it already with some "artists" and "writers"
instead of mindless information expansion, wasting human cycles for no benefit
art and information will now be judged by its actual utility
not the effort required to produce it
------
the scary thing about the current AI wave is if things were held constant (meaning, no progress at all), everything is still changed forever
------
Slightly too little socialization: I am a hermit. I have no friends. My life is to suffer alone.

Slightly too much socialization: Please leave me, I grow weary and tired. I need solitude. Peace. Let me rest.
------
it’s better to talk to gpt4 than a canadian clinician
------
A Vancouver woman who went to hospital seeking help for suicidal ideation says a clinician told her "the system is broken," there are long waits to see a psychiatrist, and: "Have you considered [medical assistance in dying?]?" https://theglobeandmail.com/gift/e0c398c069084c12bf24c9747202409cba67efee09a130203d7409700507ddac/2MUU2BTI4NE53KXB24RGTKDPLE/…
------
instead of making starlink work through storm clouds, we should geo-engineer our planet so we stop having storm clouds
------
humanity if we stopped getting distracted while waiting for gpt4 inference to finish
------
if, at any time, you feel like you shouldn't do something that you want to do because it would be too "cringe" or too "in character", you should absolutely do that thing

lean in
------
worrying about being cringe is anti based
------
non zirp promos hit different
------
I actually feel bad for people who learned to program using python. There are so many foot guns

Trying to do something functional, gpt4 recommends three Very Bad Ugly Things in a row, until I get it to do it "simply"
------
Not GPT4's fault, it's the python culture and the tools that they have available
makes it easy to do the wrong thing
------
gpt4 is my DAWG. it is my HOMIE. I call a crude approximation of the god manifold over a WEB INTERFACE. I ask it to write code in the worst programming language in the world. 

It Understands
------
think slow
execute fast
------
What's really interesting about wealth in the dating market is that it isn't how much someone already has - it's how much that person is expected to make, based on vibes alone. Women are exceptionally good at this
------
making the value of various things legible often degrades their worth. i stand by it that selling blue checks is wrong and distributing ad revenue is wrong. tis clout alone that turns the wheels of history
------
Me at my 10 million dollar lake front cottage property an hour drive away from Toronto after making a fortune selling a book titled "12 reasons why you should wipe your ass with both toilet paper AND water"
------
a girl with a male best friend just means she wasn't good enough to get a girl best friend
------
3D Gaussian Splatting for Real-Time Radiance Field Rendering

paper page: https://huggingface.co/papers/2308.04079…

Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires… Show more
------
[Q] What is the overhead for the mitigation?
[A] This depends on whether Gather is in the critical execution path of a program. According to Intel, some workloads may experience up to 50% overhead.

it was just a matter of time now
------
Dropping #Downfall, exploiting speculative forwarding of 'Gather' instruction to steal data from hardware registers. #MeltdownSequel
- Practical to exploit (POC/Demo)
- Defeat all isolation boundaries (OS, VM, SGX)
- Bypass all Meltdown/MDS mitigations.
https://downfall.page
------
It seems like no one is really close to replicating GPT-4 performance yet. Props to OpenAI for having such a big lead.
------
"mom, can we have Manhattan Project for AGI?"

"we have Manhattan Project for AGI at home sweetie"

Manhattan Project at home:
------
system design is probably the hardest part about software engineering
------
the process:
- gather explicit requirements
- socialize, talk, research, find implicit requirements. write them down. Make them explicit
- write, rewrite, rewrite, rewrite. until, it's aesthetically pleasing
- find an eng with a better aesthetic classifier, and get their gut pass
------
Take a moment to appreciate the magic of trade. A free trade is when two parties make an exchange because they both want to, and both wind up better off. Yes, fraud, coercion, and externalities exist, but the vast majority of exchanges are unregretted and positive sum.
------
LET"S FUCKING GOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO
------
wait, so a monad is just a monoid in the category of endofunctors!
------
I actually don't get people who complain about girls having high standards. okay.. so you want them to have low standards??

like, just be cool, confident, calm, handsome, tall, rich. it's really not that hard
------
dating is actually so simple. just pick a girl and be extremely lucky
------
standup was at 10 am.. so why did i wake up at 9 am
------
playstation 2 was better than xbox
------
openai is better than google
------
pc masterrace
open source masterrace
------
Studying Large Language Model Generalization with Influence Functions

https://arxiv.org/abs/2308.03296
------
me after getting paid for impressions
------
breaking: google working on chatgpt competitor "Gemini". planned release is when OpenAI builds a gravitational harvester at the center of the milky way
------
Tim Dettmers explained why you might want to use 4-bit inference in Large Language Models according to "k-bit Inference Scaling Laws"

Tim is an amazing communicator and managed to break down such a complex topic into one core claim and a few key plots! http://youtube.com/watch?v=odlQa6AE1gY…
------
valve deckard eye gaze and click
------
We have just released input streaming, which allows you to stream LLM responses and generate speech in real-time - all possible with sub-1-second latency.

Try it today: https://github.com/elevenlabs/elevenlabs-python…
------
@elevenlabsio now supports low-latency input streaming. This allows you to listen to LLMs in real-time as the text is being generated.

 "A one-sentence relaxing speech"
------
i love to work
------
when i was working with more junior eng; i'd use so much zoomer vernacular that even the junior zoomers couldn't understand me
------
Not GPT4's fault, it's the python culture and the tools that they have available
makes it easy to do the wrong thing
------
gpt4 is my DAWG. it is my HOMIE. I call a crude approximation of the god manifold over a WEB INTERFACE. I ask it to write code in the worst programming language in the world. 

It Understands
------
think slow
execute fast
------
What's really interesting about wealth in the dating market is that it isn't how much someone already has - it's how much that person is expected to make, based on vibes alone. Women are exceptionally good at this
------
making the value of various things legible often degrades their worth. i stand by it that selling blue checks is wrong and distributing ad revenue is wrong. tis clout alone that turns the wheels of history
------
Me at my 10 million dollar lake front cottage property an hour drive away from Toronto after making a fortune selling a book titled "12 reasons why you should wipe your ass with both toilet paper AND water"
------
a girl with a male best friend just means she wasn't good enough to get a girl best friend
------
3D Gaussian Splatting for Real-Time Radiance Field Rendering

paper page: https://huggingface.co/papers/2308.04079…

Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires… Show more
------
[Q] What is the overhead for the mitigation?
[A] This depends on whether Gather is in the critical execution path of a program. According to Intel, some workloads may experience up to 50% overhead.

it was just a matter of time now
------
Dropping #Downfall, exploiting speculative forwarding of 'Gather' instruction to steal data from hardware registers. #MeltdownSequel
- Practical to exploit (POC/Demo)
- Defeat all isolation boundaries (OS, VM, SGX)
- Bypass all Meltdown/MDS mitigations.
https://downfall.page
------
It seems like no one is really close to replicating GPT-4 performance yet. Props to OpenAI for having such a big lead.
------
"mom, can we have Manhattan Project for AGI?"

"we have Manhattan Project for AGI at home sweetie"

Manhattan Project at home:
------
system design is probably the hardest part about software engineering
------
the process:
- gather explicit requirements
- socialize, talk, research, find implicit requirements. write them down. Make them explicit
- write, rewrite, rewrite, rewrite. until, it's aesthetically pleasing
- find an eng with a better aesthetic classifier, and get their gut pass
------
Take a moment to appreciate the magic of trade. A free trade is when two parties make an exchange because they both want to, and both wind up better off. Yes, fraud, coercion, and externalities exist, but the vast majority of exchanges are unregretted and positive sum.
------
LET"S FUCKING GOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO
------
wait, so a monad is just a monoid in the category of endofunctors!
------
I actually don't get people who complain about girls having high standards. okay.. so you want them to have low standards??

like, just be cool, confident, calm, handsome, tall, rich. it's really not that hard
------
dating is actually so simple. just pick a girl and be extremely lucky
------
standup was at 10 am.. so why did i wake up at 9 am
------
playstation 2 was better than xbox
------
openai is better than google
------
pc masterrace
open source masterrace
------
Studying Large Language Model Generalization with Influence Functions

https://arxiv.org/abs/2308.03296
------
me after getting paid for impressions
------
breaking: google working on chatgpt competitor "Gemini". planned release is when OpenAI builds a gravitational harvester at the center of the milky way
------
Tim Dettmers explained why you might want to use 4-bit inference in Large Language Models according to "k-bit Inference Scaling Laws"

Tim is an amazing communicator and managed to break down such a complex topic into one core claim and a few key plots! http://youtube.com/watch?v=odlQa6AE1gY…
------
valve deckard eye gaze and click
------
We have just released input streaming, which allows you to stream LLM responses and generate speech in real-time - all possible with sub-1-second latency.

Try it today: https://github.com/elevenlabs/elevenlabs-python…
------
@elevenlabsio now supports low-latency input streaming. This allows you to listen to LLMs in real-time as the text is being generated.

 "A one-sentence relaxing speech"
------
i love to work
------
when i was working with more junior eng; i'd use so much zoomer vernacular that even the junior zoomers couldn't understand me
------
Let’s see what this rock can do
------
good morning, brothers and sisters
------
twitter follower count is so variable. for example, a month ago, both me and 
@goth600
 were at 1k followers. one week later we were both at 10k, but then three weeks later he's shot up to 36k while i'm still at 25k
------
"uhhhhhhh what hyperparameters should i use uhhhhhhh should i use weight decay uhhhhhhh should i use dropout uhhhhhhhhhhhhhhH"

in the time you spent deliberating over your hyperparameters, i've trained 8 qloras
I use vibes and the sound of the coil whine to evaluate them
Keep up
------
ffs my loss exploded
------
mfs will get mil spec pit bull rail gun drone as their dog breed and be like "actually this dog breed is safe, people just don't know how to train it"
------
[Q] What is the overhead for the mitigation?
[A] This depends on whether Gather is in the critical execution path of a program. According to Intel, some workloads may experience up to 50% overhead.

it was just a matter of time now
------
Dropping #Downfall, exploiting speculative forwarding of 'Gather' instruction to steal data from hardware registers. #MeltdownSequel
- Practical to exploit (POC/Demo)
- Defeat all isolation boundaries (OS, VM, SGX)
- Bypass all Meltdown/MDS mitigations.
https://downfall.page
------
It seems like no one is really close to replicating GPT-4 performance yet. Props to OpenAI for having such a big lead.
------
"mom, can we have Manhattan Project for AGI?"

"we have Manhattan Project for AGI at home sweetie"

Manhattan Project at home:
------
system design is probably the hardest part about software engineering
------
the process:
- gather explicit requirements
- socialize, talk, research, find implicit requirements. write them down. Make them explicit
- write, rewrite, rewrite, rewrite. until, it's aesthetically pleasing
- find an eng with a better aesthetic classifier, and get their gut pass
------
Take a moment to appreciate the magic of trade. A free trade is when two parties make an exchange because they both want to, and both wind up better off. Yes, fraud, coercion, and externalities exist, but the vast majority of exchanges are unregretted and positive sum.
------
LET"S FUCKING GOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO
------
wait, so a monad is just a monoid in the category of endofunctors!
------
I actually don't get people who complain about girls having high standards. okay.. so you want them to have low standards??

like, just be cool, confident, calm, handsome, tall, rich. it's really not that hard
------
dating is actually so simple. just pick a girl and be extremely lucky
------
standup was at 10 am.. so why did i wake up at 9 am
------
playstation 2 was better than xbox
------
openai is better than google
------
pc masterrace
open source masterrace
------
Studying Large Language Model Generalization with Influence Functions

https://arxiv.org/abs/2308.03296
------
me after getting paid for impressions
------
breaking: google working on chatgpt competitor "Gemini". planned release is when OpenAI builds a gravitational harvester at the center of the milky way
------
Tim Dettmers explained why you might want to use 4-bit inference in Large Language Models according to "k-bit Inference Scaling Laws"

Tim is an amazing communicator and managed to break down such a complex topic into one core claim and a few key plots! http://youtube.com/watch?v=odlQa6AE1gY…
------
valve deckard eye gaze and click
------
We have just released input streaming, which allows you to stream LLM responses and generate speech in real-time - all possible with sub-1-second latency.

Try it today: https://github.com/elevenlabs/elevenlabs-python…
------
@elevenlabsio now supports low-latency input streaming. This allows you to listen to LLMs in real-time as the text is being generated.

 "A one-sentence relaxing speech"
------
i love to work
------
when i was working with more junior eng; i'd use so much zoomer vernacular that even the junior zoomers couldn't understand me
------
Let’s see what this rock can do
------
good morning, brothers and sisters
------
twitter follower count is so variable. for example, a month ago, both me and 
@goth600
 were at 1k followers. one week later we were both at 10k, but then three weeks later he's shot up to 36k while i'm still at 25k
------
"uhhhhhhh what hyperparameters should i use uhhhhhhh should i use weight decay uhhhhhhh should i use dropout uhhhhhhhhhhhhhhH"

in the time you spent deliberating over your hyperparameters, i've trained 8 qloras
I use vibes and the sound of the coil whine to evaluate them
Keep up
------
ffs my loss exploded
------
mfs will get mil spec pit bull rail gun drone as their dog breed and be like "actually this dog breed is safe, people just don't know how to train it"
------
upper middle class who had blue neoliberal parents and work as american economists / policy analyst / financier and are secretly red but not entirely twitter is such a wild place
------
reading hackernews and shaking my head the whole time so the people on twitter know i disagree with it
------
Today FLOPS is one of the things you can spend $ on.
Tomorrow $ is one of the things you can spend FLOPS on.
A reading from the church of FLOPS.
------
hackernews users hate fun
btw, if you're a hackernews user please lmk so i can unfollow you
------
the users are such pessimistic crabs it feels like they're mostly generated. like, you can't hate the kingdom of god THAT much, right?
------
ijbol theory fancam
------
this is e/acc
------
Square Watermelons In Japan.  Watermelons are grown inside square boxes to produce this unusual square shape. Suppliers have found them more convenient for stacking, shipping, and refrigerator storage.
------
system design is probably the hardest part about software engineering
------
the process:
- gather explicit requirements
- socialize, talk, research, find implicit requirements. write them down. Make them explicit
- write, rewrite, rewrite, rewrite. until, it's aesthetically pleasing
- find an eng with a better aesthetic classifier, and get their gut pass
------
Take a moment to appreciate the magic of trade. A free trade is when two parties make an exchange because they both want to, and both wind up better off. Yes, fraud, coercion, and externalities exist, but the vast majority of exchanges are unregretted and positive sum.
------
LET"S FUCKING GOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO
------
wait, so a monad is just a monoid in the category of endofunctors!
------
I actually don't get people who complain about girls having high standards. okay.. so you want them to have low standards??

like, just be cool, confident, calm, handsome, tall, rich. it's really not that hard
------
dating is actually so simple. just pick a girl and be extremely lucky
------
standup was at 10 am.. so why did i wake up at 9 am
------
playstation 2 was better than xbox
------
openai is better than google
------
pc masterrace
open source masterrace
------
Studying Large Language Model Generalization with Influence Functions

https://arxiv.org/abs/2308.03296
------
me after getting paid for impressions
------
breaking: google working on chatgpt competitor "Gemini". planned release is when OpenAI builds a gravitational harvester at the center of the milky way
------
Tim Dettmers explained why you might want to use 4-bit inference in Large Language Models according to "k-bit Inference Scaling Laws"

Tim is an amazing communicator and managed to break down such a complex topic into one core claim and a few key plots! http://youtube.com/watch?v=odlQa6AE1gY…
------
valve deckard eye gaze and click
------
We have just released input streaming, which allows you to stream LLM responses and generate speech in real-time - all possible with sub-1-second latency.

Try it today: https://github.com/elevenlabs/elevenlabs-python…
------
@elevenlabsio now supports low-latency input streaming. This allows you to listen to LLMs in real-time as the text is being generated.

 "A one-sentence relaxing speech"
------
i love to work
------
when i was working with more junior eng; i'd use so much zoomer vernacular that even the junior zoomers couldn't understand me
------
Let’s see what this rock can do
------
good morning, brothers and sisters
------
twitter follower count is so variable. for example, a month ago, both me and 
@goth600
 were at 1k followers. one week later we were both at 10k, but then three weeks later he's shot up to 36k while i'm still at 25k
------
"uhhhhhhh what hyperparameters should i use uhhhhhhh should i use weight decay uhhhhhhh should i use dropout uhhhhhhhhhhhhhhH"

in the time you spent deliberating over your hyperparameters, i've trained 8 qloras
I use vibes and the sound of the coil whine to evaluate them
Keep up
------
ffs my loss exploded
------
mfs will get mil spec pit bull rail gun drone as their dog breed and be like "actually this dog breed is safe, people just don't know how to train it"
------
upper middle class who had blue neoliberal parents and work as american economists / policy analyst / financier and are secretly red but not entirely twitter is such a wild place
------
reading hackernews and shaking my head the whole time so the people on twitter know i disagree with it
------
Today FLOPS is one of the things you can spend $ on.
Tomorrow $ is one of the things you can spend FLOPS on.
A reading from the church of FLOPS.
------
hackernews users hate fun
btw, if you're a hackernews user please lmk so i can unfollow you
------
the users are such pessimistic crabs it feels like they're mostly generated. like, you can't hate the kingdom of god THAT much, right?
------
ijbol theory fancam
------
this is e/acc
------
Square Watermelons In Japan.  Watermelons are grown inside square boxes to produce this unusual square shape. Suppliers have found them more convenient for stacking, shipping, and refrigerator storage.
------
As I have pointed out before, AI doomerism is a kind of apocalyptic cult.

Why would its most vocal advocates come from ultra-religious families (that they broke away from because of science)?
------
July 7: The @BostonGlobe published this profile:

"Dan Hendrycks wants to save us from an AI catastrophe. He's not sure he'll succeed."

Apparently, his evangelical beliefs have been replaced by a new religion:
AI Doomerism.

https://archive.vn/Sn1v0
------
me watching
- my 3D printer print a plant pot
- GPT4 code interpreter writing unit tests for data quality, and then correcting the failures it finds
------
I'm literally just tweeting and micromanaging my machines to do work for me
post scarcity is here and it's hilarious
------
The only AI agent that I've used and worked is GPT4 code interpreter
very good work from their team
it is not easy to tame the entropic bag of weights to do actual work without collapsing
------
"It says here you are going to work as a 'wagie in a cagie' for a convicted felon named Martin Skhreli, for 0 pay, for two weeks. And you will be staying at Under The Big Shkrellster's Desk Ave, NY.

Looks good to me. Have a good flight."
------
standup was at 10 am.. so why did i wake up at 9 am
------
playstation 2 was better than xbox
------
openai is better than google
------
pc masterrace
open source masterrace
------
Studying Large Language Model Generalization with Influence Functions

https://arxiv.org/abs/2308.03296
------
me after getting paid for impressions
------
breaking: google working on chatgpt competitor "Gemini". planned release is when OpenAI builds a gravitational harvester at the center of the milky way
------
Tim Dettmers explained why you might want to use 4-bit inference in Large Language Models according to "k-bit Inference Scaling Laws"

Tim is an amazing communicator and managed to break down such a complex topic into one core claim and a few key plots! http://youtube.com/watch?v=odlQa6AE1gY…
------
valve deckard eye gaze and click
------
We have just released input streaming, which allows you to stream LLM responses and generate speech in real-time - all possible with sub-1-second latency.

Try it today: https://github.com/elevenlabs/elevenlabs-python…
------
@elevenlabsio now supports low-latency input streaming. This allows you to listen to LLMs in real-time as the text is being generated.

 "A one-sentence relaxing speech"
------
i love to work
------
when i was working with more junior eng; i'd use so much zoomer vernacular that even the junior zoomers couldn't understand me
------
Let’s see what this rock can do
------
good morning, brothers and sisters
------
twitter follower count is so variable. for example, a month ago, both me and 
@goth600
 were at 1k followers. one week later we were both at 10k, but then three weeks later he's shot up to 36k while i'm still at 25k
------
"uhhhhhhh what hyperparameters should i use uhhhhhhh should i use weight decay uhhhhhhh should i use dropout uhhhhhhhhhhhhhhH"

in the time you spent deliberating over your hyperparameters, i've trained 8 qloras
I use vibes and the sound of the coil whine to evaluate them
Keep up
------
ffs my loss exploded
------
mfs will get mil spec pit bull rail gun drone as their dog breed and be like "actually this dog breed is safe, people just don't know how to train it"
------
upper middle class who had blue neoliberal parents and work as american economists / policy analyst / financier and are secretly red but not entirely twitter is such a wild place
------
reading hackernews and shaking my head the whole time so the people on twitter know i disagree with it
------
Today FLOPS is one of the things you can spend $ on.
Tomorrow $ is one of the things you can spend FLOPS on.
A reading from the church of FLOPS.
------
hackernews users hate fun
btw, if you're a hackernews user please lmk so i can unfollow you
------
the users are such pessimistic crabs it feels like they're mostly generated. like, you can't hate the kingdom of god THAT much, right?
------
ijbol theory fancam
------
this is e/acc
------
Square Watermelons In Japan.  Watermelons are grown inside square boxes to produce this unusual square shape. Suppliers have found them more convenient for stacking, shipping, and refrigerator storage.
------
As I have pointed out before, AI doomerism is a kind of apocalyptic cult.

Why would its most vocal advocates come from ultra-religious families (that they broke away from because of science)?
------
July 7: The @BostonGlobe published this profile:

"Dan Hendrycks wants to save us from an AI catastrophe. He's not sure he'll succeed."

Apparently, his evangelical beliefs have been replaced by a new religion:
AI Doomerism.

https://archive.vn/Sn1v0
------
me watching
- my 3D printer print a plant pot
- GPT4 code interpreter writing unit tests for data quality, and then correcting the failures it finds
------
I'm literally just tweeting and micromanaging my machines to do work for me
post scarcity is here and it's hilarious
------
The only AI agent that I've used and worked is GPT4 code interpreter
very good work from their team
it is not easy to tame the entropic bag of weights to do actual work without collapsing
------
"It says here you are going to work as a 'wagie in a cagie' for a convicted felon named Martin Skhreli, for 0 pay, for two weeks. And you will be staying at Under The Big Shkrellster's Desk Ave, NY.

Looks good to me. Have a good flight."
------
companies become one man shops, geo hotz style
wrap me in a predictive cocoon
no more organizational incentive misalignment
no more communication overhead
complete democratization & independence
we don't need more cores
when we can make our cores faster
------
Tim Dettmers explained why you might want to use 4-bit inference in Large Language Models according to "k-bit Inference Scaling Laws"

Tim is an amazing communicator and managed to break down such a complex topic into one core claim and a few key plots! http://youtube.com/watch?v=odlQa6AE1gY…
------
valve deckard eye gaze and click
------
We have just released input streaming, which allows you to stream LLM responses and generate speech in real-time - all possible with sub-1-second latency.

Try it today: https://github.com/elevenlabs/elevenlabs-python…
------
@elevenlabsio now supports low-latency input streaming. This allows you to listen to LLMs in real-time as the text is being generated.

 "A one-sentence relaxing speech"
------
i love to work
------
when i was working with more junior eng; i'd use so much zoomer vernacular that even the junior zoomers couldn't understand me
------
Let’s see what this rock can do
------
good morning, brothers and sisters
------
twitter follower count is so variable. for example, a month ago, both me and 
@goth600
 were at 1k followers. one week later we were both at 10k, but then three weeks later he's shot up to 36k while i'm still at 25k
------
"uhhhhhhh what hyperparameters should i use uhhhhhhh should i use weight decay uhhhhhhh should i use dropout uhhhhhhhhhhhhhhH"

in the time you spent deliberating over your hyperparameters, i've trained 8 qloras
I use vibes and the sound of the coil whine to evaluate them
Keep up
------
ffs my loss exploded
------
mfs will get mil spec pit bull rail gun drone as their dog breed and be like "actually this dog breed is safe, people just don't know how to train it"
------
upper middle class who had blue neoliberal parents and work as american economists / policy analyst / financier and are secretly red but not entirely twitter is such a wild place
------
reading hackernews and shaking my head the whole time so the people on twitter know i disagree with it
------
Today FLOPS is one of the things you can spend $ on.
Tomorrow $ is one of the things you can spend FLOPS on.
A reading from the church of FLOPS.
------
hackernews users hate fun
btw, if you're a hackernews user please lmk so i can unfollow you
------
the users are such pessimistic crabs it feels like they're mostly generated. like, you can't hate the kingdom of god THAT much, right?
------
ijbol theory fancam
------
this is e/acc
------
Square Watermelons In Japan.  Watermelons are grown inside square boxes to produce this unusual square shape. Suppliers have found them more convenient for stacking, shipping, and refrigerator storage.
------
As I have pointed out before, AI doomerism is a kind of apocalyptic cult.

Why would its most vocal advocates come from ultra-religious families (that they broke away from because of science)?
------
July 7: The @BostonGlobe published this profile:

"Dan Hendrycks wants to save us from an AI catastrophe. He's not sure he'll succeed."

Apparently, his evangelical beliefs have been replaced by a new religion:
AI Doomerism.

https://archive.vn/Sn1v0
------
me watching
- my 3D printer print a plant pot
- GPT4 code interpreter writing unit tests for data quality, and then correcting the failures it finds
------
I'm literally just tweeting and micromanaging my machines to do work for me
post scarcity is here and it's hilarious
------
The only AI agent that I've used and worked is GPT4 code interpreter
very good work from their team
it is not easy to tame the entropic bag of weights to do actual work without collapsing
------
"It says here you are going to work as a 'wagie in a cagie' for a convicted felon named Martin Skhreli, for 0 pay, for two weeks. And you will be staying at Under The Big Shkrellster's Desk Ave, NY.

Looks good to me. Have a good flight."
------
companies become one man shops, geo hotz style
wrap me in a predictive cocoon
no more organizational incentive misalignment
no more communication overhead
complete democratization & independence
we don't need more cores
when we can make our cores faster
------
Mr burns is e/acc
------
are there plants that can survive off of the dim glow of computer monitors?
------
so curious to see the difference in usage metrics after chatGPT started defaulting to gpt4
------
you'd be surprised how much tiny pieces of toil like that hurt retention & conversion
------
learning requires overcoming pain
learning to learn is learning to enjoy that kind of pain
------
ahhahahahah fair
------
one time i was at a mcdonalds drive through. didn't know who was first. we decided with rock papers scissors
------
you can solve the organizational incentive misalignment problem really easily: just replace all the humans with computers
------
AI progress is like looking out the window, as your parents drive you down the highway
Everyone else is looking at the mountains in the distance. Things seem to be moving slowly. 
But some people have their faces inches from the ground. And the road is whipping by
------
Making some progress. Keep iterating, work on the data - by the far the biggest thing that has changed the final result of the models is the data. Not just by a little, by a significant amount
------
my.. politics?
------
it is genuinely astounding what i am capable of using gpt4
holy shit
------
Let’s see what this rock can do
------
good morning, brothers and sisters
------
twitter follower count is so variable. for example, a month ago, both me and 
@goth600
 were at 1k followers. one week later we were both at 10k, but then three weeks later he's shot up to 36k while i'm still at 25k
------
"uhhhhhhh what hyperparameters should i use uhhhhhhh should i use weight decay uhhhhhhh should i use dropout uhhhhhhhhhhhhhhH"

in the time you spent deliberating over your hyperparameters, i've trained 8 qloras
I use vibes and the sound of the coil whine to evaluate them
Keep up
------
ffs my loss exploded
------
mfs will get mil spec pit bull rail gun drone as their dog breed and be like "actually this dog breed is safe, people just don't know how to train it"
------
upper middle class who had blue neoliberal parents and work as american economists / policy analyst / financier and are secretly red but not entirely twitter is such a wild place
------
reading hackernews and shaking my head the whole time so the people on twitter know i disagree with it
------
Today FLOPS is one of the things you can spend $ on.
Tomorrow $ is one of the things you can spend FLOPS on.
A reading from the church of FLOPS.
------
hackernews users hate fun
btw, if you're a hackernews user please lmk so i can unfollow you
------
the users are such pessimistic crabs it feels like they're mostly generated. like, you can't hate the kingdom of god THAT much, right?
------
ijbol theory fancam
------
this is e/acc
------
Square Watermelons In Japan.  Watermelons are grown inside square boxes to produce this unusual square shape. Suppliers have found them more convenient for stacking, shipping, and refrigerator storage.
------
As I have pointed out before, AI doomerism is a kind of apocalyptic cult.

Why would its most vocal advocates come from ultra-religious families (that they broke away from because of science)?
------
July 7: The @BostonGlobe published this profile:

"Dan Hendrycks wants to save us from an AI catastrophe. He's not sure he'll succeed."

Apparently, his evangelical beliefs have been replaced by a new religion:
AI Doomerism.

https://archive.vn/Sn1v0
------
me watching
- my 3D printer print a plant pot
- GPT4 code interpreter writing unit tests for data quality, and then correcting the failures it finds
------
I'm literally just tweeting and micromanaging my machines to do work for me
post scarcity is here and it's hilarious
------
The only AI agent that I've used and worked is GPT4 code interpreter
very good work from their team
it is not easy to tame the entropic bag of weights to do actual work without collapsing
------
"It says here you are going to work as a 'wagie in a cagie' for a convicted felon named Martin Skhreli, for 0 pay, for two weeks. And you will be staying at Under The Big Shkrellster's Desk Ave, NY.

Looks good to me. Have a good flight."
------
companies become one man shops, geo hotz style
wrap me in a predictive cocoon
no more organizational incentive misalignment
no more communication overhead
complete democratization & independence
we don't need more cores
when we can make our cores faster
------
Mr burns is e/acc
------
are there plants that can survive off of the dim glow of computer monitors?
------
so curious to see the difference in usage metrics after chatGPT started defaulting to gpt4
------
you'd be surprised how much tiny pieces of toil like that hurt retention & conversion
------
learning requires overcoming pain
learning to learn is learning to enjoy that kind of pain
------
ahhahahahah fair
------
one time i was at a mcdonalds drive through. didn't know who was first. we decided with rock papers scissors
------
you can solve the organizational incentive misalignment problem really easily: just replace all the humans with computers
------
AI progress is like looking out the window, as your parents drive you down the highway
Everyone else is looking at the mountains in the distance. Things seem to be moving slowly. 
But some people have their faces inches from the ground. And the road is whipping by
------
Making some progress. Keep iterating, work on the data - by the far the biggest thing that has changed the final result of the models is the data. Not just by a little, by a significant amount
------
my.. politics?
------
it is genuinely astounding what i am capable of using gpt4
holy shit
------
lifting the medium intelligence high focus toil out of my knowledge work has increased my capability to an astounding degree
------
i should really stream
------
good morning
------
Today, I will remind them
You can run inference on CPU
citation: https://johannesgaessler.github.io
------
mfs will get mil spec pit bull rail gun drone as their dog breed and be like "actually this dog breed is safe, people just don't know how to train it"
------
upper middle class who had blue neoliberal parents and work as american economists / policy analyst / financier and are secretly red but not entirely twitter is such a wild place
------
reading hackernews and shaking my head the whole time so the people on twitter know i disagree with it
------
Today FLOPS is one of the things you can spend $ on.
Tomorrow $ is one of the things you can spend FLOPS on.
A reading from the church of FLOPS.
------
hackernews users hate fun
btw, if you're a hackernews user please lmk so i can unfollow you
------
the users are such pessimistic crabs it feels like they're mostly generated. like, you can't hate the kingdom of god THAT much, right?
------
ijbol theory fancam
------
this is e/acc
------
Square Watermelons In Japan.  Watermelons are grown inside square boxes to produce this unusual square shape. Suppliers have found them more convenient for stacking, shipping, and refrigerator storage.
------
As I have pointed out before, AI doomerism is a kind of apocalyptic cult.

Why would its most vocal advocates come from ultra-religious families (that they broke away from because of science)?
------
July 7: The @BostonGlobe published this profile:

"Dan Hendrycks wants to save us from an AI catastrophe. He's not sure he'll succeed."

Apparently, his evangelical beliefs have been replaced by a new religion:
AI Doomerism.

https://archive.vn/Sn1v0
------
me watching
- my 3D printer print a plant pot
- GPT4 code interpreter writing unit tests for data quality, and then correcting the failures it finds
------
I'm literally just tweeting and micromanaging my machines to do work for me
post scarcity is here and it's hilarious
------
The only AI agent that I've used and worked is GPT4 code interpreter
very good work from their team
it is not easy to tame the entropic bag of weights to do actual work without collapsing
------
"It says here you are going to work as a 'wagie in a cagie' for a convicted felon named Martin Skhreli, for 0 pay, for two weeks. And you will be staying at Under The Big Shkrellster's Desk Ave, NY.

Looks good to me. Have a good flight."
------
companies become one man shops, geo hotz style
wrap me in a predictive cocoon
no more organizational incentive misalignment
no more communication overhead
complete democratization & independence
we don't need more cores
when we can make our cores faster
------
Mr burns is e/acc
------
are there plants that can survive off of the dim glow of computer monitors?
------
so curious to see the difference in usage metrics after chatGPT started defaulting to gpt4
------
you'd be surprised how much tiny pieces of toil like that hurt retention & conversion
------
learning requires overcoming pain
learning to learn is learning to enjoy that kind of pain
------
ahhahahahah fair
------
one time i was at a mcdonalds drive through. didn't know who was first. we decided with rock papers scissors
------
you can solve the organizational incentive misalignment problem really easily: just replace all the humans with computers
------
AI progress is like looking out the window, as your parents drive you down the highway
Everyone else is looking at the mountains in the distance. Things seem to be moving slowly. 
But some people have their faces inches from the ground. And the road is whipping by
------
Making some progress. Keep iterating, work on the data - by the far the biggest thing that has changed the final result of the models is the data. Not just by a little, by a significant amount
------
my.. politics?
------
it is genuinely astounding what i am capable of using gpt4
holy shit
------
lifting the medium intelligence high focus toil out of my knowledge work has increased my capability to an astounding degree
------
i should really stream
------
good morning
------
Today, I will remind them
You can run inference on CPU
citation: https://johannesgaessler.github.io
------
what reading fabrice's wikipedia page feels like
------
the root of all evil is skill issue
------
Incumbent journalists and super large journalist organizations have been completely replaced by independent writers on s*bst*tack and twitter
------
Sorry, but organizational distribution and power doesn't work after being democratized by technology
Your words have been deemed inadequate. 
The eyeballs that matter are looking elsewhere
And you (WaPo,nyt,CBC,etc) are going to disappear into irrelevancy
------
just kickin' it with the boys
------
"don't make me listen to Lex Fridman right now"
Me trying to show my wife that 
@plinz
 agrees with me that rocks are consciously experiencing the world (though less than a banana)
------
instead of writing custom parsers for every different modality of document format, why don't we just use what humans do and use vision models?
------
I'm sure it isn't that hard
------
literally screaming "yes!!" and tearing up as I listen to 
@plinz
 on Lex Fridman

he so eloquently puts what I feel into words
------
one very very small part of the paper points to an interesting conclusion about GPT-4
https://news.ycombinator.com/item?id=37006224…
------
“Sparse to soft MoE” (Puigcerver and Riquelme, https://arxiv.org/pdf/2308.00951.pdf…) feels like a complete game changer, super surprised very few people talk about it
------
this is e/acc
------
Square Watermelons In Japan.  Watermelons are grown inside square boxes to produce this unusual square shape. Suppliers have found them more convenient for stacking, shipping, and refrigerator storage.
------
As I have pointed out before, AI doomerism is a kind of apocalyptic cult.

Why would its most vocal advocates come from ultra-religious families (that they broke away from because of science)?
------
July 7: The @BostonGlobe published this profile:

"Dan Hendrycks wants to save us from an AI catastrophe. He's not sure he'll succeed."

Apparently, his evangelical beliefs have been replaced by a new religion:
AI Doomerism.

https://archive.vn/Sn1v0
------
me watching
- my 3D printer print a plant pot
- GPT4 code interpreter writing unit tests for data quality, and then correcting the failures it finds
------
I'm literally just tweeting and micromanaging my machines to do work for me
post scarcity is here and it's hilarious
------
The only AI agent that I've used and worked is GPT4 code interpreter
very good work from their team
it is not easy to tame the entropic bag of weights to do actual work without collapsing
------
"It says here you are going to work as a 'wagie in a cagie' for a convicted felon named Martin Skhreli, for 0 pay, for two weeks. And you will be staying at Under The Big Shkrellster's Desk Ave, NY.

Looks good to me. Have a good flight."
------
companies become one man shops, geo hotz style
wrap me in a predictive cocoon
no more organizational incentive misalignment
no more communication overhead
complete democratization & independence
we don't need more cores
when we can make our cores faster
------
Mr burns is e/acc
------
are there plants that can survive off of the dim glow of computer monitors?
------
so curious to see the difference in usage metrics after chatGPT started defaulting to gpt4
------
you'd be surprised how much tiny pieces of toil like that hurt retention & conversion
------
learning requires overcoming pain
learning to learn is learning to enjoy that kind of pain
------
ahhahahahah fair
------
one time i was at a mcdonalds drive through. didn't know who was first. we decided with rock papers scissors
------
you can solve the organizational incentive misalignment problem really easily: just replace all the humans with computers
------
AI progress is like looking out the window, as your parents drive you down the highway
Everyone else is looking at the mountains in the distance. Things seem to be moving slowly. 
But some people have their faces inches from the ground. And the road is whipping by
------
Making some progress. Keep iterating, work on the data - by the far the biggest thing that has changed the final result of the models is the data. Not just by a little, by a significant amount
------
my.. politics?
------
it is genuinely astounding what i am capable of using gpt4
holy shit
------
lifting the medium intelligence high focus toil out of my knowledge work has increased my capability to an astounding degree
------
i should really stream
------
good morning
------
Today, I will remind them
You can run inference on CPU
citation: https://johannesgaessler.github.io
------
what reading fabrice's wikipedia page feels like
------
the root of all evil is skill issue
------
Incumbent journalists and super large journalist organizations have been completely replaced by independent writers on s*bst*tack and twitter
------
Sorry, but organizational distribution and power doesn't work after being democratized by technology
Your words have been deemed inadequate. 
The eyeballs that matter are looking elsewhere
And you (WaPo,nyt,CBC,etc) are going to disappear into irrelevancy
------
just kickin' it with the boys
------
"don't make me listen to Lex Fridman right now"
Me trying to show my wife that 
@plinz
 agrees with me that rocks are consciously experiencing the world (though less than a banana)
------
instead of writing custom parsers for every different modality of document format, why don't we just use what humans do and use vision models?
------
I'm sure it isn't that hard
------
literally screaming "yes!!" and tearing up as I listen to 
@plinz
 on Lex Fridman

he so eloquently puts what I feel into words
------
one very very small part of the paper points to an interesting conclusion about GPT-4
https://news.ycombinator.com/item?id=37006224…
------
“Sparse to soft MoE” (Puigcerver and Riquelme, https://arxiv.org/pdf/2308.00951.pdf…) feels like a complete game changer, super surprised very few people talk about it
------
Fabrice Bellard:
- creator of FFmpeg
- creator of QEMU 
- creator of tiny C compiler
- held the world record for digits of pi calculated by coming up with his own algorithm
- created a compression scheme for text that holds the number 1 place
------
How can agents understand the world from diverse language? 

Excited to introduce Dynalang, an agent that learns to understand language by 𝙢𝙖𝙠𝙞𝙣𝙜 𝙥𝙧𝙚𝙙𝙞𝙘𝙩𝙞𝙤𝙣𝙨 𝙖𝙗𝙤𝙪𝙩 𝙩𝙝𝙚 𝙛𝙪𝙩𝙪𝙧𝙚 with a multimodal world model!
------
Technically speaking, does the government have a DAU count?
------
one might argue that the US government has more than 700 million users
------
hmmmmmm
------
Leetcode made a better father
------
i think it actually made me a better person overall
------
you might not like it, but mindlessly grinding leetcode actually does make you a good programmer
------
good morning
------
"It says here you are going to work as a 'wagie in a cagie' for a convicted felon named Martin Skhreli, for 0 pay, for two weeks. And you will be staying at Under The Big Shkrellster's Desk Ave, NY.

Looks good to me. Have a good flight."
------
companies become one man shops, geo hotz style
wrap me in a predictive cocoon
no more organizational incentive misalignment
no more communication overhead
complete democratization & independence
we don't need more cores
when we can make our cores faster
------
Mr burns is e/acc
------
are there plants that can survive off of the dim glow of computer monitors?
------
so curious to see the difference in usage metrics after chatGPT started defaulting to gpt4
------
you'd be surprised how much tiny pieces of toil like that hurt retention & conversion
------
learning requires overcoming pain
learning to learn is learning to enjoy that kind of pain
------
ahhahahahah fair
------
one time i was at a mcdonalds drive through. didn't know who was first. we decided with rock papers scissors
------
you can solve the organizational incentive misalignment problem really easily: just replace all the humans with computers
------
AI progress is like looking out the window, as your parents drive you down the highway
Everyone else is looking at the mountains in the distance. Things seem to be moving slowly. 
But some people have their faces inches from the ground. And the road is whipping by
------
Making some progress. Keep iterating, work on the data - by the far the biggest thing that has changed the final result of the models is the data. Not just by a little, by a significant amount
------
my.. politics?
------
it is genuinely astounding what i am capable of using gpt4
holy shit
------
lifting the medium intelligence high focus toil out of my knowledge work has increased my capability to an astounding degree
------
i should really stream
------
good morning
------
Today, I will remind them
You can run inference on CPU
citation: https://johannesgaessler.github.io
------
what reading fabrice's wikipedia page feels like
------
the root of all evil is skill issue
------
Incumbent journalists and super large journalist organizations have been completely replaced by independent writers on s*bst*tack and twitter
------
Sorry, but organizational distribution and power doesn't work after being democratized by technology
Your words have been deemed inadequate. 
The eyeballs that matter are looking elsewhere
And you (WaPo,nyt,CBC,etc) are going to disappear into irrelevancy
------
just kickin' it with the boys
------
"don't make me listen to Lex Fridman right now"
Me trying to show my wife that 
@plinz
 agrees with me that rocks are consciously experiencing the world (though less than a banana)
------
instead of writing custom parsers for every different modality of document format, why don't we just use what humans do and use vision models?
------
I'm sure it isn't that hard
------
literally screaming "yes!!" and tearing up as I listen to 
@plinz
 on Lex Fridman

he so eloquently puts what I feel into words
------
one very very small part of the paper points to an interesting conclusion about GPT-4
https://news.ycombinator.com/item?id=37006224…
------
“Sparse to soft MoE” (Puigcerver and Riquelme, https://arxiv.org/pdf/2308.00951.pdf…) feels like a complete game changer, super surprised very few people talk about it
------
Fabrice Bellard:
- creator of FFmpeg
- creator of QEMU 
- creator of tiny C compiler
- held the world record for digits of pi calculated by coming up with his own algorithm
- created a compression scheme for text that holds the number 1 place
------
How can agents understand the world from diverse language? 

Excited to introduce Dynalang, an agent that learns to understand language by 𝙢𝙖𝙠𝙞𝙣𝙜 𝙥𝙧𝙚𝙙𝙞𝙘𝙩𝙞𝙤𝙣𝙨 𝙖𝙗𝙤𝙪𝙩 𝙩𝙝𝙚 𝙛𝙪𝙩𝙪𝙧𝙚 with a multimodal world model!
------
Technically speaking, does the government have a DAU count?
------
one might argue that the US government has more than 700 million users
------
hmmmmmm
------
Leetcode made a better father
------
i think it actually made me a better person overall
------
you might not like it, but mindlessly grinding leetcode actually does make you a good programmer
------
good morning
------
ahhahahahah fair
------
one time i was at a mcdonalds drive through. didn't know who was first. we decided with rock papers scissors
------
you can solve the organizational incentive misalignment problem really easily: just replace all the humans with computers
------
AI progress is like looking out the window, as your parents drive you down the highway
Everyone else is looking at the mountains in the distance. Things seem to be moving slowly. 
But some people have their faces inches from the ground. And the road is whipping by
------
Making some progress. Keep iterating, work on the data - by the far the biggest thing that has changed the final result of the models is the data. Not just by a little, by a significant amount
------
my.. politics?
------
it is genuinely astounding what i am capable of using gpt4
holy shit
------
lifting the medium intelligence high focus toil out of my knowledge work has increased my capability to an astounding degree
------
i should really stream
------
good morning
------
Today, I will remind them
You can run inference on CPU
citation: https://johannesgaessler.github.io
------
what reading fabrice's wikipedia page feels like
------
the root of all evil is skill issue
------
Incumbent journalists and super large journalist organizations have been completely replaced by independent writers on s*bst*tack and twitter
------
Sorry, but organizational distribution and power doesn't work after being democratized by technology
Your words have been deemed inadequate. 
The eyeballs that matter are looking elsewhere
And you (WaPo,nyt,CBC,etc) are going to disappear into irrelevancy
------
just kickin' it with the boys
------
"don't make me listen to Lex Fridman right now"
Me trying to show my wife that 
@plinz
 agrees with me that rocks are consciously experiencing the world (though less than a banana)
------
instead of writing custom parsers for every different modality of document format, why don't we just use what humans do and use vision models?
------
I'm sure it isn't that hard
------
literally screaming "yes!!" and tearing up as I listen to 
@plinz
 on Lex Fridman

he so eloquently puts what I feel into words
------
one very very small part of the paper points to an interesting conclusion about GPT-4
https://news.ycombinator.com/item?id=37006224…
------
“Sparse to soft MoE” (Puigcerver and Riquelme, https://arxiv.org/pdf/2308.00951.pdf…) feels like a complete game changer, super surprised very few people talk about it
------
Fabrice Bellard:
- creator of FFmpeg
- creator of QEMU 
- creator of tiny C compiler
- held the world record for digits of pi calculated by coming up with his own algorithm
- created a compression scheme for text that holds the number 1 place
------
How can agents understand the world from diverse language? 

Excited to introduce Dynalang, an agent that learns to understand language by 𝙢𝙖𝙠𝙞𝙣𝙜 𝙥𝙧𝙚𝙙𝙞𝙘𝙩𝙞𝙤𝙣𝙨 𝙖𝙗𝙤𝙪𝙩 𝙩𝙝𝙚 𝙛𝙪𝙩𝙪𝙧𝙚 with a multimodal world model!
------
Technically speaking, does the government have a DAU count?
------
one might argue that the US government has more than 700 million users
------
hmmmmmm
------
Leetcode made a better father
------
i think it actually made me a better person overall
------
you might not like it, but mindlessly grinding leetcode actually does make you a good programmer
------
good morning
------
boot up my x display manager, fire up my xeb browser, send some xeets on x, get my x bank account deposit for my xad xevenue xharing, xall my xom using my xr xeadxet

thanks xelon xusk
------
me trying to get up in the morning
------
Meissner effect or bust: Day 8.5

We made the rocks
------
the reason why my troll bait is so irresistible is because:
i'm actually not joking
------
>I'm sorry... I just don't know how to do a directed acyclic graph dynamic reverse topological sort... we can stop the interview early... sorry for wasting your time
------
gpt4 doesn't complain, why are you?
------
RLHF is central planning
AI ethics is central planning
Regulation is central planning

You can have laws for when behaviour encroaches upon rights but anything short of a totally decentralised consensus framework for AI is insufficient.
------
i'm 6'3 btw
------
good morning
------
Today, I will remind them
You can run inference on CPU
citation: https://johannesgaessler.github.io
------
what reading fabrice's wikipedia page feels like
------
the root of all evil is skill issue
------
Incumbent journalists and super large journalist organizations have been completely replaced by independent writers on s*bst*tack and twitter
------
Sorry, but organizational distribution and power doesn't work after being democratized by technology
Your words have been deemed inadequate. 
The eyeballs that matter are looking elsewhere
And you (WaPo,nyt,CBC,etc) are going to disappear into irrelevancy
------
just kickin' it with the boys
------
"don't make me listen to Lex Fridman right now"
Me trying to show my wife that 
@plinz
 agrees with me that rocks are consciously experiencing the world (though less than a banana)
------
instead of writing custom parsers for every different modality of document format, why don't we just use what humans do and use vision models?
------
I'm sure it isn't that hard
------
literally screaming "yes!!" and tearing up as I listen to 
@plinz
 on Lex Fridman

he so eloquently puts what I feel into words
------
one very very small part of the paper points to an interesting conclusion about GPT-4
https://news.ycombinator.com/item?id=37006224…
------
“Sparse to soft MoE” (Puigcerver and Riquelme, https://arxiv.org/pdf/2308.00951.pdf…) feels like a complete game changer, super surprised very few people talk about it
------
Fabrice Bellard:
- creator of FFmpeg
- creator of QEMU 
- creator of tiny C compiler
- held the world record for digits of pi calculated by coming up with his own algorithm
- created a compression scheme for text that holds the number 1 place
------
How can agents understand the world from diverse language? 

Excited to introduce Dynalang, an agent that learns to understand language by 𝙢𝙖𝙠𝙞𝙣𝙜 𝙥𝙧𝙚𝙙𝙞𝙘𝙩𝙞𝙤𝙣𝙨 𝙖𝙗𝙤𝙪𝙩 𝙩𝙝𝙚 𝙛𝙪𝙩𝙪𝙧𝙚 with a multimodal world model!
------
Technically speaking, does the government have a DAU count?
------
one might argue that the US government has more than 700 million users
------
hmmmmmm
------
Leetcode made a better father
------
i think it actually made me a better person overall
------
you might not like it, but mindlessly grinding leetcode actually does make you a good programmer
------
good morning
------
boot up my x display manager, fire up my xeb browser, send some xeets on x, get my x bank account deposit for my xad xevenue xharing, xall my xom using my xr xeadxet

thanks xelon xusk
------
me trying to get up in the morning
------
Meissner effect or bust: Day 8.5

We made the rocks
------
the reason why my troll bait is so irresistible is because:
i'm actually not joking
------
>I'm sorry... I just don't know how to do a directed acyclic graph dynamic reverse topological sort... we can stop the interview early... sorry for wasting your time
------
gpt4 doesn't complain, why are you?
------
RLHF is central planning
AI ethics is central planning
Regulation is central planning

You can have laws for when behaviour encroaches upon rights but anything short of a totally decentralised consensus framework for AI is insufficient.
------
i'm 6'3 btw
------
The goal should be to make AI so useful in so many ways to so many people that any legal threat or law that would severely diminish it's access or quality would be met with severe backlash by the public
------
imagine wanting to slow all of this down?
------
anon is concerned about getting another GPU
------
A lot of ok enough monkeys make one ok enough mixture of monkeys
------
francois chollet is based
------
it is so weird training (parameter efficient tuning) a model with less than 1k samples and then seeing it generalize. it goes against everything i've believed up until this point
------
It's preview day again!

We are on top of all the leaderboards again with our new 13B!  This time we have Llama2 and we're outpacing the original Orca for Microsoft, plus we did it with <10% of the compute budget!

https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B…

Even bigger news soon too!

 1..
------
For me, nothing beats trying iteration after iteration. A few times I hit walls and made no progress for days, but managed to push myself to read more and more importantly to keep trying. Keep a track of things that work and didn’t work
------
The high-order bit that changed in AI:
"I'll give you 10X bigger computer"
- 10 years ago: I'm not immediately sure what to do with it
- Now: Not only do I know exactly what to do with it but I can predict the metrics I will achieve
Algorithmic progress was necessity, now bonus.
------
Sorry, but organizational distribution and power doesn't work after being democratized by technology
Your words have been deemed inadequate. 
The eyeballs that matter are looking elsewhere
And you (WaPo,nyt,CBC,etc) are going to disappear into irrelevancy
------
just kickin' it with the boys
------
"don't make me listen to Lex Fridman right now"
Me trying to show my wife that 
@plinz
 agrees with me that rocks are consciously experiencing the world (though less than a banana)
------
instead of writing custom parsers for every different modality of document format, why don't we just use what humans do and use vision models?
------
I'm sure it isn't that hard
------
literally screaming "yes!!" and tearing up as I listen to 
@plinz
 on Lex Fridman

he so eloquently puts what I feel into words
------
one very very small part of the paper points to an interesting conclusion about GPT-4
https://news.ycombinator.com/item?id=37006224…
------
“Sparse to soft MoE” (Puigcerver and Riquelme, https://arxiv.org/pdf/2308.00951.pdf…) feels like a complete game changer, super surprised very few people talk about it
------
Fabrice Bellard:
- creator of FFmpeg
- creator of QEMU 
- creator of tiny C compiler
- held the world record for digits of pi calculated by coming up with his own algorithm
- created a compression scheme for text that holds the number 1 place
------
How can agents understand the world from diverse language? 

Excited to introduce Dynalang, an agent that learns to understand language by 𝙢𝙖𝙠𝙞𝙣𝙜 𝙥𝙧𝙚𝙙𝙞𝙘𝙩𝙞𝙤𝙣𝙨 𝙖𝙗𝙤𝙪𝙩 𝙩𝙝𝙚 𝙛𝙪𝙩𝙪𝙧𝙚 with a multimodal world model!
------
Technically speaking, does the government have a DAU count?
------
one might argue that the US government has more than 700 million users
------
hmmmmmm
------
Leetcode made a better father
------
i think it actually made me a better person overall
------
you might not like it, but mindlessly grinding leetcode actually does make you a good programmer
------
good morning
------
boot up my x display manager, fire up my xeb browser, send some xeets on x, get my x bank account deposit for my xad xevenue xharing, xall my xom using my xr xeadxet

thanks xelon xusk
------
me trying to get up in the morning
------
Meissner effect or bust: Day 8.5

We made the rocks
------
the reason why my troll bait is so irresistible is because:
i'm actually not joking
------
>I'm sorry... I just don't know how to do a directed acyclic graph dynamic reverse topological sort... we can stop the interview early... sorry for wasting your time
------
gpt4 doesn't complain, why are you?
------
RLHF is central planning
AI ethics is central planning
Regulation is central planning

You can have laws for when behaviour encroaches upon rights but anything short of a totally decentralised consensus framework for AI is insufficient.
------
i'm 6'3 btw
------
The goal should be to make AI so useful in so many ways to so many people that any legal threat or law that would severely diminish it's access or quality would be met with severe backlash by the public
------
imagine wanting to slow all of this down?
------
anon is concerned about getting another GPU
------
A lot of ok enough monkeys make one ok enough mixture of monkeys
------
francois chollet is based
------
it is so weird training (parameter efficient tuning) a model with less than 1k samples and then seeing it generalize. it goes against everything i've believed up until this point
------
It's preview day again!

We are on top of all the leaderboards again with our new 13B!  This time we have Llama2 and we're outpacing the original Orca for Microsoft, plus we did it with <10% of the compute budget!

https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B…

Even bigger news soon too!

 1..
------
For me, nothing beats trying iteration after iteration. A few times I hit walls and made no progress for days, but managed to push myself to read more and more importantly to keep trying. Keep a track of things that work and didn’t work
------
The high-order bit that changed in AI:
"I'll give you 10X bigger computer"
- 10 years ago: I'm not immediately sure what to do with it
- Now: Not only do I know exactly what to do with it but I can predict the metrics I will achieve
Algorithmic progress was necessity, now bonus.
------
I made someone technical very upset by being wrong today
------
you guys made it worse by liking my wrong tweet
------
Design patterns
------
Skill It! https://arxiv.org/abs/2307.14430
"Just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data"
surprising yet intuitive result
and empirically shown
this paper is a banger
------
an LLM better than GPT4
it always does what i want it to do
------
Fabrice Bellard:
- creator of FFmpeg
- creator of QEMU 
- creator of tiny C compiler
- held the world record for digits of pi calculated by coming up with his own algorithm
- created a compression scheme for text that holds the number 1 place
------
How can agents understand the world from diverse language? 

Excited to introduce Dynalang, an agent that learns to understand language by 𝙢𝙖𝙠𝙞𝙣𝙜 𝙥𝙧𝙚𝙙𝙞𝙘𝙩𝙞𝙤𝙣𝙨 𝙖𝙗𝙤𝙪𝙩 𝙩𝙝𝙚 𝙛𝙪𝙩𝙪𝙧𝙚 with a multimodal world model!
------
Technically speaking, does the government have a DAU count?
------
one might argue that the US government has more than 700 million users
------
hmmmmmm
------
Leetcode made a better father
------
i think it actually made me a better person overall
------
you might not like it, but mindlessly grinding leetcode actually does make you a good programmer
------
good morning
------
boot up my x display manager, fire up my xeb browser, send some xeets on x, get my x bank account deposit for my xad xevenue xharing, xall my xom using my xr xeadxet

thanks xelon xusk
------
me trying to get up in the morning
------
Meissner effect or bust: Day 8.5

We made the rocks
------
the reason why my troll bait is so irresistible is because:
i'm actually not joking
------
>I'm sorry... I just don't know how to do a directed acyclic graph dynamic reverse topological sort... we can stop the interview early... sorry for wasting your time
------
gpt4 doesn't complain, why are you?
------
RLHF is central planning
AI ethics is central planning
Regulation is central planning

You can have laws for when behaviour encroaches upon rights but anything short of a totally decentralised consensus framework for AI is insufficient.
------
i'm 6'3 btw
------
The goal should be to make AI so useful in so many ways to so many people that any legal threat or law that would severely diminish it's access or quality would be met with severe backlash by the public
------
imagine wanting to slow all of this down?
------
anon is concerned about getting another GPU
------
A lot of ok enough monkeys make one ok enough mixture of monkeys
------
francois chollet is based
------
it is so weird training (parameter efficient tuning) a model with less than 1k samples and then seeing it generalize. it goes against everything i've believed up until this point
------
It's preview day again!

We are on top of all the leaderboards again with our new 13B!  This time we have Llama2 and we're outpacing the original Orca for Microsoft, plus we did it with <10% of the compute budget!

https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B…

Even bigger news soon too!

 1..
------
For me, nothing beats trying iteration after iteration. A few times I hit walls and made no progress for days, but managed to push myself to read more and more importantly to keep trying. Keep a track of things that work and didn’t work
------
The high-order bit that changed in AI:
"I'll give you 10X bigger computer"
- 10 years ago: I'm not immediately sure what to do with it
- Now: Not only do I know exactly what to do with it but I can predict the metrics I will achieve
Algorithmic progress was necessity, now bonus.
------
I made someone technical very upset by being wrong today
------
you guys made it worse by liking my wrong tweet
------
Design patterns
------
Skill It! https://arxiv.org/abs/2307.14430
"Just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data"
surprising yet intuitive result
and empirically shown
this paper is a banger
------
an LLM better than GPT4
it always does what i want it to do
------
The training loss curve. The score pass@1 25.5%, model size just 1B parameters, 6k dataset size. Contamination? Only 4.27% canonical solutions, vs wizardcoder 4.88%

You don't need a ton of data
------
Knees are weak
Palms sweaty
CUDA version out of date already
Torch won't run out of memory

But on the surface he's calm and ready
SSH into head node with his creddies
Nano the script for some quick eddies

Snap back to reality
There goes capacity
Aws might after me
P5 overpriced… Show more
------
your vision’s blurry… 
can you still cook? 
------
copy code hotkey copy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkey
------
We’re rolling out a bunch of small updates to improve the ChatGPT experience. Shipping over the next week:

1. Prompt examples: A blank page can be intimidating. At the beginning of a new chat, you’ll now see examples to help you get started. 
2. Suggested replies: Go deeper with… Show more
------
yes! thank you 
@JMT3
 
copy last code on chatgpt will be very satisfying to hit
time to unplug my mouse
------
good morning
------
boot up my x display manager, fire up my xeb browser, send some xeets on x, get my x bank account deposit for my xad xevenue xharing, xall my xom using my xr xeadxet

thanks xelon xusk
------
me trying to get up in the morning
------
Meissner effect or bust: Day 8.5

We made the rocks
------
the reason why my troll bait is so irresistible is because:
i'm actually not joking
------
>I'm sorry... I just don't know how to do a directed acyclic graph dynamic reverse topological sort... we can stop the interview early... sorry for wasting your time
------
gpt4 doesn't complain, why are you?
------
RLHF is central planning
AI ethics is central planning
Regulation is central planning

You can have laws for when behaviour encroaches upon rights but anything short of a totally decentralised consensus framework for AI is insufficient.
------
i'm 6'3 btw
------
The goal should be to make AI so useful in so many ways to so many people that any legal threat or law that would severely diminish it's access or quality would be met with severe backlash by the public
------
imagine wanting to slow all of this down?
------
anon is concerned about getting another GPU
------
A lot of ok enough monkeys make one ok enough mixture of monkeys
------
francois chollet is based
------
it is so weird training (parameter efficient tuning) a model with less than 1k samples and then seeing it generalize. it goes against everything i've believed up until this point
------
It's preview day again!

We are on top of all the leaderboards again with our new 13B!  This time we have Llama2 and we're outpacing the original Orca for Microsoft, plus we did it with <10% of the compute budget!

https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B…

Even bigger news soon too!

 1..
------
For me, nothing beats trying iteration after iteration. A few times I hit walls and made no progress for days, but managed to push myself to read more and more importantly to keep trying. Keep a track of things that work and didn’t work
------
The high-order bit that changed in AI:
"I'll give you 10X bigger computer"
- 10 years ago: I'm not immediately sure what to do with it
- Now: Not only do I know exactly what to do with it but I can predict the metrics I will achieve
Algorithmic progress was necessity, now bonus.
------
I made someone technical very upset by being wrong today
------
you guys made it worse by liking my wrong tweet
------
Design patterns
------
Skill It! https://arxiv.org/abs/2307.14430
"Just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data"
surprising yet intuitive result
and empirically shown
this paper is a banger
------
an LLM better than GPT4
it always does what i want it to do
------
The training loss curve. The score pass@1 25.5%, model size just 1B parameters, 6k dataset size. Contamination? Only 4.27% canonical solutions, vs wizardcoder 4.88%

You don't need a ton of data
------
Knees are weak
Palms sweaty
CUDA version out of date already
Torch won't run out of memory

But on the surface he's calm and ready
SSH into head node with his creddies
Nano the script for some quick eddies

Snap back to reality
There goes capacity
Aws might after me
P5 overpriced… Show more
------
your vision’s blurry… 
can you still cook? 
------
copy code hotkey copy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkey
------
We’re rolling out a bunch of small updates to improve the ChatGPT experience. Shipping over the next week:

1. Prompt examples: A blank page can be intimidating. At the beginning of a new chat, you’ll now see examples to help you get started. 
2. Suggested replies: Go deeper with… Show more
------
yes! thank you 
@JMT3
 
copy last code on chatgpt will be very satisfying to hit
time to unplug my mouse
------
My mom is visiting and we both have vscode open
------
lol my mom referred to karpathy as "the AI jeune"
------
my mom wrote a transformer from scratch
i blew her mind by pasting in her logs to code interpreter after asking for a chart
------
the reason why my troll bait is so irresistible is because:
i'm actually not joking
------
>I'm sorry... I just don't know how to do a directed acyclic graph dynamic reverse topological sort... we can stop the interview early... sorry for wasting your time
------
gpt4 doesn't complain, why are you?
------
RLHF is central planning
AI ethics is central planning
Regulation is central planning

You can have laws for when behaviour encroaches upon rights but anything short of a totally decentralised consensus framework for AI is insufficient.
------
i'm 6'3 btw
------
The goal should be to make AI so useful in so many ways to so many people that any legal threat or law that would severely diminish it's access or quality would be met with severe backlash by the public
------
imagine wanting to slow all of this down?
------
anon is concerned about getting another GPU
------
A lot of ok enough monkeys make one ok enough mixture of monkeys
------
francois chollet is based
------
it is so weird training (parameter efficient tuning) a model with less than 1k samples and then seeing it generalize. it goes against everything i've believed up until this point
------
It's preview day again!

We are on top of all the leaderboards again with our new 13B!  This time we have Llama2 and we're outpacing the original Orca for Microsoft, plus we did it with <10% of the compute budget!

https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B…

Even bigger news soon too!

 1..
------
For me, nothing beats trying iteration after iteration. A few times I hit walls and made no progress for days, but managed to push myself to read more and more importantly to keep trying. Keep a track of things that work and didn’t work
------
The high-order bit that changed in AI:
"I'll give you 10X bigger computer"
- 10 years ago: I'm not immediately sure what to do with it
- Now: Not only do I know exactly what to do with it but I can predict the metrics I will achieve
Algorithmic progress was necessity, now bonus.
------
I made someone technical very upset by being wrong today
------
you guys made it worse by liking my wrong tweet
------
Design patterns
------
Skill It! https://arxiv.org/abs/2307.14430
"Just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data"
surprising yet intuitive result
and empirically shown
this paper is a banger
------
an LLM better than GPT4
it always does what i want it to do
------
The training loss curve. The score pass@1 25.5%, model size just 1B parameters, 6k dataset size. Contamination? Only 4.27% canonical solutions, vs wizardcoder 4.88%

You don't need a ton of data
------
Knees are weak
Palms sweaty
CUDA version out of date already
Torch won't run out of memory

But on the surface he's calm and ready
SSH into head node with his creddies
Nano the script for some quick eddies

Snap back to reality
There goes capacity
Aws might after me
P5 overpriced… Show more
------
your vision’s blurry… 
can you still cook? 
------
copy code hotkey copy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkey
------
We’re rolling out a bunch of small updates to improve the ChatGPT experience. Shipping over the next week:

1. Prompt examples: A blank page can be intimidating. At the beginning of a new chat, you’ll now see examples to help you get started. 
2. Suggested replies: Go deeper with… Show more
------
yes! thank you 
@JMT3
 
copy last code on chatgpt will be very satisfying to hit
time to unplug my mouse
------
My mom is visiting and we both have vscode open
------
lol my mom referred to karpathy as "the AI jeune"
------
my mom wrote a transformer from scratch
i blew her mind by pasting in her logs to code interpreter after asking for a chart
------
you might not like it, but mindlessly grinding leetcode actually does make you a good programmer
------
Legitimately better than Google. When I search google for my often very specific question, I get at best adjascent answers (often ads disguised as answers), and don't really get too close to the answer I seek
------
If measured against the resources lay people actually use, I predict LLMs would already be an improvement over the status quo.

And the important part is that they will be even better tomorrow, and better still the day after that.

Ignoring the value here is peak midwit.
------
good morning
------
getting cheap plastic crap advertisements is actually great now that I have a 3D printer. So many design ideas
------
One way to think about this: with really good models your vocab (# of symbols) becomes much larger. The model understands the symbol "face", thus what was once hundreds of thousands of symbols (pixels) is now just one (This is an oversimplification obviously).
------
it is redundant data, depending on the context
there is a new dimension in information theory that isn't explored
and that's how entropy decreases the more integrated knowledge (literally, intelligence) the observing agent has
------
imagine wanting to slow all of this down?
------
anon is concerned about getting another GPU
------
A lot of ok enough monkeys make one ok enough mixture of monkeys
------
francois chollet is based
------
it is so weird training (parameter efficient tuning) a model with less than 1k samples and then seeing it generalize. it goes against everything i've believed up until this point
------
It's preview day again!

We are on top of all the leaderboards again with our new 13B!  This time we have Llama2 and we're outpacing the original Orca for Microsoft, plus we did it with <10% of the compute budget!

https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B…

Even bigger news soon too!

 1..
------
For me, nothing beats trying iteration after iteration. A few times I hit walls and made no progress for days, but managed to push myself to read more and more importantly to keep trying. Keep a track of things that work and didn’t work
------
The high-order bit that changed in AI:
"I'll give you 10X bigger computer"
- 10 years ago: I'm not immediately sure what to do with it
- Now: Not only do I know exactly what to do with it but I can predict the metrics I will achieve
Algorithmic progress was necessity, now bonus.
------
I made someone technical very upset by being wrong today
------
you guys made it worse by liking my wrong tweet
------
Design patterns
------
Skill It! https://arxiv.org/abs/2307.14430
"Just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data"
surprising yet intuitive result
and empirically shown
this paper is a banger
------
an LLM better than GPT4
it always does what i want it to do
------
The training loss curve. The score pass@1 25.5%, model size just 1B parameters, 6k dataset size. Contamination? Only 4.27% canonical solutions, vs wizardcoder 4.88%

You don't need a ton of data
------
Knees are weak
Palms sweaty
CUDA version out of date already
Torch won't run out of memory

But on the surface he's calm and ready
SSH into head node with his creddies
Nano the script for some quick eddies

Snap back to reality
There goes capacity
Aws might after me
P5 overpriced… Show more
------
your vision’s blurry… 
can you still cook? 
------
copy code hotkey copy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkey
------
We’re rolling out a bunch of small updates to improve the ChatGPT experience. Shipping over the next week:

1. Prompt examples: A blank page can be intimidating. At the beginning of a new chat, you’ll now see examples to help you get started. 
2. Suggested replies: Go deeper with… Show more
------
yes! thank you 
@JMT3
 
copy last code on chatgpt will be very satisfying to hit
time to unplug my mouse
------
My mom is visiting and we both have vscode open
------
lol my mom referred to karpathy as "the AI jeune"
------
my mom wrote a transformer from scratch
i blew her mind by pasting in her logs to code interpreter after asking for a chart
------
you might not like it, but mindlessly grinding leetcode actually does make you a good programmer
------
Legitimately better than Google. When I search google for my often very specific question, I get at best adjascent answers (often ads disguised as answers), and don't really get too close to the answer I seek
------
If measured against the resources lay people actually use, I predict LLMs would already be an improvement over the status quo.

And the important part is that they will be even better tomorrow, and better still the day after that.

Ignoring the value here is peak midwit.
------
good morning
------
getting cheap plastic crap advertisements is actually great now that I have a 3D printer. So many design ideas
------
One way to think about this: with really good models your vocab (# of symbols) becomes much larger. The model understands the symbol "face", thus what was once hundreds of thousands of symbols (pixels) is now just one (This is an oversimplification obviously).
------
it is redundant data, depending on the context
there is a new dimension in information theory that isn't explored
and that's how entropy decreases the more integrated knowledge (literally, intelligence) the observing agent has
------
I wonder if we're severely underestimating the effect that GPT4 godlike AI models are having & will have on the rate of breakthroughs

the top percentiles must be getting really goosed
------
Design patterns
------
Skill It! https://arxiv.org/abs/2307.14430
"Just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data"
surprising yet intuitive result
and empirically shown
this paper is a banger
------
an LLM better than GPT4
it always does what i want it to do
------
The training loss curve. The score pass@1 25.5%, model size just 1B parameters, 6k dataset size. Contamination? Only 4.27% canonical solutions, vs wizardcoder 4.88%

You don't need a ton of data
------
Knees are weak
Palms sweaty
CUDA version out of date already
Torch won't run out of memory

But on the surface he's calm and ready
SSH into head node with his creddies
Nano the script for some quick eddies

Snap back to reality
There goes capacity
Aws might after me
P5 overpriced… Show more
------
your vision’s blurry… 
can you still cook? 
------
copy code hotkey copy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkey
------
We’re rolling out a bunch of small updates to improve the ChatGPT experience. Shipping over the next week:

1. Prompt examples: A blank page can be intimidating. At the beginning of a new chat, you’ll now see examples to help you get started. 
2. Suggested replies: Go deeper with… Show more
------
yes! thank you 
@JMT3
 
copy last code on chatgpt will be very satisfying to hit
time to unplug my mouse
------
My mom is visiting and we both have vscode open
------
lol my mom referred to karpathy as "the AI jeune"
------
my mom wrote a transformer from scratch
i blew her mind by pasting in her logs to code interpreter after asking for a chart
------
you might not like it, but mindlessly grinding leetcode actually does make you a good programmer
------
Legitimately better than Google. When I search google for my often very specific question, I get at best adjascent answers (often ads disguised as answers), and don't really get too close to the answer I seek
------
If measured against the resources lay people actually use, I predict LLMs would already be an improvement over the status quo.

And the important part is that they will be even better tomorrow, and better still the day after that.

Ignoring the value here is peak midwit.
------
good morning
------
getting cheap plastic crap advertisements is actually great now that I have a 3D printer. So many design ideas
------
One way to think about this: with really good models your vocab (# of symbols) becomes much larger. The model understands the symbol "face", thus what was once hundreds of thousands of symbols (pixels) is now just one (This is an oversimplification obviously).
------
it is redundant data, depending on the context
there is a new dimension in information theory that isn't explored
and that's how entropy decreases the more integrated knowledge (literally, intelligence) the observing agent has
------
I wonder if we're severely underestimating the effect that GPT4 godlike AI models are having & will have on the rate of breakthroughs

the top percentiles must be getting really goosed
------
Every skill will converge to the ability to clearly express your ideas and the drive to execute on them
------
http://www1.ece.neu.edu/~naderi/Claude%20Shannon.html…
------
The government is being heavily pressured to ban open source. How can we prevent this? By campaigning in favor of open source and electing officials who understand the fundamental importance of it.
------
joji’ing my way through a situationship i should be filthy franking
------
many cases
------
SOFTWARE
------
HARDWARE
------
GODWARE
------
No.
The research arm of Bell Labs was never about moonshots.
It was about hiring the best scientists into small departments (typically 5 to 15 people) and giving them resources and a *lot* of freedom to work on what *they* deemed most promising.
That's how you get breakthroughs.
------
For an unknown limited time (probably until some crashing bug), I am hosting Stable Beluga 2 70B (a tune of Llama 2 70B) which is a little more "guide-able" than Llama 2 via prompting. 

It's a streaming-chat Gradio app hosted on 
@LambdaAPI
 cloud.

http://209.20.159.223:7860/
------
Going on a long drive. Put on the latest 
@Plinz
 podcast. Life is good
------
The training loss curve. The score pass@1 25.5%, model size just 1B parameters, 6k dataset size. Contamination? Only 4.27% canonical solutions, vs wizardcoder 4.88%

You don't need a ton of data
------
Knees are weak
Palms sweaty
CUDA version out of date already
Torch won't run out of memory

But on the surface he's calm and ready
SSH into head node with his creddies
Nano the script for some quick eddies

Snap back to reality
There goes capacity
Aws might after me
P5 overpriced… Show more
------
your vision’s blurry… 
can you still cook? 
------
copy code hotkey copy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkeycopy code hotkey
------
We’re rolling out a bunch of small updates to improve the ChatGPT experience. Shipping over the next week:

1. Prompt examples: A blank page can be intimidating. At the beginning of a new chat, you’ll now see examples to help you get started. 
2. Suggested replies: Go deeper with… Show more
------
yes! thank you 
@JMT3
 
copy last code on chatgpt will be very satisfying to hit
time to unplug my mouse
------
My mom is visiting and we both have vscode open
------
lol my mom referred to karpathy as "the AI jeune"
------
my mom wrote a transformer from scratch
i blew her mind by pasting in her logs to code interpreter after asking for a chart
------
you might not like it, but mindlessly grinding leetcode actually does make you a good programmer
------
Legitimately better than Google. When I search google for my often very specific question, I get at best adjascent answers (often ads disguised as answers), and don't really get too close to the answer I seek
------
If measured against the resources lay people actually use, I predict LLMs would already be an improvement over the status quo.

And the important part is that they will be even better tomorrow, and better still the day after that.

Ignoring the value here is peak midwit.
------
good morning
------
getting cheap plastic crap advertisements is actually great now that I have a 3D printer. So many design ideas
------
One way to think about this: with really good models your vocab (# of symbols) becomes much larger. The model understands the symbol "face", thus what was once hundreds of thousands of symbols (pixels) is now just one (This is an oversimplification obviously).
------
it is redundant data, depending on the context
there is a new dimension in information theory that isn't explored
and that's how entropy decreases the more integrated knowledge (literally, intelligence) the observing agent has
------
I wonder if we're severely underestimating the effect that GPT4 godlike AI models are having & will have on the rate of breakthroughs

the top percentiles must be getting really goosed
------
Every skill will converge to the ability to clearly express your ideas and the drive to execute on them
------
http://www1.ece.neu.edu/~naderi/Claude%20Shannon.html…
------
The government is being heavily pressured to ban open source. How can we prevent this? By campaigning in favor of open source and electing officials who understand the fundamental importance of it.
------
joji’ing my way through a situationship i should be filthy franking
------
many cases
------
SOFTWARE
------
HARDWARE
------
GODWARE
------
No.
The research arm of Bell Labs was never about moonshots.
It was about hiring the best scientists into small departments (typically 5 to 15 people) and giving them resources and a *lot* of freedom to work on what *they* deemed most promising.
That's how you get breakthroughs.
------
For an unknown limited time (probably until some crashing bug), I am hosting Stable Beluga 2 70B (a tune of Llama 2 70B) which is a little more "guide-able" than Llama 2 via prompting. 

It's a streaming-chat Gradio app hosted on 
@LambdaAPI
 cloud.

http://209.20.159.223:7860/
------
Going on a long drive. Put on the latest 
@Plinz
 podcast. Life is good
------
I have felt a great disturbance in the force, particularly in Canada
------
this is what post scarcity looks like
------
Does anyone work anymore?

Went on a walk Monday at 11:00 am and there were young people everywhere.

Working age people.

Between all these 'creator' types, remote work, quiet quitting, etc...

I honestly wonder how many people even work anymore.

Is your city like that?
------
>your last saved photo is your intelligence level
------
your last saved photo is your intelligence level  twitter.com/Soul0Engineer/…
------
On a mission to find the most talented software engineer

Tag the first account you can think of that's more proficient in software engineering than you. If you get tagged, do the same underneath.
------
I'm putting together a team to build Mentat. I need 10x engineers to push the frontier of possibility w/ LLMs. If that's you, dm!

- work w/ small crack team on ambitious project
- open source: tweet about what you build
- apply research to make something real
- good pay + equity twitter.com/bio_bootloader…
------
Automate your day job so you can work on your side projects
------
prediction: if we do discover an RTSC the turnaround to industrial use will be head spinningly fast. there’s too many startup minded well funded engineers across the world all coordinating discoveries in real time with arxiv paper drops
------
yes! thank you 
@JMT3
 
copy last code on chatgpt will be very satisfying to hit
time to unplug my mouse
------
My mom is visiting and we both have vscode open
------
lol my mom referred to karpathy as "the AI jeune"
------
my mom wrote a transformer from scratch
i blew her mind by pasting in her logs to code interpreter after asking for a chart
------
you might not like it, but mindlessly grinding leetcode actually does make you a good programmer
------
Legitimately better than Google. When I search google for my often very specific question, I get at best adjascent answers (often ads disguised as answers), and don't really get too close to the answer I seek
------
If measured against the resources lay people actually use, I predict LLMs would already be an improvement over the status quo.

And the important part is that they will be even better tomorrow, and better still the day after that.

Ignoring the value here is peak midwit.
------
good morning
------
getting cheap plastic crap advertisements is actually great now that I have a 3D printer. So many design ideas
------
One way to think about this: with really good models your vocab (# of symbols) becomes much larger. The model understands the symbol "face", thus what was once hundreds of thousands of symbols (pixels) is now just one (This is an oversimplification obviously).
------
it is redundant data, depending on the context
there is a new dimension in information theory that isn't explored
and that's how entropy decreases the more integrated knowledge (literally, intelligence) the observing agent has
------
I wonder if we're severely underestimating the effect that GPT4 godlike AI models are having & will have on the rate of breakthroughs

the top percentiles must be getting really goosed
------
Every skill will converge to the ability to clearly express your ideas and the drive to execute on them
------
http://www1.ece.neu.edu/~naderi/Claude%20Shannon.html…
------
The government is being heavily pressured to ban open source. How can we prevent this? By campaigning in favor of open source and electing officials who understand the fundamental importance of it.
------
joji’ing my way through a situationship i should be filthy franking
------
many cases
------
SOFTWARE
------
HARDWARE
------
GODWARE
------
No.
The research arm of Bell Labs was never about moonshots.
It was about hiring the best scientists into small departments (typically 5 to 15 people) and giving them resources and a *lot* of freedom to work on what *they* deemed most promising.
That's how you get breakthroughs.
------
For an unknown limited time (probably until some crashing bug), I am hosting Stable Beluga 2 70B (a tune of Llama 2 70B) which is a little more "guide-able" than Llama 2 via prompting. 

It's a streaming-chat Gradio app hosted on 
@LambdaAPI
 cloud.

http://209.20.159.223:7860/
------
Going on a long drive. Put on the latest 
@Plinz
 podcast. Life is good
------
I have felt a great disturbance in the force, particularly in Canada
------
this is what post scarcity looks like
------
Does anyone work anymore?

Went on a walk Monday at 11:00 am and there were young people everywhere.

Working age people.

Between all these 'creator' types, remote work, quiet quitting, etc...

I honestly wonder how many people even work anymore.

Is your city like that?
------
>your last saved photo is your intelligence level
------
your last saved photo is your intelligence level  twitter.com/Soul0Engineer/…
------
On a mission to find the most talented software engineer

Tag the first account you can think of that's more proficient in software engineering than you. If you get tagged, do the same underneath.
------
I'm putting together a team to build Mentat. I need 10x engineers to push the frontier of possibility w/ LLMs. If that's you, dm!

- work w/ small crack team on ambitious project
- open source: tweet about what you build
- apply research to make something real
- good pay + equity twitter.com/bio_bootloader…
------
Automate your day job so you can work on your side projects
------
prediction: if we do discover an RTSC the turnaround to industrial use will be head spinningly fast. there’s too many startup minded well funded engineers across the world all coordinating discoveries in real time with arxiv paper drops
------
breakthrough
------
this is how it feels
to overcome a challenge
and do something that you had no idea how to do before
------
yudd is based
------
my mom wrote a transformer from scratch
i blew her mind by pasting in her logs to code interpreter after asking for a chart
------
you might not like it, but mindlessly grinding leetcode actually does make you a good programmer
------
Legitimately better than Google. When I search google for my often very specific question, I get at best adjascent answers (often ads disguised as answers), and don't really get too close to the answer I seek
------
If measured against the resources lay people actually use, I predict LLMs would already be an improvement over the status quo.

And the important part is that they will be even better tomorrow, and better still the day after that.

Ignoring the value here is peak midwit.
------
good morning
------
getting cheap plastic crap advertisements is actually great now that I have a 3D printer. So many design ideas
------
One way to think about this: with really good models your vocab (# of symbols) becomes much larger. The model understands the symbol "face", thus what was once hundreds of thousands of symbols (pixels) is now just one (This is an oversimplification obviously).
------
it is redundant data, depending on the context
there is a new dimension in information theory that isn't explored
and that's how entropy decreases the more integrated knowledge (literally, intelligence) the observing agent has
------
I wonder if we're severely underestimating the effect that GPT4 godlike AI models are having & will have on the rate of breakthroughs

the top percentiles must be getting really goosed
------
Every skill will converge to the ability to clearly express your ideas and the drive to execute on them
------
http://www1.ece.neu.edu/~naderi/Claude%20Shannon.html…
------
The government is being heavily pressured to ban open source. How can we prevent this? By campaigning in favor of open source and electing officials who understand the fundamental importance of it.
------
joji’ing my way through a situationship i should be filthy franking
------
many cases
------
SOFTWARE
------
HARDWARE
------
GODWARE
------
No.
The research arm of Bell Labs was never about moonshots.
It was about hiring the best scientists into small departments (typically 5 to 15 people) and giving them resources and a *lot* of freedom to work on what *they* deemed most promising.
That's how you get breakthroughs.
------
For an unknown limited time (probably until some crashing bug), I am hosting Stable Beluga 2 70B (a tune of Llama 2 70B) which is a little more "guide-able" than Llama 2 via prompting. 

It's a streaming-chat Gradio app hosted on 
@LambdaAPI
 cloud.

http://209.20.159.223:7860/
------
Going on a long drive. Put on the latest 
@Plinz
 podcast. Life is good
------
I have felt a great disturbance in the force, particularly in Canada
------
this is what post scarcity looks like
------
Does anyone work anymore?

Went on a walk Monday at 11:00 am and there were young people everywhere.

Working age people.

Between all these 'creator' types, remote work, quiet quitting, etc...

I honestly wonder how many people even work anymore.

Is your city like that?
------
>your last saved photo is your intelligence level
------
your last saved photo is your intelligence level  twitter.com/Soul0Engineer/…
------
On a mission to find the most talented software engineer

Tag the first account you can think of that's more proficient in software engineering than you. If you get tagged, do the same underneath.
------
I'm putting together a team to build Mentat. I need 10x engineers to push the frontier of possibility w/ LLMs. If that's you, dm!

- work w/ small crack team on ambitious project
- open source: tweet about what you build
- apply research to make something real
- good pay + equity twitter.com/bio_bootloader…
------
Automate your day job so you can work on your side projects
------
prediction: if we do discover an RTSC the turnaround to industrial use will be head spinningly fast. there’s too many startup minded well funded engineers across the world all coordinating discoveries in real time with arxiv paper drops
------
breakthrough
------
this is how it feels
to overcome a challenge
and do something that you had no idea how to do before
------
yudd is based
------
what if we’re just approaching the limits of our organizational capacity as a species
------
okay, imagine this
an embodied artificial intelligence that sits on your shoulders, catches mistakes, and helps keep you on task
a drug free aid for ADHD symptoms
(cc 
@NickADobos
, he is building this, basically)
------
inb4 i get quote tweeted by stacy making fun of me saying "they are trying to make mommy AI on tech bro twitter "

yes, we are
------
neural networks want to work
it is extremely likely that there is a bug somewhere in your training script, in your dataset, or a mismatch in your inference
you need to be look at every inference as it flies by
is it what you expect, from training?
yes, I just found a nasty bug
------
this is why i'm extremely bearish on libraries like transformers
they just hide too much
trading off simplicity for ease is a huge mistake when it comes to things that are so eager to work! I guarantee you there are really bad bugs lurking in the depths
------
noahopinion is very good substack, go subscribe
------
One useful tip for Custom Instructions is to specify your workspace environment and the tools you use to write or build code.

Below is a simple example:
------
HOLY shit its me
------
Let's get even stronger
------
i need to get good
------
just found out nelson nelhage is doing interpretability research of LLMs at anthropic
------
thinking about what 
@browserdotsys
 said once
people who do great things tend to pop up again and again, in super disparate places

the example was ggreganov coming up when he was trying to figure out how to get doom running on his cpu cooler

i need to git gud
------
One way to think about this: with really good models your vocab (# of symbols) becomes much larger. The model understands the symbol "face", thus what was once hundreds of thousands of symbols (pixels) is now just one (This is an oversimplification obviously).
------
it is redundant data, depending on the context
there is a new dimension in information theory that isn't explored
and that's how entropy decreases the more integrated knowledge (literally, intelligence) the observing agent has
------
I wonder if we're severely underestimating the effect that GPT4 godlike AI models are having & will have on the rate of breakthroughs

the top percentiles must be getting really goosed
------
Every skill will converge to the ability to clearly express your ideas and the drive to execute on them
------
http://www1.ece.neu.edu/~naderi/Claude%20Shannon.html…
------
The government is being heavily pressured to ban open source. How can we prevent this? By campaigning in favor of open source and electing officials who understand the fundamental importance of it.
------
joji’ing my way through a situationship i should be filthy franking
------
many cases
------
SOFTWARE
------
HARDWARE
------
GODWARE
------
No.
The research arm of Bell Labs was never about moonshots.
It was about hiring the best scientists into small departments (typically 5 to 15 people) and giving them resources and a *lot* of freedom to work on what *they* deemed most promising.
That's how you get breakthroughs.
------
For an unknown limited time (probably until some crashing bug), I am hosting Stable Beluga 2 70B (a tune of Llama 2 70B) which is a little more "guide-able" than Llama 2 via prompting. 

It's a streaming-chat Gradio app hosted on 
@LambdaAPI
 cloud.

http://209.20.159.223:7860/
------
Going on a long drive. Put on the latest 
@Plinz
 podcast. Life is good
------
I have felt a great disturbance in the force, particularly in Canada
------
this is what post scarcity looks like
------
Does anyone work anymore?

Went on a walk Monday at 11:00 am and there were young people everywhere.

Working age people.

Between all these 'creator' types, remote work, quiet quitting, etc...

I honestly wonder how many people even work anymore.

Is your city like that?
------
>your last saved photo is your intelligence level
------
your last saved photo is your intelligence level  twitter.com/Soul0Engineer/…
------
On a mission to find the most talented software engineer

Tag the first account you can think of that's more proficient in software engineering than you. If you get tagged, do the same underneath.
------
I'm putting together a team to build Mentat. I need 10x engineers to push the frontier of possibility w/ LLMs. If that's you, dm!

- work w/ small crack team on ambitious project
- open source: tweet about what you build
- apply research to make something real
- good pay + equity twitter.com/bio_bootloader…
------
Automate your day job so you can work on your side projects
------
prediction: if we do discover an RTSC the turnaround to industrial use will be head spinningly fast. there’s too many startup minded well funded engineers across the world all coordinating discoveries in real time with arxiv paper drops
------
breakthrough
------
this is how it feels
to overcome a challenge
and do something that you had no idea how to do before
------
yudd is based
------
what if we’re just approaching the limits of our organizational capacity as a species
------
okay, imagine this
an embodied artificial intelligence that sits on your shoulders, catches mistakes, and helps keep you on task
a drug free aid for ADHD symptoms
(cc 
@NickADobos
, he is building this, basically)
------
inb4 i get quote tweeted by stacy making fun of me saying "they are trying to make mommy AI on tech bro twitter "

yes, we are
------
neural networks want to work
it is extremely likely that there is a bug somewhere in your training script, in your dataset, or a mismatch in your inference
you need to be look at every inference as it flies by
is it what you expect, from training?
yes, I just found a nasty bug
------
this is why i'm extremely bearish on libraries like transformers
they just hide too much
trading off simplicity for ease is a huge mistake when it comes to things that are so eager to work! I guarantee you there are really bad bugs lurking in the depths
------
noahopinion is very good substack, go subscribe
------
One useful tip for Custom Instructions is to specify your workspace environment and the tools you use to write or build code.

Below is a simple example:
------
HOLY shit its me
------
Let's get even stronger
------
i need to get good
------
just found out nelson nelhage is doing interpretability research of LLMs at anthropic
------
thinking about what 
@browserdotsys
 said once
people who do great things tend to pop up again and again, in super disparate places

the example was ggreganov coming up when he was trying to figure out how to get doom running on his cpu cooler

i need to git gud
------
how am I *this* consistently ahead of the curve?
------
Mark Zuck of $META is apparently eating 4,000 calories a day, including McDonald's Quarter Pounders, apple pies, and McFlurrys, per BI.
------
bullish for me
------
Three philosophy majors just came to my front door. We had a long discussion about philosophy, and why it might be useful to many. Using their superior word power, I am now aware of my ignorance. I was simply too busy with other classes. I am truly sorry for any misimplications.
------
if I could do what I wanted, really, it would be one of those CS / Philosophy guys that trapped at the university all day
------
martin shkrelli is goated
------
every time someone tells me about a useful philosophy class they've had in college, if you squint really hard you realize it's actually a math class in disguise
------
this is a great tweet because everything is math in disguise
------
they dragging my ass on spent my entire life doing leisurely academia aka philosophy twitter
------
many cases
------
SOFTWARE
------
HARDWARE
------
GODWARE
------
No.
The research arm of Bell Labs was never about moonshots.
It was about hiring the best scientists into small departments (typically 5 to 15 people) and giving them resources and a *lot* of freedom to work on what *they* deemed most promising.
That's how you get breakthroughs.
------
For an unknown limited time (probably until some crashing bug), I am hosting Stable Beluga 2 70B (a tune of Llama 2 70B) which is a little more "guide-able" than Llama 2 via prompting. 

It's a streaming-chat Gradio app hosted on 
@LambdaAPI
 cloud.

http://209.20.159.223:7860/
------
Going on a long drive. Put on the latest 
@Plinz
 podcast. Life is good
------
I have felt a great disturbance in the force, particularly in Canada
------
this is what post scarcity looks like
------
Does anyone work anymore?

Went on a walk Monday at 11:00 am and there were young people everywhere.

Working age people.

Between all these 'creator' types, remote work, quiet quitting, etc...

I honestly wonder how many people even work anymore.

Is your city like that?
------
>your last saved photo is your intelligence level
------
your last saved photo is your intelligence level  twitter.com/Soul0Engineer/…
------
On a mission to find the most talented software engineer

Tag the first account you can think of that's more proficient in software engineering than you. If you get tagged, do the same underneath.
------
I'm putting together a team to build Mentat. I need 10x engineers to push the frontier of possibility w/ LLMs. If that's you, dm!

- work w/ small crack team on ambitious project
- open source: tweet about what you build
- apply research to make something real
- good pay + equity twitter.com/bio_bootloader…
------
Automate your day job so you can work on your side projects
------
prediction: if we do discover an RTSC the turnaround to industrial use will be head spinningly fast. there’s too many startup minded well funded engineers across the world all coordinating discoveries in real time with arxiv paper drops
------
breakthrough
------
this is how it feels
to overcome a challenge
and do something that you had no idea how to do before
------
yudd is based
------
what if we’re just approaching the limits of our organizational capacity as a species
------
okay, imagine this
an embodied artificial intelligence that sits on your shoulders, catches mistakes, and helps keep you on task
a drug free aid for ADHD symptoms
(cc 
@NickADobos
, he is building this, basically)
------
inb4 i get quote tweeted by stacy making fun of me saying "they are trying to make mommy AI on tech bro twitter "

yes, we are
------
neural networks want to work
it is extremely likely that there is a bug somewhere in your training script, in your dataset, or a mismatch in your inference
you need to be look at every inference as it flies by
is it what you expect, from training?
yes, I just found a nasty bug
------
this is why i'm extremely bearish on libraries like transformers
they just hide too much
trading off simplicity for ease is a huge mistake when it comes to things that are so eager to work! I guarantee you there are really bad bugs lurking in the depths
------
noahopinion is very good substack, go subscribe
------
One useful tip for Custom Instructions is to specify your workspace environment and the tools you use to write or build code.

Below is a simple example:
------
HOLY shit its me
------
Let's get even stronger
------
i need to get good
------
just found out nelson nelhage is doing interpretability research of LLMs at anthropic
------
thinking about what 
@browserdotsys
 said once
people who do great things tend to pop up again and again, in super disparate places

the example was ggreganov coming up when he was trying to figure out how to get doom running on his cpu cooler

i need to git gud
------
how am I *this* consistently ahead of the curve?
------
Mark Zuck of $META is apparently eating 4,000 calories a day, including McDonald's Quarter Pounders, apple pies, and McFlurrys, per BI.
------
bullish for me
------
Three philosophy majors just came to my front door. We had a long discussion about philosophy, and why it might be useful to many. Using their superior word power, I am now aware of my ignorance. I was simply too busy with other classes. I am truly sorry for any misimplications.
------
if I could do what I wanted, really, it would be one of those CS / Philosophy guys that trapped at the university all day
------
martin shkrelli is goated
------
every time someone tells me about a useful philosophy class they've had in college, if you squint really hard you realize it's actually a math class in disguise
------
this is a great tweet because everything is math in disguise
------
they dragging my ass on spent my entire life doing leisurely academia aka philosophy twitter
------
"okay but how are u gonna build wires out of ceramic"

wires????
------
amusingly my pipeline is starting to do a better job than yacine at 1 am eval
------
which way, western man?
------
this is what post scarcity looks like
------
Does anyone work anymore?

Went on a walk Monday at 11:00 am and there were young people everywhere.

Working age people.

Between all these 'creator' types, remote work, quiet quitting, etc...

I honestly wonder how many people even work anymore.

Is your city like that?
------
>your last saved photo is your intelligence level
------
your last saved photo is your intelligence level  twitter.com/Soul0Engineer/…
------
On a mission to find the most talented software engineer

Tag the first account you can think of that's more proficient in software engineering than you. If you get tagged, do the same underneath.
------
I'm putting together a team to build Mentat. I need 10x engineers to push the frontier of possibility w/ LLMs. If that's you, dm!

- work w/ small crack team on ambitious project
- open source: tweet about what you build
- apply research to make something real
- good pay + equity twitter.com/bio_bootloader…
------
Automate your day job so you can work on your side projects
------
prediction: if we do discover an RTSC the turnaround to industrial use will be head spinningly fast. there’s too many startup minded well funded engineers across the world all coordinating discoveries in real time with arxiv paper drops
------
breakthrough
------
this is how it feels
to overcome a challenge
and do something that you had no idea how to do before
------
yudd is based
------
what if we’re just approaching the limits of our organizational capacity as a species
------
okay, imagine this
an embodied artificial intelligence that sits on your shoulders, catches mistakes, and helps keep you on task
a drug free aid for ADHD symptoms
(cc 
@NickADobos
, he is building this, basically)
------
inb4 i get quote tweeted by stacy making fun of me saying "they are trying to make mommy AI on tech bro twitter "

yes, we are
------
neural networks want to work
it is extremely likely that there is a bug somewhere in your training script, in your dataset, or a mismatch in your inference
you need to be look at every inference as it flies by
is it what you expect, from training?
yes, I just found a nasty bug
------
this is why i'm extremely bearish on libraries like transformers
they just hide too much
trading off simplicity for ease is a huge mistake when it comes to things that are so eager to work! I guarantee you there are really bad bugs lurking in the depths
------
noahopinion is very good substack, go subscribe
------
One useful tip for Custom Instructions is to specify your workspace environment and the tools you use to write or build code.

Below is a simple example:
------
HOLY shit its me
------
Let's get even stronger
------
i need to get good
------
just found out nelson nelhage is doing interpretability research of LLMs at anthropic
------
thinking about what 
@browserdotsys
 said once
people who do great things tend to pop up again and again, in super disparate places

the example was ggreganov coming up when he was trying to figure out how to get doom running on his cpu cooler

i need to git gud
------
how am I *this* consistently ahead of the curve?
------
Mark Zuck of $META is apparently eating 4,000 calories a day, including McDonald's Quarter Pounders, apple pies, and McFlurrys, per BI.
------
bullish for me
------
Three philosophy majors just came to my front door. We had a long discussion about philosophy, and why it might be useful to many. Using their superior word power, I am now aware of my ignorance. I was simply too busy with other classes. I am truly sorry for any misimplications.
------
if I could do what I wanted, really, it would be one of those CS / Philosophy guys that trapped at the university all day
------
martin shkrelli is goated
------
every time someone tells me about a useful philosophy class they've had in college, if you squint really hard you realize it's actually a math class in disguise
------
this is a great tweet because everything is math in disguise
------
they dragging my ass on spent my entire life doing leisurely academia aka philosophy twitter
------
"okay but how are u gonna build wires out of ceramic"

wires????
------
amusingly my pipeline is starting to do a better job than yacine at 1 am eval
------
which way, western man?
------
re my bio
a lot of people actually do ask me for help in DMs. 80% of the answers are "lift heavy weights" or "build something useful", and sometimes both
------
also I do not mean to talk down people asking for help
I very much welcome people asking for help, and I am very happy to help people realize that it's actually very simple
Sometimes you just need someone to listen
------
Do you believe yet?
------
i am positioned with the belief that the 1 trillion tech valuation of the 2020s will correspond to the 10 trillion dollar tech valuation of the 2030s. i think things are gonna get weird with software/AI.
------
just got a golf ball ad. Is this what it feels like to climb the socio economic ladder?
------
breakthrough
------
this is how it feels
to overcome a challenge
and do something that you had no idea how to do before
------
yudd is based
------
what if we’re just approaching the limits of our organizational capacity as a species
------
okay, imagine this
an embodied artificial intelligence that sits on your shoulders, catches mistakes, and helps keep you on task
a drug free aid for ADHD symptoms
(cc 
@NickADobos
, he is building this, basically)
------
inb4 i get quote tweeted by stacy making fun of me saying "they are trying to make mommy AI on tech bro twitter "

yes, we are
------
neural networks want to work
it is extremely likely that there is a bug somewhere in your training script, in your dataset, or a mismatch in your inference
you need to be look at every inference as it flies by
is it what you expect, from training?
yes, I just found a nasty bug
------
this is why i'm extremely bearish on libraries like transformers
they just hide too much
trading off simplicity for ease is a huge mistake when it comes to things that are so eager to work! I guarantee you there are really bad bugs lurking in the depths
------
noahopinion is very good substack, go subscribe
------
One useful tip for Custom Instructions is to specify your workspace environment and the tools you use to write or build code.

Below is a simple example:
------
HOLY shit its me
------
Let's get even stronger
------
i need to get good
------
just found out nelson nelhage is doing interpretability research of LLMs at anthropic
------
thinking about what 
@browserdotsys
 said once
people who do great things tend to pop up again and again, in super disparate places

the example was ggreganov coming up when he was trying to figure out how to get doom running on his cpu cooler

i need to git gud
------
how am I *this* consistently ahead of the curve?
------
Mark Zuck of $META is apparently eating 4,000 calories a day, including McDonald's Quarter Pounders, apple pies, and McFlurrys, per BI.
------
bullish for me
------
Three philosophy majors just came to my front door. We had a long discussion about philosophy, and why it might be useful to many. Using their superior word power, I am now aware of my ignorance. I was simply too busy with other classes. I am truly sorry for any misimplications.
------
if I could do what I wanted, really, it would be one of those CS / Philosophy guys that trapped at the university all day
------
martin shkrelli is goated
------
every time someone tells me about a useful philosophy class they've had in college, if you squint really hard you realize it's actually a math class in disguise
------
this is a great tweet because everything is math in disguise
------
they dragging my ass on spent my entire life doing leisurely academia aka philosophy twitter
------
"okay but how are u gonna build wires out of ceramic"

wires????
------
amusingly my pipeline is starting to do a better job than yacine at 1 am eval
------
which way, western man?
------
re my bio
a lot of people actually do ask me for help in DMs. 80% of the answers are "lift heavy weights" or "build something useful", and sometimes both
------
also I do not mean to talk down people asking for help
I very much welcome people asking for help, and I am very happy to help people realize that it's actually very simple
Sometimes you just need someone to listen
------
Do you believe yet?
------
i am positioned with the belief that the 1 trillion tech valuation of the 2020s will correspond to the 10 trillion dollar tech valuation of the 2030s. i think things are gonna get weird with software/AI.
------
just got a golf ball ad. Is this what it feels like to climb the socio economic ladder?
------
If you haven't tried 
@sbinslashessend
's custom instructions you really should. I re-ran several older questions I'd asked and nearly all improved a good deal

"Be terse and concise without being rude. It's okay to be opinionated if there's solid justification. Call out… Show more
------
vague statement, engineered for defensibility, which still approximates the general direction of the optimistic current take, but also is completely unrelated to it at its core
------
how i look at my friends when they start displaying NGMI behavior
------
woah they updated the UI
------
One useful tip for Custom Instructions is to specify your workspace environment and the tools you use to write or build code.

Below is a simple example:
------
HOLY shit its me
------
Let's get even stronger
------
i need to get good
------
just found out nelson nelhage is doing interpretability research of LLMs at anthropic
------
thinking about what 
@browserdotsys
 said once
people who do great things tend to pop up again and again, in super disparate places

the example was ggreganov coming up when he was trying to figure out how to get doom running on his cpu cooler

i need to git gud
------
how am I *this* consistently ahead of the curve?
------
Mark Zuck of $META is apparently eating 4,000 calories a day, including McDonald's Quarter Pounders, apple pies, and McFlurrys, per BI.
------
bullish for me
------
Three philosophy majors just came to my front door. We had a long discussion about philosophy, and why it might be useful to many. Using their superior word power, I am now aware of my ignorance. I was simply too busy with other classes. I am truly sorry for any misimplications.
------
if I could do what I wanted, really, it would be one of those CS / Philosophy guys that trapped at the university all day
------
martin shkrelli is goated
------
every time someone tells me about a useful philosophy class they've had in college, if you squint really hard you realize it's actually a math class in disguise
------
this is a great tweet because everything is math in disguise
------
they dragging my ass on spent my entire life doing leisurely academia aka philosophy twitter
------
"okay but how are u gonna build wires out of ceramic"

wires????
------
amusingly my pipeline is starting to do a better job than yacine at 1 am eval
------
which way, western man?
------
re my bio
a lot of people actually do ask me for help in DMs. 80% of the answers are "lift heavy weights" or "build something useful", and sometimes both
------
also I do not mean to talk down people asking for help
I very much welcome people asking for help, and I am very happy to help people realize that it's actually very simple
Sometimes you just need someone to listen
------
Do you believe yet?
------
i am positioned with the belief that the 1 trillion tech valuation of the 2020s will correspond to the 10 trillion dollar tech valuation of the 2030s. i think things are gonna get weird with software/AI.
------
just got a golf ball ad. Is this what it feels like to climb the socio economic ladder?
------
If you haven't tried 
@sbinslashessend
's custom instructions you really should. I re-ran several older questions I'd asked and nearly all improved a good deal

"Be terse and concise without being rude. It's okay to be opinionated if there's solid justification. Call out… Show more
------
vague statement, engineered for defensibility, which still approximates the general direction of the optimistic current take, but also is completely unrelated to it at its core
------
how i look at my friends when they start displaying NGMI behavior
------
woah they updated the UI
------
THE SYMBOL OF E/ACC
------
i want BEZOS to missile launch 3D printers manufactured in china, which download and install firmware written in texas MID FLIGHT, and then parachute down right at my doorstep using a low tech drone that was 3D PRINTED AND ASSEMBLED MID FLIGHT
------
Serious question - wouldn’t amazon package missles be safer than drones? A drone is so unstable, any failure and can kill someone - we have a lot of good tech to get missles exactly where we want them to be
------
bullish for me
------
Three philosophy majors just came to my front door. We had a long discussion about philosophy, and why it might be useful to many. Using their superior word power, I am now aware of my ignorance. I was simply too busy with other classes. I am truly sorry for any misimplications.
------
if I could do what I wanted, really, it would be one of those CS / Philosophy guys that trapped at the university all day
------
martin shkrelli is goated
------
every time someone tells me about a useful philosophy class they've had in college, if you squint really hard you realize it's actually a math class in disguise
------
this is a great tweet because everything is math in disguise
------
they dragging my ass on spent my entire life doing leisurely academia aka philosophy twitter
------
"okay but how are u gonna build wires out of ceramic"

wires????
------
amusingly my pipeline is starting to do a better job than yacine at 1 am eval
------
which way, western man?
------
re my bio
a lot of people actually do ask me for help in DMs. 80% of the answers are "lift heavy weights" or "build something useful", and sometimes both
------
also I do not mean to talk down people asking for help
I very much welcome people asking for help, and I am very happy to help people realize that it's actually very simple
Sometimes you just need someone to listen
------
Do you believe yet?
------
i am positioned with the belief that the 1 trillion tech valuation of the 2020s will correspond to the 10 trillion dollar tech valuation of the 2030s. i think things are gonna get weird with software/AI.
------
just got a golf ball ad. Is this what it feels like to climb the socio economic ladder?
------
If you haven't tried 
@sbinslashessend
's custom instructions you really should. I re-ran several older questions I'd asked and nearly all improved a good deal

"Be terse and concise without being rude. It's okay to be opinionated if there's solid justification. Call out… Show more
------
vague statement, engineered for defensibility, which still approximates the general direction of the optimistic current take, but also is completely unrelated to it at its core
------
how i look at my friends when they start displaying NGMI behavior
------
woah they updated the UI
------
THE SYMBOL OF E/ACC
------
i want BEZOS to missile launch 3D printers manufactured in china, which download and install firmware written in texas MID FLIGHT, and then parachute down right at my doorstep using a low tech drone that was 3D PRINTED AND ASSEMBLED MID FLIGHT
------
Serious question - wouldn’t amazon package missles be safer than drones? A drone is so unstable, any failure and can kill someone - we have a lot of good tech to get missles exactly where we want them to be
------
most people think we know why LLMs work
------
they don't realize how early we are
------
>Although evocative, we recognise that the use of the name ‘Hydra’ is not completely mythologically accurate: sometimes only one head grows in importance

lol
------
this is the greatest time to be alive in the last 13.7 billion years
------
going to say this again because it warrants resaying

- you can get a lot further thank you think by LoRAing for a task specific model with 1000 samples on a small large model
- LoRA (adapters) are better than full tunes for production because humanity is currently VRAM/bus bound
------
rfc1925
------
beta is god's alpha
------
this is going to be the most chaotic decade in human history
------
good morni-
------
"okay but how are u gonna build wires out of ceramic"

wires????
------
amusingly my pipeline is starting to do a better job than yacine at 1 am eval
------
which way, western man?
------
re my bio
a lot of people actually do ask me for help in DMs. 80% of the answers are "lift heavy weights" or "build something useful", and sometimes both
------
also I do not mean to talk down people asking for help
I very much welcome people asking for help, and I am very happy to help people realize that it's actually very simple
Sometimes you just need someone to listen
------
Do you believe yet?
------
i am positioned with the belief that the 1 trillion tech valuation of the 2020s will correspond to the 10 trillion dollar tech valuation of the 2030s. i think things are gonna get weird with software/AI.
------
just got a golf ball ad. Is this what it feels like to climb the socio economic ladder?
------
If you haven't tried 
@sbinslashessend
's custom instructions you really should. I re-ran several older questions I'd asked and nearly all improved a good deal

"Be terse and concise without being rude. It's okay to be opinionated if there's solid justification. Call out… Show more
------
vague statement, engineered for defensibility, which still approximates the general direction of the optimistic current take, but also is completely unrelated to it at its core
------
how i look at my friends when they start displaying NGMI behavior
------
woah they updated the UI
------
THE SYMBOL OF E/ACC
------
i want BEZOS to missile launch 3D printers manufactured in china, which download and install firmware written in texas MID FLIGHT, and then parachute down right at my doorstep using a low tech drone that was 3D PRINTED AND ASSEMBLED MID FLIGHT
------
Serious question - wouldn’t amazon package missles be safer than drones? A drone is so unstable, any failure and can kill someone - we have a lot of good tech to get missles exactly where we want them to be
------
most people think we know why LLMs work
------
they don't realize how early we are
------
>Although evocative, we recognise that the use of the name ‘Hydra’ is not completely mythologically accurate: sometimes only one head grows in importance

lol
------
this is the greatest time to be alive in the last 13.7 billion years
------
going to say this again because it warrants resaying

- you can get a lot further thank you think by LoRAing for a task specific model with 1000 samples on a small large model
- LoRA (adapters) are better than full tunes for production because humanity is currently VRAM/bus bound
------
rfc1925
------
beta is god's alpha
------
this is going to be the most chaotic decade in human history
------
good morni-
------
canada is the staging server for america
------
my friend is visiting from berlin. apparently they really are eating the bugs
------
what is up with all the jackedphobia
why do people hate on our homies that have left humanity behind?
i thought we were supposed to accept each other
------
lmao not even friendly fire is allowed? zietgeist has gone too far bros https://twitter.com/ShakthiJ/status/1685760230448107520…
------
We’re thankful for the incredible response to Llama 2 and early uses + development already emerging in the community. With 150K+ download requests in just the first week, we can't wait to see how you build with these models.

More in this note 
------
instead of trying to get what poorly abstracted OOP dependencies (tr*nsformers) to do exactly what I want to do, I should just rewrite them to get closer to the metal

The problem is then, walking through the code
This means I should just hack it to prove, and punt til later
------
The very very last thing you should ever ever do is use someone else's abstractions. If you're implementing someone else's interface into a stateful object you're literally never going to make it
------
As far as I can tell, autodebloatifying transformers should be possible, at the very least, the forward pass should be easy to debloatify
------
This isn't autism, you have just normalized incompetence as a society and it is biting you in the ass
------
autism is bullish
------
re my bio
a lot of people actually do ask me for help in DMs. 80% of the answers are "lift heavy weights" or "build something useful", and sometimes both
------
also I do not mean to talk down people asking for help
I very much welcome people asking for help, and I am very happy to help people realize that it's actually very simple
Sometimes you just need someone to listen
------
Do you believe yet?
------
i am positioned with the belief that the 1 trillion tech valuation of the 2020s will correspond to the 10 trillion dollar tech valuation of the 2030s. i think things are gonna get weird with software/AI.
------
just got a golf ball ad. Is this what it feels like to climb the socio economic ladder?
------
If you haven't tried 
@sbinslashessend
's custom instructions you really should. I re-ran several older questions I'd asked and nearly all improved a good deal

"Be terse and concise without being rude. It's okay to be opinionated if there's solid justification. Call out… Show more
------
vague statement, engineered for defensibility, which still approximates the general direction of the optimistic current take, but also is completely unrelated to it at its core
------
how i look at my friends when they start displaying NGMI behavior
------
woah they updated the UI
------
THE SYMBOL OF E/ACC
------
i want BEZOS to missile launch 3D printers manufactured in china, which download and install firmware written in texas MID FLIGHT, and then parachute down right at my doorstep using a low tech drone that was 3D PRINTED AND ASSEMBLED MID FLIGHT
------
Serious question - wouldn’t amazon package missles be safer than drones? A drone is so unstable, any failure and can kill someone - we have a lot of good tech to get missles exactly where we want them to be
------
most people think we know why LLMs work
------
they don't realize how early we are
------
>Although evocative, we recognise that the use of the name ‘Hydra’ is not completely mythologically accurate: sometimes only one head grows in importance

lol
------
this is the greatest time to be alive in the last 13.7 billion years
------
going to say this again because it warrants resaying

- you can get a lot further thank you think by LoRAing for a task specific model with 1000 samples on a small large model
- LoRA (adapters) are better than full tunes for production because humanity is currently VRAM/bus bound
------
rfc1925
------
beta is god's alpha
------
this is going to be the most chaotic decade in human history
------
good morni-
------
canada is the staging server for america
------
my friend is visiting from berlin. apparently they really are eating the bugs
------
what is up with all the jackedphobia
why do people hate on our homies that have left humanity behind?
i thought we were supposed to accept each other
------
lmao not even friendly fire is allowed? zietgeist has gone too far bros https://twitter.com/ShakthiJ/status/1685760230448107520…
------
We’re thankful for the incredible response to Llama 2 and early uses + development already emerging in the community. With 150K+ download requests in just the first week, we can't wait to see how you build with these models.

More in this note 
------
instead of trying to get what poorly abstracted OOP dependencies (tr*nsformers) to do exactly what I want to do, I should just rewrite them to get closer to the metal

The problem is then, walking through the code
This means I should just hack it to prove, and punt til later
------
The very very last thing you should ever ever do is use someone else's abstractions. If you're implementing someone else's interface into a stateful object you're literally never going to make it
------
As far as I can tell, autodebloatifying transformers should be possible, at the very least, the forward pass should be easy to debloatify
------
This isn't autism, you have just normalized incompetence as a society and it is biting you in the ass
------
autism is bullish
------
equip anime profile picture
stats: +10 credibility
------
men
------
I fucking hate this liberal bullshit
------
everyone is wrong and I am right. It is that simple
------
The thesis that one big centralized model can beat millions of local expert models is the same assumption behind why some think Communism (top-down control) can work better than Capitalism (bottom-up emergent self-adaptation).

Fundamentally wrong, as the market will show.
------
Now, that's the bloody thing with nuclear bombs. They're no joke
------
If you haven't tried 
@sbinslashessend
's custom instructions you really should. I re-ran several older questions I'd asked and nearly all improved a good deal

"Be terse and concise without being rude. It's okay to be opinionated if there's solid justification. Call out… Show more
------
vague statement, engineered for defensibility, which still approximates the general direction of the optimistic current take, but also is completely unrelated to it at its core
------
how i look at my friends when they start displaying NGMI behavior
------
woah they updated the UI
------
THE SYMBOL OF E/ACC
------
i want BEZOS to missile launch 3D printers manufactured in china, which download and install firmware written in texas MID FLIGHT, and then parachute down right at my doorstep using a low tech drone that was 3D PRINTED AND ASSEMBLED MID FLIGHT
------
Serious question - wouldn’t amazon package missles be safer than drones? A drone is so unstable, any failure and can kill someone - we have a lot of good tech to get missles exactly where we want them to be
------
most people think we know why LLMs work
------
they don't realize how early we are
------
>Although evocative, we recognise that the use of the name ‘Hydra’ is not completely mythologically accurate: sometimes only one head grows in importance

lol
------
this is the greatest time to be alive in the last 13.7 billion years
------
going to say this again because it warrants resaying

- you can get a lot further thank you think by LoRAing for a task specific model with 1000 samples on a small large model
- LoRA (adapters) are better than full tunes for production because humanity is currently VRAM/bus bound
------
rfc1925
------
beta is god's alpha
------
this is going to be the most chaotic decade in human history
------
good morni-
------
canada is the staging server for america
------
my friend is visiting from berlin. apparently they really are eating the bugs
------
what is up with all the jackedphobia
why do people hate on our homies that have left humanity behind?
i thought we were supposed to accept each other
------
lmao not even friendly fire is allowed? zietgeist has gone too far bros https://twitter.com/ShakthiJ/status/1685760230448107520…
------
We’re thankful for the incredible response to Llama 2 and early uses + development already emerging in the community. With 150K+ download requests in just the first week, we can't wait to see how you build with these models.

More in this note 
------
instead of trying to get what poorly abstracted OOP dependencies (tr*nsformers) to do exactly what I want to do, I should just rewrite them to get closer to the metal

The problem is then, walking through the code
This means I should just hack it to prove, and punt til later
------
The very very last thing you should ever ever do is use someone else's abstractions. If you're implementing someone else's interface into a stateful object you're literally never going to make it
------
As far as I can tell, autodebloatifying transformers should be possible, at the very least, the forward pass should be easy to debloatify
------
This isn't autism, you have just normalized incompetence as a society and it is biting you in the ass
------
autism is bullish
------
equip anime profile picture
stats: +10 credibility
------
men
------
I fucking hate this liberal bullshit
------
everyone is wrong and I am right. It is that simple
------
The thesis that one big centralized model can beat millions of local expert models is the same assumption behind why some think Communism (top-down control) can work better than Capitalism (bottom-up emergent self-adaptation).

Fundamentally wrong, as the market will show.
------
Now, that's the bloody thing with nuclear bombs. They're no joke
------
this is really bad
------
technological advance that improves labor productivity by 10x is a much bigger first order consideration than second order correction terms like guilds or trade unions. technological advance wins every time
------
hackernews has the type of pessimistic celery that you'd only expect from someone who still thinks there is alpha in hackernews
------
a man's life should fit into a backpack, and a 32GB drive
------
it's 2023, no one reads resumes
------
the biggest mistake I see ML researchers is think that there is something special going on with humans

we are a layer of hacks, just like our artificial analogues
------
A 500k year/engineer may literally be 10x more likely to run a project without it going off the rails than a 150k/year engineer.
------
If you're a politician, technical worker, salesperson, blue collar worker, academic, the highest return thing you can do for your career, your goals, your life, is still get extremely jacked
------
There are 0 situations where being jacked doesn't make you more successful

Humans simply like people who are jacked/physically fit more
------
The future of technical books will be small, and aesthetic, with little elaboration

They will guide the reader through important concepts, which they can then thoroughly learn using a godlike QA agent like GPT4, when necessary
------
THE SYMBOL OF E/ACC
------
i want BEZOS to missile launch 3D printers manufactured in china, which download and install firmware written in texas MID FLIGHT, and then parachute down right at my doorstep using a low tech drone that was 3D PRINTED AND ASSEMBLED MID FLIGHT
------
Serious question - wouldn’t amazon package missles be safer than drones? A drone is so unstable, any failure and can kill someone - we have a lot of good tech to get missles exactly where we want them to be
------
most people think we know why LLMs work
------
they don't realize how early we are
------
>Although evocative, we recognise that the use of the name ‘Hydra’ is not completely mythologically accurate: sometimes only one head grows in importance

lol
------
this is the greatest time to be alive in the last 13.7 billion years
------
going to say this again because it warrants resaying

- you can get a lot further thank you think by LoRAing for a task specific model with 1000 samples on a small large model
- LoRA (adapters) are better than full tunes for production because humanity is currently VRAM/bus bound
------
rfc1925
------
beta is god's alpha
------
this is going to be the most chaotic decade in human history
------
good morni-
------
canada is the staging server for america
------
my friend is visiting from berlin. apparently they really are eating the bugs
------
what is up with all the jackedphobia
why do people hate on our homies that have left humanity behind?
i thought we were supposed to accept each other
------
lmao not even friendly fire is allowed? zietgeist has gone too far bros https://twitter.com/ShakthiJ/status/1685760230448107520…
------
We’re thankful for the incredible response to Llama 2 and early uses + development already emerging in the community. With 150K+ download requests in just the first week, we can't wait to see how you build with these models.

More in this note 
------
instead of trying to get what poorly abstracted OOP dependencies (tr*nsformers) to do exactly what I want to do, I should just rewrite them to get closer to the metal

The problem is then, walking through the code
This means I should just hack it to prove, and punt til later
------
The very very last thing you should ever ever do is use someone else's abstractions. If you're implementing someone else's interface into a stateful object you're literally never going to make it
------
As far as I can tell, autodebloatifying transformers should be possible, at the very least, the forward pass should be easy to debloatify
------
This isn't autism, you have just normalized incompetence as a society and it is biting you in the ass
------
autism is bullish
------
equip anime profile picture
stats: +10 credibility
------
men
------
I fucking hate this liberal bullshit
------
everyone is wrong and I am right. It is that simple
------
The thesis that one big centralized model can beat millions of local expert models is the same assumption behind why some think Communism (top-down control) can work better than Capitalism (bottom-up emergent self-adaptation).

Fundamentally wrong, as the market will show.
------
Now, that's the bloody thing with nuclear bombs. They're no joke
------
this is really bad
------
technological advance that improves labor productivity by 10x is a much bigger first order consideration than second order correction terms like guilds or trade unions. technological advance wins every time
------
hackernews has the type of pessimistic celery that you'd only expect from someone who still thinks there is alpha in hackernews
------
a man's life should fit into a backpack, and a 32GB drive
------
it's 2023, no one reads resumes
------
the biggest mistake I see ML researchers is think that there is something special going on with humans

we are a layer of hacks, just like our artificial analogues
------
A 500k year/engineer may literally be 10x more likely to run a project without it going off the rails than a 150k/year engineer.
------
If you're a politician, technical worker, salesperson, blue collar worker, academic, the highest return thing you can do for your career, your goals, your life, is still get extremely jacked
------
There are 0 situations where being jacked doesn't make you more successful

Humans simply like people who are jacked/physically fit more
------
The future of technical books will be small, and aesthetic, with little elaboration

They will guide the reader through important concepts, which they can then thoroughly learn using a godlike QA agent like GPT4, when necessary
------
WE WILL NEVER STOP
BUILDING TECHNOLOGY
------
The constant, unwavering forward progression
Has given us, hackers, untold powers
We can generate art at the click of a button
Process documents better than any lawyer
Create tools out of processed plants, faster than any woodworker

enter the age of the nerd
augment yourself
------
My mind is actually boggled at 
@BambulabGlobal
's 3D printer & self correcting ML tech
This thing is so cheap and so fast
It's actually alien technology. Just worked out of the box
This is actually some "I saw this in the movies in 1980s" tech
------
between this and my GPU training models, my room is getting a little hot
the future rocks
------
>Although evocative, we recognise that the use of the name ‘Hydra’ is not completely mythologically accurate: sometimes only one head grows in importance

lol
------
this is the greatest time to be alive in the last 13.7 billion years
------
going to say this again because it warrants resaying

- you can get a lot further thank you think by LoRAing for a task specific model with 1000 samples on a small large model
- LoRA (adapters) are better than full tunes for production because humanity is currently VRAM/bus bound
------
rfc1925
------
beta is god's alpha
------
this is going to be the most chaotic decade in human history
------
good morni-
------
canada is the staging server for america
------
my friend is visiting from berlin. apparently they really are eating the bugs
------
what is up with all the jackedphobia
why do people hate on our homies that have left humanity behind?
i thought we were supposed to accept each other
------
lmao not even friendly fire is allowed? zietgeist has gone too far bros https://twitter.com/ShakthiJ/status/1685760230448107520…
------
We’re thankful for the incredible response to Llama 2 and early uses + development already emerging in the community. With 150K+ download requests in just the first week, we can't wait to see how you build with these models.

More in this note 
------
instead of trying to get what poorly abstracted OOP dependencies (tr*nsformers) to do exactly what I want to do, I should just rewrite them to get closer to the metal

The problem is then, walking through the code
This means I should just hack it to prove, and punt til later
------
The very very last thing you should ever ever do is use someone else's abstractions. If you're implementing someone else's interface into a stateful object you're literally never going to make it
------
As far as I can tell, autodebloatifying transformers should be possible, at the very least, the forward pass should be easy to debloatify
------
This isn't autism, you have just normalized incompetence as a society and it is biting you in the ass
------
autism is bullish
------
equip anime profile picture
stats: +10 credibility
------
men
------
I fucking hate this liberal bullshit
------
everyone is wrong and I am right. It is that simple
------
The thesis that one big centralized model can beat millions of local expert models is the same assumption behind why some think Communism (top-down control) can work better than Capitalism (bottom-up emergent self-adaptation).

Fundamentally wrong, as the market will show.
------
Now, that's the bloody thing with nuclear bombs. They're no joke
------
this is really bad
------
technological advance that improves labor productivity by 10x is a much bigger first order consideration than second order correction terms like guilds or trade unions. technological advance wins every time
------
hackernews has the type of pessimistic celery that you'd only expect from someone who still thinks there is alpha in hackernews
------
a man's life should fit into a backpack, and a 32GB drive
------
it's 2023, no one reads resumes
------
the biggest mistake I see ML researchers is think that there is something special going on with humans

we are a layer of hacks, just like our artificial analogues
------
A 500k year/engineer may literally be 10x more likely to run a project without it going off the rails than a 150k/year engineer.
------
If you're a politician, technical worker, salesperson, blue collar worker, academic, the highest return thing you can do for your career, your goals, your life, is still get extremely jacked
------
There are 0 situations where being jacked doesn't make you more successful

Humans simply like people who are jacked/physically fit more
------
The future of technical books will be small, and aesthetic, with little elaboration

They will guide the reader through important concepts, which they can then thoroughly learn using a godlike QA agent like GPT4, when necessary
------
WE WILL NEVER STOP
BUILDING TECHNOLOGY
------
The constant, unwavering forward progression
Has given us, hackers, untold powers
We can generate art at the click of a button
Process documents better than any lawyer
Create tools out of processed plants, faster than any woodworker

enter the age of the nerd
augment yourself
------
My mind is actually boggled at 
@BambulabGlobal
's 3D printer & self correcting ML tech
This thing is so cheap and so fast
It's actually alien technology. Just worked out of the box
This is actually some "I saw this in the movies in 1980s" tech
------
between this and my GPU training models, my room is getting a little hot
the future rocks
------
this alone justified the entire purchase
------
my friend is visiting from berlin. apparently they really are eating the bugs
------
what is up with all the jackedphobia
why do people hate on our homies that have left humanity behind?
i thought we were supposed to accept each other
------
lmao not even friendly fire is allowed? zietgeist has gone too far bros https://twitter.com/ShakthiJ/status/1685760230448107520…
------
We’re thankful for the incredible response to Llama 2 and early uses + development already emerging in the community. With 150K+ download requests in just the first week, we can't wait to see how you build with these models.

More in this note 
------
instead of trying to get what poorly abstracted OOP dependencies (tr*nsformers) to do exactly what I want to do, I should just rewrite them to get closer to the metal

The problem is then, walking through the code
This means I should just hack it to prove, and punt til later
------
The very very last thing you should ever ever do is use someone else's abstractions. If you're implementing someone else's interface into a stateful object you're literally never going to make it
------
As far as I can tell, autodebloatifying transformers should be possible, at the very least, the forward pass should be easy to debloatify
------
This isn't autism, you have just normalized incompetence as a society and it is biting you in the ass
------
autism is bullish
------
equip anime profile picture
stats: +10 credibility
------
men
------
I fucking hate this liberal bullshit
------
everyone is wrong and I am right. It is that simple
------
The thesis that one big centralized model can beat millions of local expert models is the same assumption behind why some think Communism (top-down control) can work better than Capitalism (bottom-up emergent self-adaptation).

Fundamentally wrong, as the market will show.
------
Now, that's the bloody thing with nuclear bombs. They're no joke
------
this is really bad
------
technological advance that improves labor productivity by 10x is a much bigger first order consideration than second order correction terms like guilds or trade unions. technological advance wins every time
------
hackernews has the type of pessimistic celery that you'd only expect from someone who still thinks there is alpha in hackernews
------
a man's life should fit into a backpack, and a 32GB drive
------
it's 2023, no one reads resumes
------
the biggest mistake I see ML researchers is think that there is something special going on with humans

we are a layer of hacks, just like our artificial analogues
------
A 500k year/engineer may literally be 10x more likely to run a project without it going off the rails than a 150k/year engineer.
------
If you're a politician, technical worker, salesperson, blue collar worker, academic, the highest return thing you can do for your career, your goals, your life, is still get extremely jacked
------
There are 0 situations where being jacked doesn't make you more successful

Humans simply like people who are jacked/physically fit more
------
The future of technical books will be small, and aesthetic, with little elaboration

They will guide the reader through important concepts, which they can then thoroughly learn using a godlike QA agent like GPT4, when necessary
------
WE WILL NEVER STOP
BUILDING TECHNOLOGY
------
The constant, unwavering forward progression
Has given us, hackers, untold powers
We can generate art at the click of a button
Process documents better than any lawyer
Create tools out of processed plants, faster than any woodworker

enter the age of the nerd
augment yourself
------
My mind is actually boggled at 
@BambulabGlobal
's 3D printer & self correcting ML tech
This thing is so cheap and so fast
It's actually alien technology. Just worked out of the box
This is actually some "I saw this in the movies in 1980s" tech
------
between this and my GPU training models, my room is getting a little hot
the future rocks
------
this alone justified the entire purchase
------
That's one thing we agree on.
I suggest a gentle ramp up:
- Surely you are joking Mr Feynman 
- QED
- The Feynman Lectures on Physics
- Quantum Mechanics and Path Integrals (Feynman & Hibbs)
------
People should read Feynman’s books
------
thanks for writing this 
@francoisfleuret
!
------
I was looking for a no frills book. 
Not all the way through, but learning a lot going through it
------
LOL 
@ylecun
 got hacked. Somebody reach out to him!
------
nooooooooooo they got my boy lecunster  ;_; mooooooooooods
------
trying to read, understand, or use the transformers library & logic
------
Actually, I am going to spend 12 hours rewriting everything in Jax. I would suffer less spiritual pain doing so
------
hearing the big guy (marc andreesen) pretty much sum up what is happening was really cathartic
------
you cannot stop technology
------
men
------
I fucking hate this liberal bullshit
------
everyone is wrong and I am right. It is that simple
------
The thesis that one big centralized model can beat millions of local expert models is the same assumption behind why some think Communism (top-down control) can work better than Capitalism (bottom-up emergent self-adaptation).

Fundamentally wrong, as the market will show.
------
Now, that's the bloody thing with nuclear bombs. They're no joke
------
this is really bad
------
technological advance that improves labor productivity by 10x is a much bigger first order consideration than second order correction terms like guilds or trade unions. technological advance wins every time
------
hackernews has the type of pessimistic celery that you'd only expect from someone who still thinks there is alpha in hackernews
------
a man's life should fit into a backpack, and a 32GB drive
------
it's 2023, no one reads resumes
------
the biggest mistake I see ML researchers is think that there is something special going on with humans

we are a layer of hacks, just like our artificial analogues
------
A 500k year/engineer may literally be 10x more likely to run a project without it going off the rails than a 150k/year engineer.
------
If you're a politician, technical worker, salesperson, blue collar worker, academic, the highest return thing you can do for your career, your goals, your life, is still get extremely jacked
------
There are 0 situations where being jacked doesn't make you more successful

Humans simply like people who are jacked/physically fit more
------
The future of technical books will be small, and aesthetic, with little elaboration

They will guide the reader through important concepts, which they can then thoroughly learn using a godlike QA agent like GPT4, when necessary
------
WE WILL NEVER STOP
BUILDING TECHNOLOGY
------
The constant, unwavering forward progression
Has given us, hackers, untold powers
We can generate art at the click of a button
Process documents better than any lawyer
Create tools out of processed plants, faster than any woodworker

enter the age of the nerd
augment yourself
------
My mind is actually boggled at 
@BambulabGlobal
's 3D printer & self correcting ML tech
This thing is so cheap and so fast
It's actually alien technology. Just worked out of the box
This is actually some "I saw this in the movies in 1980s" tech
------
between this and my GPU training models, my room is getting a little hot
the future rocks
------
this alone justified the entire purchase
------
That's one thing we agree on.
I suggest a gentle ramp up:
- Surely you are joking Mr Feynman 
- QED
- The Feynman Lectures on Physics
- Quantum Mechanics and Path Integrals (Feynman & Hibbs)
------
People should read Feynman’s books
------
thanks for writing this 
@francoisfleuret
!
------
I was looking for a no frills book. 
Not all the way through, but learning a lot going through it
------
LOL 
@ylecun
 got hacked. Somebody reach out to him!
------
nooooooooooo they got my boy lecunster  ;_; mooooooooooods
------
trying to read, understand, or use the transformers library & logic
------
Actually, I am going to spend 12 hours rewriting everything in Jax. I would suffer less spiritual pain doing so
------
hearing the big guy (marc andreesen) pretty much sum up what is happening was really cathartic
------
you cannot stop technology
------
Another sad day for open source. I personally wrote the first version of token-streaming for this.
------
Today is a huge milestone for one of our latest libraries: Text Generation Inference - we released v1.0 and under a new license: HFOIL 1.0
https://github.com/huggingface/text-generation-inference…

This  explains what this new license means, and why the change!
------
Us? Oh, we're the anti bad guys. So of course we would be pro good guys. And being anti anti bad guys would then mean you are pro bad guys. You're not pro bad guy, right? That would be extremely immoral of you to do, and that would risk your career. So, are you pro anti bad guys?
------
I am immediately skeptical of any software company that pays its respects to cultural zeitgeist "We are the le good guys" memes
------
please reel in the people who control your comms
you have no idea how much damage it causes
------
as in, to your company
------
I am hearing reports that because people can simulate AI girlfriends, they are realizing how bad they want companionship and subsequently, are touching grass and attempting to meet women for the first time

It looks like we're back?
------
There is a non zero chance that AI girlfriends end up being tutorial island for men, which creates good outcomes for both genders
------
technological advance that improves labor productivity by 10x is a much bigger first order consideration than second order correction terms like guilds or trade unions. technological advance wins every time
------
hackernews has the type of pessimistic celery that you'd only expect from someone who still thinks there is alpha in hackernews
------
a man's life should fit into a backpack, and a 32GB drive
------
it's 2023, no one reads resumes
------
the biggest mistake I see ML researchers is think that there is something special going on with humans

we are a layer of hacks, just like our artificial analogues
------
A 500k year/engineer may literally be 10x more likely to run a project without it going off the rails than a 150k/year engineer.
------
If you're a politician, technical worker, salesperson, blue collar worker, academic, the highest return thing you can do for your career, your goals, your life, is still get extremely jacked
------
There are 0 situations where being jacked doesn't make you more successful

Humans simply like people who are jacked/physically fit more
------
The future of technical books will be small, and aesthetic, with little elaboration

They will guide the reader through important concepts, which they can then thoroughly learn using a godlike QA agent like GPT4, when necessary
------
WE WILL NEVER STOP
BUILDING TECHNOLOGY
------
The constant, unwavering forward progression
Has given us, hackers, untold powers
We can generate art at the click of a button
Process documents better than any lawyer
Create tools out of processed plants, faster than any woodworker

enter the age of the nerd
augment yourself
------
My mind is actually boggled at 
@BambulabGlobal
's 3D printer & self correcting ML tech
This thing is so cheap and so fast
It's actually alien technology. Just worked out of the box
This is actually some "I saw this in the movies in 1980s" tech
------
between this and my GPU training models, my room is getting a little hot
the future rocks
------
this alone justified the entire purchase
------
That's one thing we agree on.
I suggest a gentle ramp up:
- Surely you are joking Mr Feynman 
- QED
- The Feynman Lectures on Physics
- Quantum Mechanics and Path Integrals (Feynman & Hibbs)
------
People should read Feynman’s books
------
thanks for writing this 
@francoisfleuret
!
------
I was looking for a no frills book. 
Not all the way through, but learning a lot going through it
------
LOL 
@ylecun
 got hacked. Somebody reach out to him!
------
nooooooooooo they got my boy lecunster  ;_; mooooooooooods
------
trying to read, understand, or use the transformers library & logic
------
Actually, I am going to spend 12 hours rewriting everything in Jax. I would suffer less spiritual pain doing so
------
hearing the big guy (marc andreesen) pretty much sum up what is happening was really cathartic
------
you cannot stop technology
------
Another sad day for open source. I personally wrote the first version of token-streaming for this.
------
Today is a huge milestone for one of our latest libraries: Text Generation Inference - we released v1.0 and under a new license: HFOIL 1.0
https://github.com/huggingface/text-generation-inference…

This  explains what this new license means, and why the change!
------
Us? Oh, we're the anti bad guys. So of course we would be pro good guys. And being anti anti bad guys would then mean you are pro bad guys. You're not pro bad guy, right? That would be extremely immoral of you to do, and that would risk your career. So, are you pro anti bad guys?
------
I am immediately skeptical of any software company that pays its respects to cultural zeitgeist "We are the le good guys" memes
------
please reel in the people who control your comms
you have no idea how much damage it causes
------
as in, to your company
------
I am hearing reports that because people can simulate AI girlfriends, they are realizing how bad they want companionship and subsequently, are touching grass and attempting to meet women for the first time

It looks like we're back?
------
There is a non zero chance that AI girlfriends end up being tutorial island for men, which creates good outcomes for both genders
------
Providing cheap and easy ways to inference open-source AI models ~is~ a way to support open source.

The goal ~should~ be to drive down the cost for anyone to play and make it as simple as possible. 

I've said many times and personally conveyed that 
@huggingface
 should host… Show more
------
Thanks so much for your contribution Yannic (on this and on the field in general) and sorry it sucks. Looking back, it was probably a mistake on our part not to create this specific library with a different license from day 1 a few months ago.  

From what I understand from the… Show more
------
A quality of good software is that I can remove it, delete it, gut it, and consequent systems do not suffer
------
Watching LLMs hallucinate is so amusing

"Luffy became a sworn sworn enemy of the government, but he didn't care about that and continued his journey as a pirate, searching for the "Golden Fleece."

7b params is just gomu gomu no bohhhhhhh
------
"The main character of One Piece is Monkey D. Lewin, better known as Luffy. Luffy is a teenager who, along with his friends, Goku and Uta, has become a pirate to search for the legendary "Soft Money" (Lufy's words, not the author's)"
------
"The main character of One Piece is a boy named Monkey D. Dragon, better known as Nico Robin."
------
WE WILL NEVER STOP
BUILDING TECHNOLOGY
------
The constant, unwavering forward progression
Has given us, hackers, untold powers
We can generate art at the click of a button
Process documents better than any lawyer
Create tools out of processed plants, faster than any woodworker

enter the age of the nerd
augment yourself
------
My mind is actually boggled at 
@BambulabGlobal
's 3D printer & self correcting ML tech
This thing is so cheap and so fast
It's actually alien technology. Just worked out of the box
This is actually some "I saw this in the movies in 1980s" tech
------
between this and my GPU training models, my room is getting a little hot
the future rocks
------
this alone justified the entire purchase
------
That's one thing we agree on.
I suggest a gentle ramp up:
- Surely you are joking Mr Feynman 
- QED
- The Feynman Lectures on Physics
- Quantum Mechanics and Path Integrals (Feynman & Hibbs)
------
People should read Feynman’s books
------
thanks for writing this 
@francoisfleuret
!
------
I was looking for a no frills book. 
Not all the way through, but learning a lot going through it
------
LOL 
@ylecun
 got hacked. Somebody reach out to him!
------
nooooooooooo they got my boy lecunster  ;_; mooooooooooods
------
trying to read, understand, or use the transformers library & logic
------
Actually, I am going to spend 12 hours rewriting everything in Jax. I would suffer less spiritual pain doing so
------
hearing the big guy (marc andreesen) pretty much sum up what is happening was really cathartic
------
you cannot stop technology
------
Another sad day for open source. I personally wrote the first version of token-streaming for this.
------
Today is a huge milestone for one of our latest libraries: Text Generation Inference - we released v1.0 and under a new license: HFOIL 1.0
https://github.com/huggingface/text-generation-inference…

This  explains what this new license means, and why the change!
------
Us? Oh, we're the anti bad guys. So of course we would be pro good guys. And being anti anti bad guys would then mean you are pro bad guys. You're not pro bad guy, right? That would be extremely immoral of you to do, and that would risk your career. So, are you pro anti bad guys?
------
I am immediately skeptical of any software company that pays its respects to cultural zeitgeist "We are the le good guys" memes
------
please reel in the people who control your comms
you have no idea how much damage it causes
------
as in, to your company
------
I am hearing reports that because people can simulate AI girlfriends, they are realizing how bad they want companionship and subsequently, are touching grass and attempting to meet women for the first time

It looks like we're back?
------
There is a non zero chance that AI girlfriends end up being tutorial island for men, which creates good outcomes for both genders
------
Providing cheap and easy ways to inference open-source AI models ~is~ a way to support open source.

The goal ~should~ be to drive down the cost for anyone to play and make it as simple as possible. 

I've said many times and personally conveyed that 
@huggingface
 should host… Show more
------
Thanks so much for your contribution Yannic (on this and on the field in general) and sorry it sucks. Looking back, it was probably a mistake on our part not to create this specific library with a different license from day 1 a few months ago.  

From what I understand from the… Show more
------
A quality of good software is that I can remove it, delete it, gut it, and consequent systems do not suffer
------
Watching LLMs hallucinate is so amusing

"Luffy became a sworn sworn enemy of the government, but he didn't care about that and continued his journey as a pirate, searching for the "Golden Fleece."

7b params is just gomu gomu no bohhhhhhh
------
"The main character of One Piece is Monkey D. Lewin, better known as Luffy. Luffy is a teenager who, along with his friends, Goku and Uta, has become a pirate to search for the legendary "Soft Money" (Lufy's words, not the author's)"
------
"The main character of One Piece is a boy named Monkey D. Dragon, better known as Nico Robin."
------
weights & biases is actually just good software
it doesn't get in the way
------
i was not paid for this tweet
------
convincing my warhammer 40k friend that GPT4 is really AGI by being able to answer anything about its lore, and keeping the conversation going when he sends me youtube videos about it
------
I will soon reveal the truth to him
------
build useful [1] stuff
[1] useful: your users actually use it
------
The biggest trick that mega journalist organizations have played on us is convincing many of us that alignment & acceleration are incompatible
------
we will simply align the AI to align faster
------
the goodies: an X T-Shirt and a note saying "The Future God-Emperor of Mankind sends his regards", with elon's thumbprint made in what seems to be blood
------
How do i sue a billionaire?
------
this alone justified the entire purchase
------
That's one thing we agree on.
I suggest a gentle ramp up:
- Surely you are joking Mr Feynman 
- QED
- The Feynman Lectures on Physics
- Quantum Mechanics and Path Integrals (Feynman & Hibbs)
------
People should read Feynman’s books
------
thanks for writing this 
@francoisfleuret
!
------
I was looking for a no frills book. 
Not all the way through, but learning a lot going through it
------
LOL 
@ylecun
 got hacked. Somebody reach out to him!
------
nooooooooooo they got my boy lecunster  ;_; mooooooooooods
------
trying to read, understand, or use the transformers library & logic
------
Actually, I am going to spend 12 hours rewriting everything in Jax. I would suffer less spiritual pain doing so
------
hearing the big guy (marc andreesen) pretty much sum up what is happening was really cathartic
------
you cannot stop technology
------
Another sad day for open source. I personally wrote the first version of token-streaming for this.
------
Today is a huge milestone for one of our latest libraries: Text Generation Inference - we released v1.0 and under a new license: HFOIL 1.0
https://github.com/huggingface/text-generation-inference…

This  explains what this new license means, and why the change!
------
Us? Oh, we're the anti bad guys. So of course we would be pro good guys. And being anti anti bad guys would then mean you are pro bad guys. You're not pro bad guy, right? That would be extremely immoral of you to do, and that would risk your career. So, are you pro anti bad guys?
------
I am immediately skeptical of any software company that pays its respects to cultural zeitgeist "We are the le good guys" memes
------
please reel in the people who control your comms
you have no idea how much damage it causes
------
as in, to your company
------
I am hearing reports that because people can simulate AI girlfriends, they are realizing how bad they want companionship and subsequently, are touching grass and attempting to meet women for the first time

It looks like we're back?
------
There is a non zero chance that AI girlfriends end up being tutorial island for men, which creates good outcomes for both genders
------
Providing cheap and easy ways to inference open-source AI models ~is~ a way to support open source.

The goal ~should~ be to drive down the cost for anyone to play and make it as simple as possible. 

I've said many times and personally conveyed that 
@huggingface
 should host… Show more
------
Thanks so much for your contribution Yannic (on this and on the field in general) and sorry it sucks. Looking back, it was probably a mistake on our part not to create this specific library with a different license from day 1 a few months ago.  

From what I understand from the… Show more
------
A quality of good software is that I can remove it, delete it, gut it, and consequent systems do not suffer
------
Watching LLMs hallucinate is so amusing

"Luffy became a sworn sworn enemy of the government, but he didn't care about that and continued his journey as a pirate, searching for the "Golden Fleece."

7b params is just gomu gomu no bohhhhhhh
------
"The main character of One Piece is Monkey D. Lewin, better known as Luffy. Luffy is a teenager who, along with his friends, Goku and Uta, has become a pirate to search for the legendary "Soft Money" (Lufy's words, not the author's)"
------
"The main character of One Piece is a boy named Monkey D. Dragon, better known as Nico Robin."
------
weights & biases is actually just good software
it doesn't get in the way
------
i was not paid for this tweet
------
convincing my warhammer 40k friend that GPT4 is really AGI by being able to answer anything about its lore, and keeping the conversation going when he sends me youtube videos about it
------
I will soon reveal the truth to him
------
build useful [1] stuff
[1] useful: your users actually use it
------
The biggest trick that mega journalist organizations have played on us is convincing many of us that alignment & acceleration are incompatible
------
we will simply align the AI to align faster
------
the goodies: an X T-Shirt and a note saying "The Future God-Emperor of Mankind sends his regards", with elon's thumbprint made in what seems to be blood
------
How do i sue a billionaire?
------
I much prefer this version of Cyber-Feudalism, as opposed to the faceless misaligned corporations' "you will eat bugs and like it" flavor

at least this is honest
------
oops it's fake
elon musk plz add notes
------
PSA you should scrape everything off of youtube now
------
this is a message to competing megacorps, not people like you and me
------
>g*ry marcus
------
the democratic neoliberalism must flow
------
LOL 
@ylecun
 got hacked. Somebody reach out to him!
------
nooooooooooo they got my boy lecunster  ;_; mooooooooooods
------
trying to read, understand, or use the transformers library & logic
------
Actually, I am going to spend 12 hours rewriting everything in Jax. I would suffer less spiritual pain doing so
------
hearing the big guy (marc andreesen) pretty much sum up what is happening was really cathartic
------
you cannot stop technology
------
Another sad day for open source. I personally wrote the first version of token-streaming for this.
------
Today is a huge milestone for one of our latest libraries: Text Generation Inference - we released v1.0 and under a new license: HFOIL 1.0
https://github.com/huggingface/text-generation-inference…

This  explains what this new license means, and why the change!
------
Us? Oh, we're the anti bad guys. So of course we would be pro good guys. And being anti anti bad guys would then mean you are pro bad guys. You're not pro bad guy, right? That would be extremely immoral of you to do, and that would risk your career. So, are you pro anti bad guys?
------
I am immediately skeptical of any software company that pays its respects to cultural zeitgeist "We are the le good guys" memes
------
please reel in the people who control your comms
you have no idea how much damage it causes
------
as in, to your company
------
I am hearing reports that because people can simulate AI girlfriends, they are realizing how bad they want companionship and subsequently, are touching grass and attempting to meet women for the first time

It looks like we're back?
------
There is a non zero chance that AI girlfriends end up being tutorial island for men, which creates good outcomes for both genders
------
Providing cheap and easy ways to inference open-source AI models ~is~ a way to support open source.

The goal ~should~ be to drive down the cost for anyone to play and make it as simple as possible. 

I've said many times and personally conveyed that 
@huggingface
 should host… Show more
------
Thanks so much for your contribution Yannic (on this and on the field in general) and sorry it sucks. Looking back, it was probably a mistake on our part not to create this specific library with a different license from day 1 a few months ago.  

From what I understand from the… Show more
------
A quality of good software is that I can remove it, delete it, gut it, and consequent systems do not suffer
------
Watching LLMs hallucinate is so amusing

"Luffy became a sworn sworn enemy of the government, but he didn't care about that and continued his journey as a pirate, searching for the "Golden Fleece."

7b params is just gomu gomu no bohhhhhhh
------
"The main character of One Piece is Monkey D. Lewin, better known as Luffy. Luffy is a teenager who, along with his friends, Goku and Uta, has become a pirate to search for the legendary "Soft Money" (Lufy's words, not the author's)"
------
"The main character of One Piece is a boy named Monkey D. Dragon, better known as Nico Robin."
------
weights & biases is actually just good software
it doesn't get in the way
------
i was not paid for this tweet
------
convincing my warhammer 40k friend that GPT4 is really AGI by being able to answer anything about its lore, and keeping the conversation going when he sends me youtube videos about it
------
I will soon reveal the truth to him
------
build useful [1] stuff
[1] useful: your users actually use it
------
The biggest trick that mega journalist organizations have played on us is convincing many of us that alignment & acceleration are incompatible
------
we will simply align the AI to align faster
------
the goodies: an X T-Shirt and a note saying "The Future God-Emperor of Mankind sends his regards", with elon's thumbprint made in what seems to be blood
------
How do i sue a billionaire?
------
I much prefer this version of Cyber-Feudalism, as opposed to the faceless misaligned corporations' "you will eat bugs and like it" flavor

at least this is honest
------
oops it's fake
elon musk plz add notes
------
PSA you should scrape everything off of youtube now
------
this is a message to competing megacorps, not people like you and me
------
>g*ry marcus
------
the democratic neoliberalism must flow
------
controversy is good as long as you're on the right wrong side
------
I'm ablating. Brooo I need to ablate. Bro I swear just let me move the needle. Theory? No let me just remove this one ingredient a tiny bit. I'm gonna ablate. I'm just going to bin search every possible combination one variable at a time. I'm ablating. I'm going to take other… Show more
------
 omg JH Kim just takes a giant  on physicists and posts this on his LinkedIn. 

Didn’t find room temperature superconductors? It’s a skill issue bro, you just needed to be a chemist and do 1000 experiments over 19 years. 

Anyone can do it.  twitter.com/8teAPi/status/…
------
i find it uplifting that one's creativity tends to transfer. if you are good at thinking outside of the box in one area it's typical true you can also do so for another
------
I took a quick look at the Web Integrity API, and my conclusion is that google is launching a blatant attack on privacy, and is attempting to control the web
------
I try to not be negative, but the proposal is:
- unclearly specified (Low.. entropy? Who are the.. attesters?)
- not put into a standards track or through a cross-industry group (it is literally up on a github page)
- already got implemented & merged
------
there is a reason you should get things through by engaging with the IETF working groups

And there probably a reason why it wasn't
pic related
------
you cannot stop technology
------
Another sad day for open source. I personally wrote the first version of token-streaming for this.
------
Today is a huge milestone for one of our latest libraries: Text Generation Inference - we released v1.0 and under a new license: HFOIL 1.0
https://github.com/huggingface/text-generation-inference…

This  explains what this new license means, and why the change!
------
Us? Oh, we're the anti bad guys. So of course we would be pro good guys. And being anti anti bad guys would then mean you are pro bad guys. You're not pro bad guy, right? That would be extremely immoral of you to do, and that would risk your career. So, are you pro anti bad guys?
------
I am immediately skeptical of any software company that pays its respects to cultural zeitgeist "We are the le good guys" memes
------
please reel in the people who control your comms
you have no idea how much damage it causes
------
as in, to your company
------
I am hearing reports that because people can simulate AI girlfriends, they are realizing how bad they want companionship and subsequently, are touching grass and attempting to meet women for the first time

It looks like we're back?
------
There is a non zero chance that AI girlfriends end up being tutorial island for men, which creates good outcomes for both genders
------
Providing cheap and easy ways to inference open-source AI models ~is~ a way to support open source.

The goal ~should~ be to drive down the cost for anyone to play and make it as simple as possible. 

I've said many times and personally conveyed that 
@huggingface
 should host… Show more
------
Thanks so much for your contribution Yannic (on this and on the field in general) and sorry it sucks. Looking back, it was probably a mistake on our part not to create this specific library with a different license from day 1 a few months ago.  

From what I understand from the… Show more
------
A quality of good software is that I can remove it, delete it, gut it, and consequent systems do not suffer
------
Watching LLMs hallucinate is so amusing

"Luffy became a sworn sworn enemy of the government, but he didn't care about that and continued his journey as a pirate, searching for the "Golden Fleece."

7b params is just gomu gomu no bohhhhhhh
------
"The main character of One Piece is Monkey D. Lewin, better known as Luffy. Luffy is a teenager who, along with his friends, Goku and Uta, has become a pirate to search for the legendary "Soft Money" (Lufy's words, not the author's)"
------
"The main character of One Piece is a boy named Monkey D. Dragon, better known as Nico Robin."
------
weights & biases is actually just good software
it doesn't get in the way
------
i was not paid for this tweet
------
convincing my warhammer 40k friend that GPT4 is really AGI by being able to answer anything about its lore, and keeping the conversation going when he sends me youtube videos about it
------
I will soon reveal the truth to him
------
build useful [1] stuff
[1] useful: your users actually use it
------
The biggest trick that mega journalist organizations have played on us is convincing many of us that alignment & acceleration are incompatible
------
we will simply align the AI to align faster
------
the goodies: an X T-Shirt and a note saying "The Future God-Emperor of Mankind sends his regards", with elon's thumbprint made in what seems to be blood
------
How do i sue a billionaire?
------
I much prefer this version of Cyber-Feudalism, as opposed to the faceless misaligned corporations' "you will eat bugs and like it" flavor

at least this is honest
------
oops it's fake
elon musk plz add notes
------
PSA you should scrape everything off of youtube now
------
this is a message to competing megacorps, not people like you and me
------
>g*ry marcus
------
the democratic neoliberalism must flow
------
controversy is good as long as you're on the right wrong side
------
I'm ablating. Brooo I need to ablate. Bro I swear just let me move the needle. Theory? No let me just remove this one ingredient a tiny bit. I'm gonna ablate. I'm just going to bin search every possible combination one variable at a time. I'm ablating. I'm going to take other… Show more
------
 omg JH Kim just takes a giant  on physicists and posts this on his LinkedIn. 

Didn’t find room temperature superconductors? It’s a skill issue bro, you just needed to be a chemist and do 1000 experiments over 19 years. 

Anyone can do it.  twitter.com/8teAPi/status/…
------
i find it uplifting that one's creativity tends to transfer. if you are good at thinking outside of the box in one area it's typical true you can also do so for another
------
I took a quick look at the Web Integrity API, and my conclusion is that google is launching a blatant attack on privacy, and is attempting to control the web
------
I try to not be negative, but the proposal is:
- unclearly specified (Low.. entropy? Who are the.. attesters?)
- not put into a standards track or through a cross-industry group (it is literally up on a github page)
- already got implemented & merged
------
there is a reason you should get things through by engaging with the IETF working groups

And there probably a reason why it wasn't
pic related
------
Web Integrity API? Sorry google, interest rates getting a little high, eh? I'm going to take out ol' trusty (firefox) and keep on trucking :thumbs_up:
------
Web Integrity Api? Sorry, chuds. BrendanEich, the inventor of the best programming language in the world, has got my back
------
We are a fork, have been all along, the “reskinned” claim is complete nonsense. We won’t be shipping WEI support, just as we disable or otherwise nullify lots of other junk that Google puts into Chromium. https://github.com/brave/brave-browser/wiki/Deviations-from-Chromium-(features-we-disable-or-remove)… http://brave.com/privacy-updates.
------
Who owns the attester APIs?
What does low entropy mean? 
Why track at all? If you actually want users to identify themselves, why don't you just make people log in?

Web Environment Integrity seems extremely sketchy
What is it even trying to fix? Why the tracking?
------
Personally, I would have chosen a better name. Like; Low Entropy User Tracking while Giving Sites The Ability To Deny Access

:P

that said, trade off space is difficult to navigate. I just don't get the point of centralization
------
humanity ftw; never doubt the compounding power of scientific progress over long periods of time
------
when I was young I was jealous I wasn’t living in the time of Einstein and Bohr and Fermi watching mental models of the universe rearrange every week. but things are actually moving way faster today
------
is there a version of oppenheimer without the liberal arts major written sex scenes? christ...
------
btw liberal arts majors are good
it's just that the sex scene was a little.. too.. liberal arts
------
Providing cheap and easy ways to inference open-source AI models ~is~ a way to support open source.

The goal ~should~ be to drive down the cost for anyone to play and make it as simple as possible. 

I've said many times and personally conveyed that 
@huggingface
 should host… Show more
------
Thanks so much for your contribution Yannic (on this and on the field in general) and sorry it sucks. Looking back, it was probably a mistake on our part not to create this specific library with a different license from day 1 a few months ago.  

From what I understand from the… Show more
------
A quality of good software is that I can remove it, delete it, gut it, and consequent systems do not suffer
------
Watching LLMs hallucinate is so amusing

"Luffy became a sworn sworn enemy of the government, but he didn't care about that and continued his journey as a pirate, searching for the "Golden Fleece."

7b params is just gomu gomu no bohhhhhhh
------
"The main character of One Piece is Monkey D. Lewin, better known as Luffy. Luffy is a teenager who, along with his friends, Goku and Uta, has become a pirate to search for the legendary "Soft Money" (Lufy's words, not the author's)"
------
"The main character of One Piece is a boy named Monkey D. Dragon, better known as Nico Robin."
------
weights & biases is actually just good software
it doesn't get in the way
------
i was not paid for this tweet
------
convincing my warhammer 40k friend that GPT4 is really AGI by being able to answer anything about its lore, and keeping the conversation going when he sends me youtube videos about it
------
I will soon reveal the truth to him
------
build useful [1] stuff
[1] useful: your users actually use it
------
The biggest trick that mega journalist organizations have played on us is convincing many of us that alignment & acceleration are incompatible
------
we will simply align the AI to align faster
------
the goodies: an X T-Shirt and a note saying "The Future God-Emperor of Mankind sends his regards", with elon's thumbprint made in what seems to be blood
------
How do i sue a billionaire?
------
I much prefer this version of Cyber-Feudalism, as opposed to the faceless misaligned corporations' "you will eat bugs and like it" flavor

at least this is honest
------
oops it's fake
elon musk plz add notes
------
PSA you should scrape everything off of youtube now
------
this is a message to competing megacorps, not people like you and me
------
>g*ry marcus
------
the democratic neoliberalism must flow
------
controversy is good as long as you're on the right wrong side
------
I'm ablating. Brooo I need to ablate. Bro I swear just let me move the needle. Theory? No let me just remove this one ingredient a tiny bit. I'm gonna ablate. I'm just going to bin search every possible combination one variable at a time. I'm ablating. I'm going to take other… Show more
------
 omg JH Kim just takes a giant  on physicists and posts this on his LinkedIn. 

Didn’t find room temperature superconductors? It’s a skill issue bro, you just needed to be a chemist and do 1000 experiments over 19 years. 

Anyone can do it.  twitter.com/8teAPi/status/…
------
i find it uplifting that one's creativity tends to transfer. if you are good at thinking outside of the box in one area it's typical true you can also do so for another
------
I took a quick look at the Web Integrity API, and my conclusion is that google is launching a blatant attack on privacy, and is attempting to control the web
------
I try to not be negative, but the proposal is:
- unclearly specified (Low.. entropy? Who are the.. attesters?)
- not put into a standards track or through a cross-industry group (it is literally up on a github page)
- already got implemented & merged
------
there is a reason you should get things through by engaging with the IETF working groups

And there probably a reason why it wasn't
pic related
------
Web Integrity API? Sorry google, interest rates getting a little high, eh? I'm going to take out ol' trusty (firefox) and keep on trucking :thumbs_up:
------
Web Integrity Api? Sorry, chuds. BrendanEich, the inventor of the best programming language in the world, has got my back
------
We are a fork, have been all along, the “reskinned” claim is complete nonsense. We won’t be shipping WEI support, just as we disable or otherwise nullify lots of other junk that Google puts into Chromium. https://github.com/brave/brave-browser/wiki/Deviations-from-Chromium-(features-we-disable-or-remove)… http://brave.com/privacy-updates.
------
Who owns the attester APIs?
What does low entropy mean? 
Why track at all? If you actually want users to identify themselves, why don't you just make people log in?

Web Environment Integrity seems extremely sketchy
What is it even trying to fix? Why the tracking?
------
Personally, I would have chosen a better name. Like; Low Entropy User Tracking while Giving Sites The Ability To Deny Access

:P

that said, trade off space is difficult to navigate. I just don't get the point of centralization
------
humanity ftw; never doubt the compounding power of scientific progress over long periods of time
------
when I was young I was jealous I wasn’t living in the time of Einstein and Bohr and Fermi watching mental models of the universe rearrange every week. but things are actually moving way faster today
------
is there a version of oppenheimer without the liberal arts major written sex scenes? christ...
------
btw liberal arts majors are good
it's just that the sex scene was a little.. too.. liberal arts
------
Releasing LLongMA-2 16k, a suite of Llama-2 models, trained at 16k context length using linear positional interpolation scaling. The model was trained in collaboration with 
@theemozilla
 of 
@NousResearch
 and 
@kaiokendev1
.
------
good morning
------
is Gzip all you need?! - writing our own gzip+kNN classifier for sentiment analysis in response to the “Low-Resource” TextClassification: A Parameter-Free Classification Method with Compressors paper.

Video: https://youtube.com/watch?v=jkdWzvMOPuo…
Github: https://github.com/Sentdex/Simple-kNN-Gzip…
------
NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection

paper page: https://huggingface.co/papers/2307.14620…

present NeRF-Det, a novel method for indoor 3D detection with posed RGB images as input. Unlike existing indoor 3D detection methods that struggle… Show more
------
weights & biases is actually just good software
it doesn't get in the way
------
i was not paid for this tweet
------
convincing my warhammer 40k friend that GPT4 is really AGI by being able to answer anything about its lore, and keeping the conversation going when he sends me youtube videos about it
------
I will soon reveal the truth to him
------
build useful [1] stuff
[1] useful: your users actually use it
------
The biggest trick that mega journalist organizations have played on us is convincing many of us that alignment & acceleration are incompatible
------
we will simply align the AI to align faster
------
the goodies: an X T-Shirt and a note saying "The Future God-Emperor of Mankind sends his regards", with elon's thumbprint made in what seems to be blood
------
How do i sue a billionaire?
------
I much prefer this version of Cyber-Feudalism, as opposed to the faceless misaligned corporations' "you will eat bugs and like it" flavor

at least this is honest
------
oops it's fake
elon musk plz add notes
------
PSA you should scrape everything off of youtube now
------
this is a message to competing megacorps, not people like you and me
------
>g*ry marcus
------
the democratic neoliberalism must flow
------
controversy is good as long as you're on the right wrong side
------
I'm ablating. Brooo I need to ablate. Bro I swear just let me move the needle. Theory? No let me just remove this one ingredient a tiny bit. I'm gonna ablate. I'm just going to bin search every possible combination one variable at a time. I'm ablating. I'm going to take other… Show more
------
 omg JH Kim just takes a giant  on physicists and posts this on his LinkedIn. 

Didn’t find room temperature superconductors? It’s a skill issue bro, you just needed to be a chemist and do 1000 experiments over 19 years. 

Anyone can do it.  twitter.com/8teAPi/status/…
------
i find it uplifting that one's creativity tends to transfer. if you are good at thinking outside of the box in one area it's typical true you can also do so for another
------
I took a quick look at the Web Integrity API, and my conclusion is that google is launching a blatant attack on privacy, and is attempting to control the web
------
I try to not be negative, but the proposal is:
- unclearly specified (Low.. entropy? Who are the.. attesters?)
- not put into a standards track or through a cross-industry group (it is literally up on a github page)
- already got implemented & merged
------
there is a reason you should get things through by engaging with the IETF working groups

And there probably a reason why it wasn't
pic related
------
Web Integrity API? Sorry google, interest rates getting a little high, eh? I'm going to take out ol' trusty (firefox) and keep on trucking :thumbs_up:
------
Web Integrity Api? Sorry, chuds. BrendanEich, the inventor of the best programming language in the world, has got my back
------
We are a fork, have been all along, the “reskinned” claim is complete nonsense. We won’t be shipping WEI support, just as we disable or otherwise nullify lots of other junk that Google puts into Chromium. https://github.com/brave/brave-browser/wiki/Deviations-from-Chromium-(features-we-disable-or-remove)… http://brave.com/privacy-updates.
------
Who owns the attester APIs?
What does low entropy mean? 
Why track at all? If you actually want users to identify themselves, why don't you just make people log in?

Web Environment Integrity seems extremely sketchy
What is it even trying to fix? Why the tracking?
------
Personally, I would have chosen a better name. Like; Low Entropy User Tracking while Giving Sites The Ability To Deny Access

:P

that said, trade off space is difficult to navigate. I just don't get the point of centralization
------
humanity ftw; never doubt the compounding power of scientific progress over long periods of time
------
when I was young I was jealous I wasn’t living in the time of Einstein and Bohr and Fermi watching mental models of the universe rearrange every week. but things are actually moving way faster today
------
is there a version of oppenheimer without the liberal arts major written sex scenes? christ...
------
btw liberal arts majors are good
it's just that the sex scene was a little.. too.. liberal arts
------
Releasing LLongMA-2 16k, a suite of Llama-2 models, trained at 16k context length using linear positional interpolation scaling. The model was trained in collaboration with 
@theemozilla
 of 
@NousResearch
 and 
@kaiokendev1
.
------
good morning
------
is Gzip all you need?! - writing our own gzip+kNN classifier for sentiment analysis in response to the “Low-Resource” TextClassification: A Parameter-Free Classification Method with Compressors paper.

Video: https://youtube.com/watch?v=jkdWzvMOPuo…
Github: https://github.com/Sentdex/Simple-kNN-Gzip…
------
NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection

paper page: https://huggingface.co/papers/2307.14620…

present NeRF-Det, a novel method for indoor 3D detection with posed RGB images as input. Unlike existing indoor 3D detection methods that struggle… Show more
------
Elon liking my tweet
------
the goodies: an X T-Shirt and a note saying "The Future God-Emperor of Mankind sends his regards", with elon's thumbprint made in what seems to be blood
------
How do i sue a billionaire?
------
I much prefer this version of Cyber-Feudalism, as opposed to the faceless misaligned corporations' "you will eat bugs and like it" flavor

at least this is honest
------
oops it's fake
elon musk plz add notes
------
PSA you should scrape everything off of youtube now
------
this is a message to competing megacorps, not people like you and me
------
>g*ry marcus
------
the democratic neoliberalism must flow
------
controversy is good as long as you're on the right wrong side
------
I'm ablating. Brooo I need to ablate. Bro I swear just let me move the needle. Theory? No let me just remove this one ingredient a tiny bit. I'm gonna ablate. I'm just going to bin search every possible combination one variable at a time. I'm ablating. I'm going to take other… Show more
------
 omg JH Kim just takes a giant  on physicists and posts this on his LinkedIn. 

Didn’t find room temperature superconductors? It’s a skill issue bro, you just needed to be a chemist and do 1000 experiments over 19 years. 

Anyone can do it.  twitter.com/8teAPi/status/…
------
i find it uplifting that one's creativity tends to transfer. if you are good at thinking outside of the box in one area it's typical true you can also do so for another
------
I took a quick look at the Web Integrity API, and my conclusion is that google is launching a blatant attack on privacy, and is attempting to control the web
------
I try to not be negative, but the proposal is:
- unclearly specified (Low.. entropy? Who are the.. attesters?)
- not put into a standards track or through a cross-industry group (it is literally up on a github page)
- already got implemented & merged
------
there is a reason you should get things through by engaging with the IETF working groups

And there probably a reason why it wasn't
pic related
------
Web Integrity API? Sorry google, interest rates getting a little high, eh? I'm going to take out ol' trusty (firefox) and keep on trucking :thumbs_up:
------
Web Integrity Api? Sorry, chuds. BrendanEich, the inventor of the best programming language in the world, has got my back
------
We are a fork, have been all along, the “reskinned” claim is complete nonsense. We won’t be shipping WEI support, just as we disable or otherwise nullify lots of other junk that Google puts into Chromium. https://github.com/brave/brave-browser/wiki/Deviations-from-Chromium-(features-we-disable-or-remove)… http://brave.com/privacy-updates.
------
Who owns the attester APIs?
What does low entropy mean? 
Why track at all? If you actually want users to identify themselves, why don't you just make people log in?

Web Environment Integrity seems extremely sketchy
What is it even trying to fix? Why the tracking?
------
Personally, I would have chosen a better name. Like; Low Entropy User Tracking while Giving Sites The Ability To Deny Access

:P

that said, trade off space is difficult to navigate. I just don't get the point of centralization
------
humanity ftw; never doubt the compounding power of scientific progress over long periods of time
------
when I was young I was jealous I wasn’t living in the time of Einstein and Bohr and Fermi watching mental models of the universe rearrange every week. but things are actually moving way faster today
------
is there a version of oppenheimer without the liberal arts major written sex scenes? christ...
------
btw liberal arts majors are good
it's just that the sex scene was a little.. too.. liberal arts
------
Releasing LLongMA-2 16k, a suite of Llama-2 models, trained at 16k context length using linear positional interpolation scaling. The model was trained in collaboration with 
@theemozilla
 of 
@NousResearch
 and 
@kaiokendev1
.
------
good morning
------
is Gzip all you need?! - writing our own gzip+kNN classifier for sentiment analysis in response to the “Low-Resource” TextClassification: A Parameter-Free Classification Method with Compressors paper.

Video: https://youtube.com/watch?v=jkdWzvMOPuo…
Github: https://github.com/Sentdex/Simple-kNN-Gzip…
------
NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection

paper page: https://huggingface.co/papers/2307.14620…

present NeRF-Det, a novel method for indoor 3D detection with posed RGB images as input. Unlike existing indoor 3D detection methods that struggle… Show more
------
Elon liking my tweet
------
GM ITS TIME TO BUIDL
------
sup
------
while I'm lawful good, and not chaotic good, I respect and am thankful for Elbakyan's work & scihub. May history favor her forever
------
amusingly, the highest payoff thing you can do, as a politician, is get jacked and start producing tiktok shorts in a tight T-shirt spending time with your family
------
i've asked you this question a million times, and i've mastered my executive function enough to actually put in an amazon order

@darrenjennings
 what is your streamer voice mic that you used during meetings? I need it
------
I've been helping someone out w/ constrained image generation, and as a result I've figured out how to finetune stable diffusion

I now have a plan to collect vagabond panels and train a model
------
I should say thank you to the open source community
I am now capable of things I never have been before
I will make sure to repay the debt
------
humans are terrifying
------
we have actual dragonball z level power
can see it from space type capability of destruction
i hope that we continue to stay stable
war would be very bad
------
I'm ablating. Brooo I need to ablate. Bro I swear just let me move the needle. Theory? No let me just remove this one ingredient a tiny bit. I'm gonna ablate. I'm just going to bin search every possible combination one variable at a time. I'm ablating. I'm going to take other… Show more
------
 omg JH Kim just takes a giant  on physicists and posts this on his LinkedIn. 

Didn’t find room temperature superconductors? It’s a skill issue bro, you just needed to be a chemist and do 1000 experiments over 19 years. 

Anyone can do it.  twitter.com/8teAPi/status/…
------
i find it uplifting that one's creativity tends to transfer. if you are good at thinking outside of the box in one area it's typical true you can also do so for another
------
I took a quick look at the Web Integrity API, and my conclusion is that google is launching a blatant attack on privacy, and is attempting to control the web
------
I try to not be negative, but the proposal is:
- unclearly specified (Low.. entropy? Who are the.. attesters?)
- not put into a standards track or through a cross-industry group (it is literally up on a github page)
- already got implemented & merged
------
there is a reason you should get things through by engaging with the IETF working groups

And there probably a reason why it wasn't
pic related
------
Web Integrity API? Sorry google, interest rates getting a little high, eh? I'm going to take out ol' trusty (firefox) and keep on trucking :thumbs_up:
------
Web Integrity Api? Sorry, chuds. BrendanEich, the inventor of the best programming language in the world, has got my back
------
We are a fork, have been all along, the “reskinned” claim is complete nonsense. We won’t be shipping WEI support, just as we disable or otherwise nullify lots of other junk that Google puts into Chromium. https://github.com/brave/brave-browser/wiki/Deviations-from-Chromium-(features-we-disable-or-remove)… http://brave.com/privacy-updates.
------
Who owns the attester APIs?
What does low entropy mean? 
Why track at all? If you actually want users to identify themselves, why don't you just make people log in?

Web Environment Integrity seems extremely sketchy
What is it even trying to fix? Why the tracking?
------
Personally, I would have chosen a better name. Like; Low Entropy User Tracking while Giving Sites The Ability To Deny Access

:P

that said, trade off space is difficult to navigate. I just don't get the point of centralization
------
humanity ftw; never doubt the compounding power of scientific progress over long periods of time
------
when I was young I was jealous I wasn’t living in the time of Einstein and Bohr and Fermi watching mental models of the universe rearrange every week. but things are actually moving way faster today
------
is there a version of oppenheimer without the liberal arts major written sex scenes? christ...
------
btw liberal arts majors are good
it's just that the sex scene was a little.. too.. liberal arts
------
Releasing LLongMA-2 16k, a suite of Llama-2 models, trained at 16k context length using linear positional interpolation scaling. The model was trained in collaboration with 
@theemozilla
 of 
@NousResearch
 and 
@kaiokendev1
.
------
good morning
------
is Gzip all you need?! - writing our own gzip+kNN classifier for sentiment analysis in response to the “Low-Resource” TextClassification: A Parameter-Free Classification Method with Compressors paper.

Video: https://youtube.com/watch?v=jkdWzvMOPuo…
Github: https://github.com/Sentdex/Simple-kNN-Gzip…
------
NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection

paper page: https://huggingface.co/papers/2307.14620…

present NeRF-Det, a novel method for indoor 3D detection with posed RGB images as input. Unlike existing indoor 3D detection methods that struggle… Show more
------
Elon liking my tweet
------
GM ITS TIME TO BUIDL
------
sup
------
while I'm lawful good, and not chaotic good, I respect and am thankful for Elbakyan's work & scihub. May history favor her forever
------
amusingly, the highest payoff thing you can do, as a politician, is get jacked and start producing tiktok shorts in a tight T-shirt spending time with your family
------
i've asked you this question a million times, and i've mastered my executive function enough to actually put in an amazon order

@darrenjennings
 what is your streamer voice mic that you used during meetings? I need it
------
I've been helping someone out w/ constrained image generation, and as a result I've figured out how to finetune stable diffusion

I now have a plan to collect vagabond panels and train a model
------
I should say thank you to the open source community
I am now capable of things I never have been before
I will make sure to repay the debt
------
humans are terrifying
------
we have actual dragonball z level power
can see it from space type capability of destruction
i hope that we continue to stay stable
war would be very bad
------
"okay babe read this one part of the bhagavad gita while we have le seggs :DD"
never trust interns with prod
------
Congrats Pangu-coder2, which leverages our Evol-Instruct as WizardLM & WizardCoder to construct their training corpus, and achieves 62.20% pass@1 on HumanEval. 

Evol-Instruct has become one of the most necessary methods for SFT.

Hope they can open-source the model soon.
------
In case you didn't know, the increase of polarization in the US House of Representatives started long before social networks or the internet existed.
------
Mitosis of Congress
------
Pretty funny how financial types don't understand the concept of long-term R&D investment.
To them, R&D investment is "lost money".
It only becomes "lost" if/when you give up on the whole product concept.
But that has not happened.
------
Mark Zuckerberg's Metaverse has lost over $21.3 billion since 2022.
------
Web Integrity API? Sorry google, interest rates getting a little high, eh? I'm going to take out ol' trusty (firefox) and keep on trucking :thumbs_up:
------
Web Integrity Api? Sorry, chuds. BrendanEich, the inventor of the best programming language in the world, has got my back
------
We are a fork, have been all along, the “reskinned” claim is complete nonsense. We won’t be shipping WEI support, just as we disable or otherwise nullify lots of other junk that Google puts into Chromium. https://github.com/brave/brave-browser/wiki/Deviations-from-Chromium-(features-we-disable-or-remove)… http://brave.com/privacy-updates.
------
Who owns the attester APIs?
What does low entropy mean? 
Why track at all? If you actually want users to identify themselves, why don't you just make people log in?

Web Environment Integrity seems extremely sketchy
What is it even trying to fix? Why the tracking?
------
Personally, I would have chosen a better name. Like; Low Entropy User Tracking while Giving Sites The Ability To Deny Access

:P

that said, trade off space is difficult to navigate. I just don't get the point of centralization
------
humanity ftw; never doubt the compounding power of scientific progress over long periods of time
------
when I was young I was jealous I wasn’t living in the time of Einstein and Bohr and Fermi watching mental models of the universe rearrange every week. but things are actually moving way faster today
------
is there a version of oppenheimer without the liberal arts major written sex scenes? christ...
------
btw liberal arts majors are good
it's just that the sex scene was a little.. too.. liberal arts
------
Releasing LLongMA-2 16k, a suite of Llama-2 models, trained at 16k context length using linear positional interpolation scaling. The model was trained in collaboration with 
@theemozilla
 of 
@NousResearch
 and 
@kaiokendev1
.
------
good morning
------
is Gzip all you need?! - writing our own gzip+kNN classifier for sentiment analysis in response to the “Low-Resource” TextClassification: A Parameter-Free Classification Method with Compressors paper.

Video: https://youtube.com/watch?v=jkdWzvMOPuo…
Github: https://github.com/Sentdex/Simple-kNN-Gzip…
------
NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection

paper page: https://huggingface.co/papers/2307.14620…

present NeRF-Det, a novel method for indoor 3D detection with posed RGB images as input. Unlike existing indoor 3D detection methods that struggle… Show more
------
Elon liking my tweet
------
GM ITS TIME TO BUIDL
------
sup
------
while I'm lawful good, and not chaotic good, I respect and am thankful for Elbakyan's work & scihub. May history favor her forever
------
amusingly, the highest payoff thing you can do, as a politician, is get jacked and start producing tiktok shorts in a tight T-shirt spending time with your family
------
i've asked you this question a million times, and i've mastered my executive function enough to actually put in an amazon order

@darrenjennings
 what is your streamer voice mic that you used during meetings? I need it
------
I've been helping someone out w/ constrained image generation, and as a result I've figured out how to finetune stable diffusion

I now have a plan to collect vagabond panels and train a model
------
I should say thank you to the open source community
I am now capable of things I never have been before
I will make sure to repay the debt
------
humans are terrifying
------
we have actual dragonball z level power
can see it from space type capability of destruction
i hope that we continue to stay stable
war would be very bad
------
"okay babe read this one part of the bhagavad gita while we have le seggs :DD"
never trust interns with prod
------
Congrats Pangu-coder2, which leverages our Evol-Instruct as WizardLM & WizardCoder to construct their training corpus, and achieves 62.20% pass@1 on HumanEval. 

Evol-Instruct has become one of the most necessary methods for SFT.

Hope they can open-source the model soon.
------
In case you didn't know, the increase of polarization in the US House of Representatives started long before social networks or the internet existed.
------
Mitosis of Congress
------
Pretty funny how financial types don't understand the concept of long-term R&D investment.
To them, R&D investment is "lost money".
It only becomes "lost" if/when you give up on the whole product concept.
But that has not happened.
------
Mark Zuckerberg's Metaverse has lost over $21.3 billion since 2022.
------
Across all categories, we saw increased generalisation performance compared to previous baselines, such as on RT-1 models.

We also evaluated RT-2 on a number of unseen objects and environments where it could successfully adapt to new situations:
------
the speed of light is such an annoying thing
------
was asked why my google product tweets seem more negative than average

so a few minutes ago I had to fill out several recaptchas in firefox. the same form in google chrome gives me zero recaptchas, even with no cookies/incognito

it's the small things like that. really tiresome
------
yann lecun watching people load in superHOT-8k-airoboros llama 2 fine tunes to simulate girlfriends on anonymous imageboards
------
my biggest fear is Francois Chollet blocking me, so every time I respond in disagreement I follow it up with a tweet that says something like "I love you"
------
Yeah he’s surprisingly based
------
Releasing LLongMA-2 16k, a suite of Llama-2 models, trained at 16k context length using linear positional interpolation scaling. The model was trained in collaboration with 
@theemozilla
 of 
@NousResearch
 and 
@kaiokendev1
.
------
good morning
------
is Gzip all you need?! - writing our own gzip+kNN classifier for sentiment analysis in response to the “Low-Resource” TextClassification: A Parameter-Free Classification Method with Compressors paper.

Video: https://youtube.com/watch?v=jkdWzvMOPuo…
Github: https://github.com/Sentdex/Simple-kNN-Gzip…
------
NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection

paper page: https://huggingface.co/papers/2307.14620…

present NeRF-Det, a novel method for indoor 3D detection with posed RGB images as input. Unlike existing indoor 3D detection methods that struggle… Show more
------
Elon liking my tweet
------
GM ITS TIME TO BUIDL
------
sup
------
while I'm lawful good, and not chaotic good, I respect and am thankful for Elbakyan's work & scihub. May history favor her forever
------
amusingly, the highest payoff thing you can do, as a politician, is get jacked and start producing tiktok shorts in a tight T-shirt spending time with your family
------
i've asked you this question a million times, and i've mastered my executive function enough to actually put in an amazon order

@darrenjennings
 what is your streamer voice mic that you used during meetings? I need it
------
I've been helping someone out w/ constrained image generation, and as a result I've figured out how to finetune stable diffusion

I now have a plan to collect vagabond panels and train a model
------
I should say thank you to the open source community
I am now capable of things I never have been before
I will make sure to repay the debt
------
humans are terrifying
------
we have actual dragonball z level power
can see it from space type capability of destruction
i hope that we continue to stay stable
war would be very bad
------
"okay babe read this one part of the bhagavad gita while we have le seggs :DD"
never trust interns with prod
------
Congrats Pangu-coder2, which leverages our Evol-Instruct as WizardLM & WizardCoder to construct their training corpus, and achieves 62.20% pass@1 on HumanEval. 

Evol-Instruct has become one of the most necessary methods for SFT.

Hope they can open-source the model soon.
------
In case you didn't know, the increase of polarization in the US House of Representatives started long before social networks or the internet existed.
------
Mitosis of Congress
------
Pretty funny how financial types don't understand the concept of long-term R&D investment.
To them, R&D investment is "lost money".
It only becomes "lost" if/when you give up on the whole product concept.
But that has not happened.
------
Mark Zuckerberg's Metaverse has lost over $21.3 billion since 2022.
------
Across all categories, we saw increased generalisation performance compared to previous baselines, such as on RT-1 models.

We also evaluated RT-2 on a number of unseen objects and environments where it could successfully adapt to new situations:
------
the speed of light is such an annoying thing
------
was asked why my google product tweets seem more negative than average

so a few minutes ago I had to fill out several recaptchas in firefox. the same form in google chrome gives me zero recaptchas, even with no cookies/incognito

it's the small things like that. really tiresome
------
yann lecun watching people load in superHOT-8k-airoboros llama 2 fine tunes to simulate girlfriends on anonymous imageboards
------
my biggest fear is Francois Chollet blocking me, so every time I respond in disagreement I follow it up with a tweet that says something like "I love you"
------
Yeah he’s surprisingly based
------
Gn anons!

Another tuff day, but we keep on marching!
------
absolutely incredible exchange
------
.
------
The AI / atomic bombs analogies are dumb as hell.
------
New post today - remember something like 1% of people create 90% of what we see, hear and view in the world. Showing up is underrated
------
Elon liking my tweet
------
GM ITS TIME TO BUIDL
------
sup
------
while I'm lawful good, and not chaotic good, I respect and am thankful for Elbakyan's work & scihub. May history favor her forever
------
amusingly, the highest payoff thing you can do, as a politician, is get jacked and start producing tiktok shorts in a tight T-shirt spending time with your family
------
i've asked you this question a million times, and i've mastered my executive function enough to actually put in an amazon order

@darrenjennings
 what is your streamer voice mic that you used during meetings? I need it
------
I've been helping someone out w/ constrained image generation, and as a result I've figured out how to finetune stable diffusion

I now have a plan to collect vagabond panels and train a model
------
I should say thank you to the open source community
I am now capable of things I never have been before
I will make sure to repay the debt
------
humans are terrifying
------
we have actual dragonball z level power
can see it from space type capability of destruction
i hope that we continue to stay stable
war would be very bad
------
"okay babe read this one part of the bhagavad gita while we have le seggs :DD"
never trust interns with prod
------
Congrats Pangu-coder2, which leverages our Evol-Instruct as WizardLM & WizardCoder to construct their training corpus, and achieves 62.20% pass@1 on HumanEval. 

Evol-Instruct has become one of the most necessary methods for SFT.

Hope they can open-source the model soon.
------
In case you didn't know, the increase of polarization in the US House of Representatives started long before social networks or the internet existed.
------
Mitosis of Congress
------
Pretty funny how financial types don't understand the concept of long-term R&D investment.
To them, R&D investment is "lost money".
It only becomes "lost" if/when you give up on the whole product concept.
But that has not happened.
------
Mark Zuckerberg's Metaverse has lost over $21.3 billion since 2022.
------
Across all categories, we saw increased generalisation performance compared to previous baselines, such as on RT-1 models.

We also evaluated RT-2 on a number of unseen objects and environments where it could successfully adapt to new situations:
------
the speed of light is such an annoying thing
------
was asked why my google product tweets seem more negative than average

so a few minutes ago I had to fill out several recaptchas in firefox. the same form in google chrome gives me zero recaptchas, even with no cookies/incognito

it's the small things like that. really tiresome
------
yann lecun watching people load in superHOT-8k-airoboros llama 2 fine tunes to simulate girlfriends on anonymous imageboards
------
my biggest fear is Francois Chollet blocking me, so every time I respond in disagreement I follow it up with a tweet that says something like "I love you"
------
Yeah he’s surprisingly based
------
Gn anons!

Another tuff day, but we keep on marching!
------
absolutely incredible exchange
------
.
------
The AI / atomic bombs analogies are dumb as hell.
------
New post today - remember something like 1% of people create 90% of what we see, hear and view in the world. Showing up is underrated
------
When companies create this microservices bog and then, when any problem comes up, they say, “distributed systems are hard” it reminds me of when my toddler throws food on the floor then says, “look, big mess”
------
one twitter space later after asking anons advice re hyperparameters. Getting good results with qlora training w/ LIMA on a 7b llama model :DD

One 3090 is all you need

Again, thank you @Dogesator & 
@theemozilla
------
shout out to @Dogesator & 
@theemozilla
 for the twitter space today. I learned a lot!

If I had to boil it down to a single tweet:
- Get your train-eval loop down
- Mess with the hyperparameters
- There aint nothing to it, but to do it
------
Your success is directly related to how quickly you can experiment & validate
You need to actually try it
Focus on getting the time down
------
riffing on missing ingredients for symbolic agi
the goal here is to build one single dependency graph of knowledge & beliefs -- as opposed to having many separate belief graphs that either duplicate the same abstractions in incompatible ways, or ignore their dependencies entirely
------
sup
------
while I'm lawful good, and not chaotic good, I respect and am thankful for Elbakyan's work & scihub. May history favor her forever
------
amusingly, the highest payoff thing you can do, as a politician, is get jacked and start producing tiktok shorts in a tight T-shirt spending time with your family
------
i've asked you this question a million times, and i've mastered my executive function enough to actually put in an amazon order

@darrenjennings
 what is your streamer voice mic that you used during meetings? I need it
------
I've been helping someone out w/ constrained image generation, and as a result I've figured out how to finetune stable diffusion

I now have a plan to collect vagabond panels and train a model
------
I should say thank you to the open source community
I am now capable of things I never have been before
I will make sure to repay the debt
------
humans are terrifying
------
we have actual dragonball z level power
can see it from space type capability of destruction
i hope that we continue to stay stable
war would be very bad
------
"okay babe read this one part of the bhagavad gita while we have le seggs :DD"
never trust interns with prod
------
Congrats Pangu-coder2, which leverages our Evol-Instruct as WizardLM & WizardCoder to construct their training corpus, and achieves 62.20% pass@1 on HumanEval. 

Evol-Instruct has become one of the most necessary methods for SFT.

Hope they can open-source the model soon.
------
In case you didn't know, the increase of polarization in the US House of Representatives started long before social networks or the internet existed.
------
Mitosis of Congress
------
Pretty funny how financial types don't understand the concept of long-term R&D investment.
To them, R&D investment is "lost money".
It only becomes "lost" if/when you give up on the whole product concept.
But that has not happened.
------
Mark Zuckerberg's Metaverse has lost over $21.3 billion since 2022.
------
Across all categories, we saw increased generalisation performance compared to previous baselines, such as on RT-1 models.

We also evaluated RT-2 on a number of unseen objects and environments where it could successfully adapt to new situations:
------
the speed of light is such an annoying thing
------
was asked why my google product tweets seem more negative than average

so a few minutes ago I had to fill out several recaptchas in firefox. the same form in google chrome gives me zero recaptchas, even with no cookies/incognito

it's the small things like that. really tiresome
------
yann lecun watching people load in superHOT-8k-airoboros llama 2 fine tunes to simulate girlfriends on anonymous imageboards
------
my biggest fear is Francois Chollet blocking me, so every time I respond in disagreement I follow it up with a tweet that says something like "I love you"
------
Yeah he’s surprisingly based
------
Gn anons!

Another tuff day, but we keep on marching!
------
absolutely incredible exchange
------
.
------
The AI / atomic bombs analogies are dumb as hell.
------
New post today - remember something like 1% of people create 90% of what we see, hear and view in the world. Showing up is underrated
------
When companies create this microservices bog and then, when any problem comes up, they say, “distributed systems are hard” it reminds me of when my toddler throws food on the floor then says, “look, big mess”
------
one twitter space later after asking anons advice re hyperparameters. Getting good results with qlora training w/ LIMA on a 7b llama model :DD

One 3090 is all you need

Again, thank you @Dogesator & 
@theemozilla
------
shout out to @Dogesator & 
@theemozilla
 for the twitter space today. I learned a lot!

If I had to boil it down to a single tweet:
- Get your train-eval loop down
- Mess with the hyperparameters
- There aint nothing to it, but to do it
------
Your success is directly related to how quickly you can experiment & validate
You need to actually try it
Focus on getting the time down
------
riffing on missing ingredients for symbolic agi
the goal here is to build one single dependency graph of knowledge & beliefs -- as opposed to having many separate belief graphs that either duplicate the same abstractions in incompatible ways, or ignore their dependencies entirely
------
By the way, our demo and code are both released!

Demo: http://hf.co/spaces/sail/lorahub…
Code:
------
Today we're releasing the Open Catalyst Demo to the public — this new service will allow researchers to accelerate work in material sciences by enabling them to simulate the reactivity of catalyst materials ~1000x faster than existing computational methods using AI.

Demo 
------
@Dogesator and everyone, for a good description of dropout, I would recommend this paragraph https://smerity.com/arxiv.org/abs/1804.404/1804.404.pdf…
------
Congrats Pangu-coder2, which leverages our Evol-Instruct as WizardLM & WizardCoder to construct their training corpus, and achieves 62.20% pass@1 on HumanEval. 

Evol-Instruct has become one of the most necessary methods for SFT.

Hope they can open-source the model soon.
------
In case you didn't know, the increase of polarization in the US House of Representatives started long before social networks or the internet existed.
------
Mitosis of Congress
------
Pretty funny how financial types don't understand the concept of long-term R&D investment.
To them, R&D investment is "lost money".
It only becomes "lost" if/when you give up on the whole product concept.
But that has not happened.
------
Mark Zuckerberg's Metaverse has lost over $21.3 billion since 2022.
------
Across all categories, we saw increased generalisation performance compared to previous baselines, such as on RT-1 models.

We also evaluated RT-2 on a number of unseen objects and environments where it could successfully adapt to new situations:
------
the speed of light is such an annoying thing
------
was asked why my google product tweets seem more negative than average

so a few minutes ago I had to fill out several recaptchas in firefox. the same form in google chrome gives me zero recaptchas, even with no cookies/incognito

it's the small things like that. really tiresome
------
yann lecun watching people load in superHOT-8k-airoboros llama 2 fine tunes to simulate girlfriends on anonymous imageboards
------
my biggest fear is Francois Chollet blocking me, so every time I respond in disagreement I follow it up with a tweet that says something like "I love you"
------
Yeah he’s surprisingly based
------
Gn anons!

Another tuff day, but we keep on marching!
------
absolutely incredible exchange
------
.
------
The AI / atomic bombs analogies are dumb as hell.
------
New post today - remember something like 1% of people create 90% of what we see, hear and view in the world. Showing up is underrated
------
When companies create this microservices bog and then, when any problem comes up, they say, “distributed systems are hard” it reminds me of when my toddler throws food on the floor then says, “look, big mess”
------
one twitter space later after asking anons advice re hyperparameters. Getting good results with qlora training w/ LIMA on a 7b llama model :DD

One 3090 is all you need

Again, thank you @Dogesator & 
@theemozilla
------
shout out to @Dogesator & 
@theemozilla
 for the twitter space today. I learned a lot!

If I had to boil it down to a single tweet:
- Get your train-eval loop down
- Mess with the hyperparameters
- There aint nothing to it, but to do it
------
Your success is directly related to how quickly you can experiment & validate
You need to actually try it
Focus on getting the time down
------
riffing on missing ingredients for symbolic agi
the goal here is to build one single dependency graph of knowledge & beliefs -- as opposed to having many separate belief graphs that either duplicate the same abstractions in incompatible ways, or ignore their dependencies entirely
------
By the way, our demo and code are both released!

Demo: http://hf.co/spaces/sail/lorahub…
Code:
------
Today we're releasing the Open Catalyst Demo to the public — this new service will allow researchers to accelerate work in material sciences by enabling them to simulate the reactivity of catalyst materials ~1000x faster than existing computational methods using AI.

Demo 
------
@Dogesator and everyone, for a good description of dropout, I would recommend this paragraph https://smerity.com/arxiv.org/abs/1804.404/1804.404.pdf…
------
This is a nice place to work:>
One day I'll be able to do this orbiting the moon of the real exoplanet hehe. One step at a time, we're building an every more awesome cosmos
------
gonna give my kids Linux without a display manager and tell them "good luck", and hack all the drivers and networks to have packet loss
------
No worse habit in the world.

I think there is a direct correlation with being overweight, lonely, single, broke and depressed.

My kids will not own video game systems while in my home.
------
chatbots are incidental. acing the IMO doesn’t matter. the only thing that matters is creating the god machine that solves the rest of science. whoever summons this thing will become the most wealthy and powerful people to ever exist, unless they deliberately choose not to
------
Tracking Anything in High Quality

repo: https://github.com/jiawen-zhu/HQTrack…
abs: https://arxiv.org/abs/2307.13974
------
whatever happens
------
Across all categories, we saw increased generalisation performance compared to previous baselines, such as on RT-1 models.

We also evaluated RT-2 on a number of unseen objects and environments where it could successfully adapt to new situations:
------
the speed of light is such an annoying thing
------
was asked why my google product tweets seem more negative than average

so a few minutes ago I had to fill out several recaptchas in firefox. the same form in google chrome gives me zero recaptchas, even with no cookies/incognito

it's the small things like that. really tiresome
------
yann lecun watching people load in superHOT-8k-airoboros llama 2 fine tunes to simulate girlfriends on anonymous imageboards
------
my biggest fear is Francois Chollet blocking me, so every time I respond in disagreement I follow it up with a tweet that says something like "I love you"
------
Yeah he’s surprisingly based
------
Gn anons!

Another tuff day, but we keep on marching!
------
absolutely incredible exchange
------
.
------
The AI / atomic bombs analogies are dumb as hell.
------
New post today - remember something like 1% of people create 90% of what we see, hear and view in the world. Showing up is underrated
------
When companies create this microservices bog and then, when any problem comes up, they say, “distributed systems are hard” it reminds me of when my toddler throws food on the floor then says, “look, big mess”
------
one twitter space later after asking anons advice re hyperparameters. Getting good results with qlora training w/ LIMA on a 7b llama model :DD

One 3090 is all you need

Again, thank you @Dogesator & 
@theemozilla
------
shout out to @Dogesator & 
@theemozilla
 for the twitter space today. I learned a lot!

If I had to boil it down to a single tweet:
- Get your train-eval loop down
- Mess with the hyperparameters
- There aint nothing to it, but to do it
------
Your success is directly related to how quickly you can experiment & validate
You need to actually try it
Focus on getting the time down
------
riffing on missing ingredients for symbolic agi
the goal here is to build one single dependency graph of knowledge & beliefs -- as opposed to having many separate belief graphs that either duplicate the same abstractions in incompatible ways, or ignore their dependencies entirely
------
By the way, our demo and code are both released!

Demo: http://hf.co/spaces/sail/lorahub…
Code:
------
Today we're releasing the Open Catalyst Demo to the public — this new service will allow researchers to accelerate work in material sciences by enabling them to simulate the reactivity of catalyst materials ~1000x faster than existing computational methods using AI.

Demo 
------
@Dogesator and everyone, for a good description of dropout, I would recommend this paragraph https://smerity.com/arxiv.org/abs/1804.404/1804.404.pdf…
------
This is a nice place to work:>
One day I'll be able to do this orbiting the moon of the real exoplanet hehe. One step at a time, we're building an every more awesome cosmos
------
gonna give my kids Linux without a display manager and tell them "good luck", and hack all the drivers and networks to have packet loss
------
No worse habit in the world.

I think there is a direct correlation with being overweight, lonely, single, broke and depressed.

My kids will not own video game systems while in my home.
------
chatbots are incidental. acing the IMO doesn’t matter. the only thing that matters is creating the god machine that solves the rest of science. whoever summons this thing will become the most wealthy and powerful people to ever exist, unless they deliberately choose not to
------
Tracking Anything in High Quality

repo: https://github.com/jiawen-zhu/HQTrack…
abs: https://arxiv.org/abs/2307.13974
------
whatever happens
------
happens
------
mfw mfs are talking about making money when we are on the cusp of solving literally everything
------
i actually have followers moment
this is not financial advice. this is a shitpost. I am JOKING
------
Gn anons!

Another tuff day, but we keep on marching!
------
absolutely incredible exchange
------
.
------
The AI / atomic bombs analogies are dumb as hell.
------
New post today - remember something like 1% of people create 90% of what we see, hear and view in the world. Showing up is underrated
------
When companies create this microservices bog and then, when any problem comes up, they say, “distributed systems are hard” it reminds me of when my toddler throws food on the floor then says, “look, big mess”
------
one twitter space later after asking anons advice re hyperparameters. Getting good results with qlora training w/ LIMA on a 7b llama model :DD

One 3090 is all you need

Again, thank you @Dogesator & 
@theemozilla
------
shout out to @Dogesator & 
@theemozilla
 for the twitter space today. I learned a lot!

If I had to boil it down to a single tweet:
- Get your train-eval loop down
- Mess with the hyperparameters
- There aint nothing to it, but to do it
------
Your success is directly related to how quickly you can experiment & validate
You need to actually try it
Focus on getting the time down
------
riffing on missing ingredients for symbolic agi
the goal here is to build one single dependency graph of knowledge & beliefs -- as opposed to having many separate belief graphs that either duplicate the same abstractions in incompatible ways, or ignore their dependencies entirely
------
By the way, our demo and code are both released!

Demo: http://hf.co/spaces/sail/lorahub…
Code:
------
Today we're releasing the Open Catalyst Demo to the public — this new service will allow researchers to accelerate work in material sciences by enabling them to simulate the reactivity of catalyst materials ~1000x faster than existing computational methods using AI.

Demo 
------
@Dogesator and everyone, for a good description of dropout, I would recommend this paragraph https://smerity.com/arxiv.org/abs/1804.404/1804.404.pdf…
------
This is a nice place to work:>
One day I'll be able to do this orbiting the moon of the real exoplanet hehe. One step at a time, we're building an every more awesome cosmos
------
gonna give my kids Linux without a display manager and tell them "good luck", and hack all the drivers and networks to have packet loss
------
No worse habit in the world.

I think there is a direct correlation with being overweight, lonely, single, broke and depressed.

My kids will not own video game systems while in my home.
------
chatbots are incidental. acing the IMO doesn’t matter. the only thing that matters is creating the god machine that solves the rest of science. whoever summons this thing will become the most wealthy and powerful people to ever exist, unless they deliberately choose not to
------
Tracking Anything in High Quality

repo: https://github.com/jiawen-zhu/HQTrack…
abs: https://arxiv.org/abs/2307.13974
------
whatever happens
------
happens
------
mfw mfs are talking about making money when we are on the cusp of solving literally everything
------
i actually have followers moment
this is not financial advice. this is a shitpost. I am JOKING
------
>Prompting Large Language Models with Speech Recognition Abilities
https://arxiv.org/abs/2307.11795
this is fun. Can LLMs listen? (yes). i guess if it works for images, it should work for audio too. WER isn't the best but, pretty cool
------
what i mean when I say, it works on images
palm-e
https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html…
------
What Is To Be Done?

I propose a simple plan:

Big AI companies should be allowed to build AI as fast and aggressively as they can – but not allowed to achieve regulatory capture, not allowed to establish a government-protect cartel that is insulated from market competition due… Show more
------
The most important lesson about programming I've learnt recently: keep everything shallow.

Data structures should be shallow.
Module hierarchies should be shallow.
Pipelines should be shallow.

Top-to-bottom, end-to-end should be four steps or less.
------
today I cried during the Oppenheimer movie. multiple tears
------
I am become tech bro
predictable of behaviors
------
Superconducting magnet engineer chiming in. 

This result could be very big news, and overnight revolutionize all of electronics and energy. It might not.

Here's a mental model for the non-expert to understand what's going on.

RTAPS: The good, the bad, and the ugly: 
------
Every time I read about AI regulation, it feels like the goal is to attempt to regulate something that doesn't even exist, and no one knows what it will be if/when it does.
------
Llama 2 presents a real shift in LLMs. If you ever wanted to learn about LLMs, now is the time. Early enough to really get ahead by putting these models to use. Most are probably not even exploring the 70B model, larger models are better with reasoning and instruction following
------
New post today - remember something like 1% of people create 90% of what we see, hear and view in the world. Showing up is underrated
------
When companies create this microservices bog and then, when any problem comes up, they say, “distributed systems are hard” it reminds me of when my toddler throws food on the floor then says, “look, big mess”
------
one twitter space later after asking anons advice re hyperparameters. Getting good results with qlora training w/ LIMA on a 7b llama model :DD

One 3090 is all you need

Again, thank you @Dogesator & 
@theemozilla
------
shout out to @Dogesator & 
@theemozilla
 for the twitter space today. I learned a lot!

If I had to boil it down to a single tweet:
- Get your train-eval loop down
- Mess with the hyperparameters
- There aint nothing to it, but to do it
------
Your success is directly related to how quickly you can experiment & validate
You need to actually try it
Focus on getting the time down
------
riffing on missing ingredients for symbolic agi
the goal here is to build one single dependency graph of knowledge & beliefs -- as opposed to having many separate belief graphs that either duplicate the same abstractions in incompatible ways, or ignore their dependencies entirely
------
By the way, our demo and code are both released!

Demo: http://hf.co/spaces/sail/lorahub…
Code:
------
Today we're releasing the Open Catalyst Demo to the public — this new service will allow researchers to accelerate work in material sciences by enabling them to simulate the reactivity of catalyst materials ~1000x faster than existing computational methods using AI.

Demo 
------
@Dogesator and everyone, for a good description of dropout, I would recommend this paragraph https://smerity.com/arxiv.org/abs/1804.404/1804.404.pdf…
------
This is a nice place to work:>
One day I'll be able to do this orbiting the moon of the real exoplanet hehe. One step at a time, we're building an every more awesome cosmos
------
gonna give my kids Linux without a display manager and tell them "good luck", and hack all the drivers and networks to have packet loss
------
No worse habit in the world.

I think there is a direct correlation with being overweight, lonely, single, broke and depressed.

My kids will not own video game systems while in my home.
------
chatbots are incidental. acing the IMO doesn’t matter. the only thing that matters is creating the god machine that solves the rest of science. whoever summons this thing will become the most wealthy and powerful people to ever exist, unless they deliberately choose not to
------
Tracking Anything in High Quality

repo: https://github.com/jiawen-zhu/HQTrack…
abs: https://arxiv.org/abs/2307.13974
------
whatever happens
------
happens
------
mfw mfs are talking about making money when we are on the cusp of solving literally everything
------
i actually have followers moment
this is not financial advice. this is a shitpost. I am JOKING
------
>Prompting Large Language Models with Speech Recognition Abilities
https://arxiv.org/abs/2307.11795
this is fun. Can LLMs listen? (yes). i guess if it works for images, it should work for audio too. WER isn't the best but, pretty cool
------
what i mean when I say, it works on images
palm-e
https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html…
------
What Is To Be Done?

I propose a simple plan:

Big AI companies should be allowed to build AI as fast and aggressively as they can – but not allowed to achieve regulatory capture, not allowed to establish a government-protect cartel that is insulated from market competition due… Show more
------
The most important lesson about programming I've learnt recently: keep everything shallow.

Data structures should be shallow.
Module hierarchies should be shallow.
Pipelines should be shallow.

Top-to-bottom, end-to-end should be four steps or less.
------
today I cried during the Oppenheimer movie. multiple tears
------
I am become tech bro
predictable of behaviors
------
Superconducting magnet engineer chiming in. 

This result could be very big news, and overnight revolutionize all of electronics and energy. It might not.

Here's a mental model for the non-expert to understand what's going on.

RTAPS: The good, the bad, and the ugly: 
------
Every time I read about AI regulation, it feels like the goal is to attempt to regulate something that doesn't even exist, and no one knows what it will be if/when it does.
------
Llama 2 presents a real shift in LLMs. If you ever wanted to learn about LLMs, now is the time. Early enough to really get ahead by putting these models to use. Most are probably not even exploring the 70B model, larger models are better with reasoning and instruction following
------
while waiting on a training run; read the code that you're running
I'm trying to figure out when 4bit happens

bnb config gets passed down to AutoModelForCauslLm.from_pretrained
I tried to follow the path, but impossible w/ HF oop

greppin around, i see
https://github.com/huggingface/transformers/blob/1689aea73346816b936b84932e12b774974e61a6/src/transformers/utils/bitsandbytes.py#L185-L223…
------
This does the transformation
it recurses down the torch NN modules
this costs compute to do
so i want to know when, exactly, it happens
I want to understand when it happens (or, when huggingface's abstractions call it)
------
that gets called from huggingface's from_pretrained function, which is about 1k lines long, but not the good kind of 1k long fn. the kind that branches. a lot. based on config :D
(oop sux)
------
By the way, our demo and code are both released!

Demo: http://hf.co/spaces/sail/lorahub…
Code:
------
Today we're releasing the Open Catalyst Demo to the public — this new service will allow researchers to accelerate work in material sciences by enabling them to simulate the reactivity of catalyst materials ~1000x faster than existing computational methods using AI.

Demo 
------
@Dogesator and everyone, for a good description of dropout, I would recommend this paragraph https://smerity.com/arxiv.org/abs/1804.404/1804.404.pdf…
------
This is a nice place to work:>
One day I'll be able to do this orbiting the moon of the real exoplanet hehe. One step at a time, we're building an every more awesome cosmos
------
gonna give my kids Linux without a display manager and tell them "good luck", and hack all the drivers and networks to have packet loss
------
No worse habit in the world.

I think there is a direct correlation with being overweight, lonely, single, broke and depressed.

My kids will not own video game systems while in my home.
------
chatbots are incidental. acing the IMO doesn’t matter. the only thing that matters is creating the god machine that solves the rest of science. whoever summons this thing will become the most wealthy and powerful people to ever exist, unless they deliberately choose not to
------
Tracking Anything in High Quality

repo: https://github.com/jiawen-zhu/HQTrack…
abs: https://arxiv.org/abs/2307.13974
------
whatever happens
------
happens
------
mfw mfs are talking about making money when we are on the cusp of solving literally everything
------
i actually have followers moment
this is not financial advice. this is a shitpost. I am JOKING
------
>Prompting Large Language Models with Speech Recognition Abilities
https://arxiv.org/abs/2307.11795
this is fun. Can LLMs listen? (yes). i guess if it works for images, it should work for audio too. WER isn't the best but, pretty cool
------
what i mean when I say, it works on images
palm-e
https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html…
------
What Is To Be Done?

I propose a simple plan:

Big AI companies should be allowed to build AI as fast and aggressively as they can – but not allowed to achieve regulatory capture, not allowed to establish a government-protect cartel that is insulated from market competition due… Show more
------
The most important lesson about programming I've learnt recently: keep everything shallow.

Data structures should be shallow.
Module hierarchies should be shallow.
Pipelines should be shallow.

Top-to-bottom, end-to-end should be four steps or less.
------
today I cried during the Oppenheimer movie. multiple tears
------
I am become tech bro
predictable of behaviors
------
Superconducting magnet engineer chiming in. 

This result could be very big news, and overnight revolutionize all of electronics and energy. It might not.

Here's a mental model for the non-expert to understand what's going on.

RTAPS: The good, the bad, and the ugly: 
------
Every time I read about AI regulation, it feels like the goal is to attempt to regulate something that doesn't even exist, and no one knows what it will be if/when it does.
------
Llama 2 presents a real shift in LLMs. If you ever wanted to learn about LLMs, now is the time. Early enough to really get ahead by putting these models to use. Most are probably not even exploring the 70B model, larger models are better with reasoning and instruction following
------
while waiting on a training run; read the code that you're running
I'm trying to figure out when 4bit happens

bnb config gets passed down to AutoModelForCauslLm.from_pretrained
I tried to follow the path, but impossible w/ HF oop

greppin around, i see
https://github.com/huggingface/transformers/blob/1689aea73346816b936b84932e12b774974e61a6/src/transformers/utils/bitsandbytes.py#L185-L223…
------
This does the transformation
it recurses down the torch NN modules
this costs compute to do
so i want to know when, exactly, it happens
I want to understand when it happens (or, when huggingface's abstractions call it)
------
that gets called from huggingface's from_pretrained function, which is about 1k lines long, but not the good kind of 1k long fn. the kind that branches. a lot. based on config :D
(oop sux)
------
Datasets uploaded: airoboros-gpt4-2.0 and m-2.0 on 

15000 new instructions.

2.0 is all new using only June gpt-4.

m-2.0 is 2.0 with 1.4.1 merged in.

https://huggingface.co/datasets/jondurbin/airoboros-gpt4-2.0…
https://huggingface.co/datasets/jondurbin/airoboros-gpt4-m2.0…
------
i don't want to play with man's abstractions, the ones he created in hubris. i want to play with god's primitives
------
I want to know what is happening
On my computer
Stop disrespecting me with your abstractions
------
nerds will beat the market by an astounding margin not for money, but for bragging rights
------
LoraHub - a paper with an astounding result
- They compose a random set of fine tuned LoRA adapters to merge in
- They then "adapt" the weights of the individual adapters based on the few shot examples

Finding: its close to the performance of in context learning!
------
Why this matters: 
- searching for the weight of each adapter can be done very cheaply, on CPU
- you save all of the tokens that you would need for in context learning

Skepticism: 
This might not generalize. See the fine tuned models https://huggingface.co/models?search=lorahub… & relate to task
------
This is a nice place to work:>
One day I'll be able to do this orbiting the moon of the real exoplanet hehe. One step at a time, we're building an every more awesome cosmos
------
gonna give my kids Linux without a display manager and tell them "good luck", and hack all the drivers and networks to have packet loss
------
No worse habit in the world.

I think there is a direct correlation with being overweight, lonely, single, broke and depressed.

My kids will not own video game systems while in my home.
------
chatbots are incidental. acing the IMO doesn’t matter. the only thing that matters is creating the god machine that solves the rest of science. whoever summons this thing will become the most wealthy and powerful people to ever exist, unless they deliberately choose not to
------
Tracking Anything in High Quality

repo: https://github.com/jiawen-zhu/HQTrack…
abs: https://arxiv.org/abs/2307.13974
------
whatever happens
------
happens
------
mfw mfs are talking about making money when we are on the cusp of solving literally everything
------
i actually have followers moment
this is not financial advice. this is a shitpost. I am JOKING
------
>Prompting Large Language Models with Speech Recognition Abilities
https://arxiv.org/abs/2307.11795
this is fun. Can LLMs listen? (yes). i guess if it works for images, it should work for audio too. WER isn't the best but, pretty cool
------
what i mean when I say, it works on images
palm-e
https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html…
------
What Is To Be Done?

I propose a simple plan:

Big AI companies should be allowed to build AI as fast and aggressively as they can – but not allowed to achieve regulatory capture, not allowed to establish a government-protect cartel that is insulated from market competition due… Show more
------
The most important lesson about programming I've learnt recently: keep everything shallow.

Data structures should be shallow.
Module hierarchies should be shallow.
Pipelines should be shallow.

Top-to-bottom, end-to-end should be four steps or less.
------
today I cried during the Oppenheimer movie. multiple tears
------
I am become tech bro
predictable of behaviors
------
Superconducting magnet engineer chiming in. 

This result could be very big news, and overnight revolutionize all of electronics and energy. It might not.

Here's a mental model for the non-expert to understand what's going on.

RTAPS: The good, the bad, and the ugly: 
------
Every time I read about AI regulation, it feels like the goal is to attempt to regulate something that doesn't even exist, and no one knows what it will be if/when it does.
------
Llama 2 presents a real shift in LLMs. If you ever wanted to learn about LLMs, now is the time. Early enough to really get ahead by putting these models to use. Most are probably not even exploring the 70B model, larger models are better with reasoning and instruction following
------
while waiting on a training run; read the code that you're running
I'm trying to figure out when 4bit happens

bnb config gets passed down to AutoModelForCauslLm.from_pretrained
I tried to follow the path, but impossible w/ HF oop

greppin around, i see
https://github.com/huggingface/transformers/blob/1689aea73346816b936b84932e12b774974e61a6/src/transformers/utils/bitsandbytes.py#L185-L223…
------
This does the transformation
it recurses down the torch NN modules
this costs compute to do
so i want to know when, exactly, it happens
I want to understand when it happens (or, when huggingface's abstractions call it)
------
that gets called from huggingface's from_pretrained function, which is about 1k lines long, but not the good kind of 1k long fn. the kind that branches. a lot. based on config :D
(oop sux)
------
Datasets uploaded: airoboros-gpt4-2.0 and m-2.0 on 

15000 new instructions.

2.0 is all new using only June gpt-4.

m-2.0 is 2.0 with 1.4.1 merged in.

https://huggingface.co/datasets/jondurbin/airoboros-gpt4-2.0…
https://huggingface.co/datasets/jondurbin/airoboros-gpt4-m2.0…
------
i don't want to play with man's abstractions, the ones he created in hubris. i want to play with god's primitives
------
I want to know what is happening
On my computer
Stop disrespecting me with your abstractions
------
nerds will beat the market by an astounding margin not for money, but for bragging rights
------
LoraHub - a paper with an astounding result
- They compose a random set of fine tuned LoRA adapters to merge in
- They then "adapt" the weights of the individual adapters based on the few shot examples

Finding: its close to the performance of in context learning!
------
Why this matters: 
- searching for the weight of each adapter can be done very cheaply, on CPU
- you save all of the tokens that you would need for in context learning

Skepticism: 
This might not generalize. See the fine tuned models https://huggingface.co/models?search=lorahub… & relate to task
------
I'm really bullish on LoRAs, and parameter efficient fine tuning in general, being a pathway to online learning for some constrained systems

(My inference server swaps LoRAs on my entropic bag of beans based on endpoint)

If I made a mistake, please correct me
------
happens
------
mfw mfs are talking about making money when we are on the cusp of solving literally everything
------
i actually have followers moment
this is not financial advice. this is a shitpost. I am JOKING
------
>Prompting Large Language Models with Speech Recognition Abilities
https://arxiv.org/abs/2307.11795
this is fun. Can LLMs listen? (yes). i guess if it works for images, it should work for audio too. WER isn't the best but, pretty cool
------
what i mean when I say, it works on images
palm-e
https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html…
------
What Is To Be Done?

I propose a simple plan:

Big AI companies should be allowed to build AI as fast and aggressively as they can – but not allowed to achieve regulatory capture, not allowed to establish a government-protect cartel that is insulated from market competition due… Show more
------
The most important lesson about programming I've learnt recently: keep everything shallow.

Data structures should be shallow.
Module hierarchies should be shallow.
Pipelines should be shallow.

Top-to-bottom, end-to-end should be four steps or less.
------
today I cried during the Oppenheimer movie. multiple tears
------
I am become tech bro
predictable of behaviors
------
Superconducting magnet engineer chiming in. 

This result could be very big news, and overnight revolutionize all of electronics and energy. It might not.

Here's a mental model for the non-expert to understand what's going on.

RTAPS: The good, the bad, and the ugly: 
------
Every time I read about AI regulation, it feels like the goal is to attempt to regulate something that doesn't even exist, and no one knows what it will be if/when it does.
------
Llama 2 presents a real shift in LLMs. If you ever wanted to learn about LLMs, now is the time. Early enough to really get ahead by putting these models to use. Most are probably not even exploring the 70B model, larger models are better with reasoning and instruction following
------
while waiting on a training run; read the code that you're running
I'm trying to figure out when 4bit happens

bnb config gets passed down to AutoModelForCauslLm.from_pretrained
I tried to follow the path, but impossible w/ HF oop

greppin around, i see
https://github.com/huggingface/transformers/blob/1689aea73346816b936b84932e12b774974e61a6/src/transformers/utils/bitsandbytes.py#L185-L223…
------
This does the transformation
it recurses down the torch NN modules
this costs compute to do
so i want to know when, exactly, it happens
I want to understand when it happens (or, when huggingface's abstractions call it)
------
that gets called from huggingface's from_pretrained function, which is about 1k lines long, but not the good kind of 1k long fn. the kind that branches. a lot. based on config :D
(oop sux)
------
Datasets uploaded: airoboros-gpt4-2.0 and m-2.0 on 

15000 new instructions.

2.0 is all new using only June gpt-4.

m-2.0 is 2.0 with 1.4.1 merged in.

https://huggingface.co/datasets/jondurbin/airoboros-gpt4-2.0…
https://huggingface.co/datasets/jondurbin/airoboros-gpt4-m2.0…
------
i don't want to play with man's abstractions, the ones he created in hubris. i want to play with god's primitives
------
I want to know what is happening
On my computer
Stop disrespecting me with your abstractions
------
nerds will beat the market by an astounding margin not for money, but for bragging rights
------
LoraHub - a paper with an astounding result
- They compose a random set of fine tuned LoRA adapters to merge in
- They then "adapt" the weights of the individual adapters based on the few shot examples

Finding: its close to the performance of in context learning!
------
Why this matters: 
- searching for the weight of each adapter can be done very cheaply, on CPU
- you save all of the tokens that you would need for in context learning

Skepticism: 
This might not generalize. See the fine tuned models https://huggingface.co/models?search=lorahub… & relate to task
------
I'm really bullish on LoRAs, and parameter efficient fine tuning in general, being a pathway to online learning for some constrained systems

(My inference server swaps LoRAs on my entropic bag of beans based on endpoint)

If I made a mistake, please correct me
------
Cashing in my receipt
------
yeah
just tradeoffs across different dimensions
e.g. i am trying to bootstrap so my costs are
sample efficiency
the ability to use less compute with different models (lora can swap)
------
haha that's really easy, all you have to do is swap LoRA 
https://github.com/yacineMTB/llama.cpp/pull/3…
------
FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios

repo: https://github.com/GAIR-NLP/factool…
abs: https://arxiv.org/abs/2307.13528
------
You can tell crazy-sounding physics claims are real when the experiments replicate. Much like you can tell AI claims are real when the model makes it into products and drives revenue.
------
makes me happy that some of the best guides for running LLMs locally originate from 4chan users who want to make their waifus real
------
good morning
the future rocks
get to work
------
now I understand people's skepticism about the inevitability of AGI (and the fact that GPT4 is basically AGI)

technology sometimes feels too good to be true
you want to temper your expectations, lest you get heartbroken
------
Innocence is the child, and forgetfulness, and a new beginning, a game, a self-rolling wheel, a first movement

a holy "yes"
------
what i mean when I say, it works on images
palm-e
https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html…
------
What Is To Be Done?

I propose a simple plan:

Big AI companies should be allowed to build AI as fast and aggressively as they can – but not allowed to achieve regulatory capture, not allowed to establish a government-protect cartel that is insulated from market competition due… Show more
------
The most important lesson about programming I've learnt recently: keep everything shallow.

Data structures should be shallow.
Module hierarchies should be shallow.
Pipelines should be shallow.

Top-to-bottom, end-to-end should be four steps or less.
------
today I cried during the Oppenheimer movie. multiple tears
------
I am become tech bro
predictable of behaviors
------
Superconducting magnet engineer chiming in. 

This result could be very big news, and overnight revolutionize all of electronics and energy. It might not.

Here's a mental model for the non-expert to understand what's going on.

RTAPS: The good, the bad, and the ugly: 
------
Every time I read about AI regulation, it feels like the goal is to attempt to regulate something that doesn't even exist, and no one knows what it will be if/when it does.
------
Llama 2 presents a real shift in LLMs. If you ever wanted to learn about LLMs, now is the time. Early enough to really get ahead by putting these models to use. Most are probably not even exploring the 70B model, larger models are better with reasoning and instruction following
------
while waiting on a training run; read the code that you're running
I'm trying to figure out when 4bit happens

bnb config gets passed down to AutoModelForCauslLm.from_pretrained
I tried to follow the path, but impossible w/ HF oop

greppin around, i see
https://github.com/huggingface/transformers/blob/1689aea73346816b936b84932e12b774974e61a6/src/transformers/utils/bitsandbytes.py#L185-L223…
------
This does the transformation
it recurses down the torch NN modules
this costs compute to do
so i want to know when, exactly, it happens
I want to understand when it happens (or, when huggingface's abstractions call it)
------
that gets called from huggingface's from_pretrained function, which is about 1k lines long, but not the good kind of 1k long fn. the kind that branches. a lot. based on config :D
(oop sux)
------
Datasets uploaded: airoboros-gpt4-2.0 and m-2.0 on 

15000 new instructions.

2.0 is all new using only June gpt-4.

m-2.0 is 2.0 with 1.4.1 merged in.

https://huggingface.co/datasets/jondurbin/airoboros-gpt4-2.0…
https://huggingface.co/datasets/jondurbin/airoboros-gpt4-m2.0…
------
i don't want to play with man's abstractions, the ones he created in hubris. i want to play with god's primitives
------
I want to know what is happening
On my computer
Stop disrespecting me with your abstractions
------
nerds will beat the market by an astounding margin not for money, but for bragging rights
------
LoraHub - a paper with an astounding result
- They compose a random set of fine tuned LoRA adapters to merge in
- They then "adapt" the weights of the individual adapters based on the few shot examples

Finding: its close to the performance of in context learning!
------
Why this matters: 
- searching for the weight of each adapter can be done very cheaply, on CPU
- you save all of the tokens that you would need for in context learning

Skepticism: 
This might not generalize. See the fine tuned models https://huggingface.co/models?search=lorahub… & relate to task
------
I'm really bullish on LoRAs, and parameter efficient fine tuning in general, being a pathway to online learning for some constrained systems

(My inference server swaps LoRAs on my entropic bag of beans based on endpoint)

If I made a mistake, please correct me
------
Cashing in my receipt
------
yeah
just tradeoffs across different dimensions
e.g. i am trying to bootstrap so my costs are
sample efficiency
the ability to use less compute with different models (lora can swap)
------
haha that's really easy, all you have to do is swap LoRA 
https://github.com/yacineMTB/llama.cpp/pull/3…
------
FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios

repo: https://github.com/GAIR-NLP/factool…
abs: https://arxiv.org/abs/2307.13528
------
You can tell crazy-sounding physics claims are real when the experiments replicate. Much like you can tell AI claims are real when the model makes it into products and drives revenue.
------
makes me happy that some of the best guides for running LLMs locally originate from 4chan users who want to make their waifus real
------
good morning
the future rocks
get to work
------
now I understand people's skepticism about the inevitability of AGI (and the fact that GPT4 is basically AGI)

technology sometimes feels too good to be true
you want to temper your expectations, lest you get heartbroken
------
Innocence is the child, and forgetfulness, and a new beginning, a game, a self-rolling wheel, a first movement

a holy "yes"
------
pictured: a man who knows how little he knows
------
i don't think people understand how much M bucks are worth to nerds
in about 3 years, I will be at a party where the biggest nerd legend will ask to see my score
and based on that, he will either respect me or not
------
this is a good app
------
When you ask backend developer to make frontend
------
while waiting on a training run; read the code that you're running
I'm trying to figure out when 4bit happens

bnb config gets passed down to AutoModelForCauslLm.from_pretrained
I tried to follow the path, but impossible w/ HF oop

greppin around, i see
https://github.com/huggingface/transformers/blob/1689aea73346816b936b84932e12b774974e61a6/src/transformers/utils/bitsandbytes.py#L185-L223…
------
This does the transformation
it recurses down the torch NN modules
this costs compute to do
so i want to know when, exactly, it happens
I want to understand when it happens (or, when huggingface's abstractions call it)
------
that gets called from huggingface's from_pretrained function, which is about 1k lines long, but not the good kind of 1k long fn. the kind that branches. a lot. based on config :D
(oop sux)
------
Datasets uploaded: airoboros-gpt4-2.0 and m-2.0 on 

15000 new instructions.

2.0 is all new using only June gpt-4.

m-2.0 is 2.0 with 1.4.1 merged in.

https://huggingface.co/datasets/jondurbin/airoboros-gpt4-2.0…
https://huggingface.co/datasets/jondurbin/airoboros-gpt4-m2.0…
------
i don't want to play with man's abstractions, the ones he created in hubris. i want to play with god's primitives
------
I want to know what is happening
On my computer
Stop disrespecting me with your abstractions
------
nerds will beat the market by an astounding margin not for money, but for bragging rights
------
LoraHub - a paper with an astounding result
- They compose a random set of fine tuned LoRA adapters to merge in
- They then "adapt" the weights of the individual adapters based on the few shot examples

Finding: its close to the performance of in context learning!
------
Why this matters: 
- searching for the weight of each adapter can be done very cheaply, on CPU
- you save all of the tokens that you would need for in context learning

Skepticism: 
This might not generalize. See the fine tuned models https://huggingface.co/models?search=lorahub… & relate to task
------
I'm really bullish on LoRAs, and parameter efficient fine tuning in general, being a pathway to online learning for some constrained systems

(My inference server swaps LoRAs on my entropic bag of beans based on endpoint)

If I made a mistake, please correct me
------
Cashing in my receipt
------
yeah
just tradeoffs across different dimensions
e.g. i am trying to bootstrap so my costs are
sample efficiency
the ability to use less compute with different models (lora can swap)
------
haha that's really easy, all you have to do is swap LoRA 
https://github.com/yacineMTB/llama.cpp/pull/3…
------
FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios

repo: https://github.com/GAIR-NLP/factool…
abs: https://arxiv.org/abs/2307.13528
------
You can tell crazy-sounding physics claims are real when the experiments replicate. Much like you can tell AI claims are real when the model makes it into products and drives revenue.
------
makes me happy that some of the best guides for running LLMs locally originate from 4chan users who want to make their waifus real
------
good morning
the future rocks
get to work
------
now I understand people's skepticism about the inevitability of AGI (and the fact that GPT4 is basically AGI)

technology sometimes feels too good to be true
you want to temper your expectations, lest you get heartbroken
------
Innocence is the child, and forgetfulness, and a new beginning, a game, a self-rolling wheel, a first movement

a holy "yes"
------
pictured: a man who knows how little he knows
------
i don't think people understand how much M bucks are worth to nerds
in about 3 years, I will be at a party where the biggest nerd legend will ask to see my score
and based on that, he will either respect me or not
------
this is a good app
------
When you ask backend developer to make frontend
------
also, this is kinda kino design & branding
jugaad branding
type of page to make nat friedman
------
I've worked and met a lot of talented people with 0 credentials

you used to actually need the credentials. of course, the credentials are useful as a filtering function. I'm hoping that we move to more "proof is in the pudding"
------
there is no correlation to academic distinguishment to applied CS skill

Measured by people asking me "hey, who has this very rare skillset that is hard to acquire", and my recommendation class being a toss between an MIT PhD or a eastern european highschool student
------
honestly i'm really glad that elon didn't buy 4chan
------
amusingly it would have been a much more capital efficient purchase
------
Datasets uploaded: airoboros-gpt4-2.0 and m-2.0 on 

15000 new instructions.

2.0 is all new using only June gpt-4.

m-2.0 is 2.0 with 1.4.1 merged in.

https://huggingface.co/datasets/jondurbin/airoboros-gpt4-2.0…
https://huggingface.co/datasets/jondurbin/airoboros-gpt4-m2.0…
------
i don't want to play with man's abstractions, the ones he created in hubris. i want to play with god's primitives
------
I want to know what is happening
On my computer
Stop disrespecting me with your abstractions
------
nerds will beat the market by an astounding margin not for money, but for bragging rights
------
LoraHub - a paper with an astounding result
- They compose a random set of fine tuned LoRA adapters to merge in
- They then "adapt" the weights of the individual adapters based on the few shot examples

Finding: its close to the performance of in context learning!
------
Why this matters: 
- searching for the weight of each adapter can be done very cheaply, on CPU
- you save all of the tokens that you would need for in context learning

Skepticism: 
This might not generalize. See the fine tuned models https://huggingface.co/models?search=lorahub… & relate to task
------
I'm really bullish on LoRAs, and parameter efficient fine tuning in general, being a pathway to online learning for some constrained systems

(My inference server swaps LoRAs on my entropic bag of beans based on endpoint)

If I made a mistake, please correct me
------
Cashing in my receipt
------
yeah
just tradeoffs across different dimensions
e.g. i am trying to bootstrap so my costs are
sample efficiency
the ability to use less compute with different models (lora can swap)
------
haha that's really easy, all you have to do is swap LoRA 
https://github.com/yacineMTB/llama.cpp/pull/3…
------
FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios

repo: https://github.com/GAIR-NLP/factool…
abs: https://arxiv.org/abs/2307.13528
------
You can tell crazy-sounding physics claims are real when the experiments replicate. Much like you can tell AI claims are real when the model makes it into products and drives revenue.
------
makes me happy that some of the best guides for running LLMs locally originate from 4chan users who want to make their waifus real
------
good morning
the future rocks
get to work
------
now I understand people's skepticism about the inevitability of AGI (and the fact that GPT4 is basically AGI)

technology sometimes feels too good to be true
you want to temper your expectations, lest you get heartbroken
------
Innocence is the child, and forgetfulness, and a new beginning, a game, a self-rolling wheel, a first movement

a holy "yes"
------
pictured: a man who knows how little he knows
------
i don't think people understand how much M bucks are worth to nerds
in about 3 years, I will be at a party where the biggest nerd legend will ask to see my score
and based on that, he will either respect me or not
------
this is a good app
------
When you ask backend developer to make frontend
------
also, this is kinda kino design & branding
jugaad branding
type of page to make nat friedman
------
I've worked and met a lot of talented people with 0 credentials

you used to actually need the credentials. of course, the credentials are useful as a filtering function. I'm hoping that we move to more "proof is in the pudding"
------
there is no correlation to academic distinguishment to applied CS skill

Measured by people asking me "hey, who has this very rare skillset that is hard to acquire", and my recommendation class being a toss between an MIT PhD or a eastern european highschool student
------
honestly i'm really glad that elon didn't buy 4chan
------
amusingly it would have been a much more capital efficient purchase
------
good morning
------
does anyone know if a standard home oven can get to 750c? Is there some limiter I can remove or something?
------
Has anyone reproduced? No one is going to reproduce, right?
------
This actually can't be real
------
We are releasing the Advanced Reasoning Benchmark dataset for LLMs (ARB)!

- Evaluates SotA LLMs on ARB, on which even GPT4 struggles
- Explores the feasibility of letting LLMs generate and use rubrics to evaluate generated solutions.

proj: https://arb.duckai.org
(1/N)
------
MY HYPERPRIORS ARE SIMPLY BETTER THAN YOURS. YOU DONT EVEN HAVE A HYPERPRIOR. YOUR PRIOR IS SIMPLY A DERIVATIVE OF THE HYPERPRIOR OF MY ETHOS. SIMPLE AS
------
Imagine being a leading quantum physics researcher, dedicating your life to the edges of reality. You have climbed the ranks to manage multiple labs. Then, some Koreans figure the thing out and you get 100 texts, at 8 am, with a poorly formatted arxiv link saying "cap or no cap?"
------
Journalism hahahahahahhaahahhaahhaaha ohhhh hahahahahah *breathes in* ahahahahahahhahaahahahahahah
------
This chart from Alex Karp’s op-ed in the NYT is insanely misleading. The y-axes are not comparable.
------
how many conference accepts does JohannesGaessler have, I wonder?
------
Is this the *minimum* requirement for a new grad in machine learning now? #NVIDIA
------
Why this matters: 
- searching for the weight of each adapter can be done very cheaply, on CPU
- you save all of the tokens that you would need for in context learning

Skepticism: 
This might not generalize. See the fine tuned models https://huggingface.co/models?search=lorahub… & relate to task
------
I'm really bullish on LoRAs, and parameter efficient fine tuning in general, being a pathway to online learning for some constrained systems

(My inference server swaps LoRAs on my entropic bag of beans based on endpoint)

If I made a mistake, please correct me
------
Cashing in my receipt
------
yeah
just tradeoffs across different dimensions
e.g. i am trying to bootstrap so my costs are
sample efficiency
the ability to use less compute with different models (lora can swap)
------
haha that's really easy, all you have to do is swap LoRA 
https://github.com/yacineMTB/llama.cpp/pull/3…
------
FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios

repo: https://github.com/GAIR-NLP/factool…
abs: https://arxiv.org/abs/2307.13528
------
You can tell crazy-sounding physics claims are real when the experiments replicate. Much like you can tell AI claims are real when the model makes it into products and drives revenue.
------
makes me happy that some of the best guides for running LLMs locally originate from 4chan users who want to make their waifus real
------
good morning
the future rocks
get to work
------
now I understand people's skepticism about the inevitability of AGI (and the fact that GPT4 is basically AGI)

technology sometimes feels too good to be true
you want to temper your expectations, lest you get heartbroken
------
Innocence is the child, and forgetfulness, and a new beginning, a game, a self-rolling wheel, a first movement

a holy "yes"
------
pictured: a man who knows how little he knows
------
i don't think people understand how much M bucks are worth to nerds
in about 3 years, I will be at a party where the biggest nerd legend will ask to see my score
and based on that, he will either respect me or not
------
this is a good app
------
When you ask backend developer to make frontend
------
also, this is kinda kino design & branding
jugaad branding
type of page to make nat friedman
------
I've worked and met a lot of talented people with 0 credentials

you used to actually need the credentials. of course, the credentials are useful as a filtering function. I'm hoping that we move to more "proof is in the pudding"
------
there is no correlation to academic distinguishment to applied CS skill

Measured by people asking me "hey, who has this very rare skillset that is hard to acquire", and my recommendation class being a toss between an MIT PhD or a eastern european highschool student
------
honestly i'm really glad that elon didn't buy 4chan
------
amusingly it would have been a much more capital efficient purchase
------
good morning
------
does anyone know if a standard home oven can get to 750c? Is there some limiter I can remove or something?
------
Has anyone reproduced? No one is going to reproduce, right?
------
This actually can't be real
------
We are releasing the Advanced Reasoning Benchmark dataset for LLMs (ARB)!

- Evaluates SotA LLMs on ARB, on which even GPT4 struggles
- Explores the feasibility of letting LLMs generate and use rubrics to evaluate generated solutions.

proj: https://arb.duckai.org
(1/N)
------
MY HYPERPRIORS ARE SIMPLY BETTER THAN YOURS. YOU DONT EVEN HAVE A HYPERPRIOR. YOUR PRIOR IS SIMPLY A DERIVATIVE OF THE HYPERPRIOR OF MY ETHOS. SIMPLE AS
------
Imagine being a leading quantum physics researcher, dedicating your life to the edges of reality. You have climbed the ranks to manage multiple labs. Then, some Koreans figure the thing out and you get 100 texts, at 8 am, with a poorly formatted arxiv link saying "cap or no cap?"
------
Journalism hahahahahahhaahahhaahhaaha ohhhh hahahahahah *breathes in* ahahahahahahhahaahahahahahah
------
This chart from Alex Karp’s op-ed in the NYT is insanely misleading. The y-axes are not comparable.
------
how many conference accepts does JohannesGaessler have, I wonder?
------
Is this the *minimum* requirement for a new grad in machine learning now? #NVIDIA
------
actually shocked at these midjourney outputs
------
precisely. the proof is in the pudding, kid
------
midjourney has gotten SO good. scrolling through the discord invokes more wonder in me than a museum ever has
------
Introducing Mentat - an open source, GPT-4 powered coding assistant!

Mentat runs in your command line, giving it the context of your projects and allowing it to coordinate edits across multiple files!

More videos and a link to github below:
------
makes me happy that some of the best guides for running LLMs locally originate from 4chan users who want to make their waifus real
------
good morning
the future rocks
get to work
------
now I understand people's skepticism about the inevitability of AGI (and the fact that GPT4 is basically AGI)

technology sometimes feels too good to be true
you want to temper your expectations, lest you get heartbroken
------
Innocence is the child, and forgetfulness, and a new beginning, a game, a self-rolling wheel, a first movement

a holy "yes"
------
pictured: a man who knows how little he knows
------
i don't think people understand how much M bucks are worth to nerds
in about 3 years, I will be at a party where the biggest nerd legend will ask to see my score
and based on that, he will either respect me or not
------
this is a good app
------
When you ask backend developer to make frontend
------
also, this is kinda kino design & branding
jugaad branding
type of page to make nat friedman
------
I've worked and met a lot of talented people with 0 credentials

you used to actually need the credentials. of course, the credentials are useful as a filtering function. I'm hoping that we move to more "proof is in the pudding"
------
there is no correlation to academic distinguishment to applied CS skill

Measured by people asking me "hey, who has this very rare skillset that is hard to acquire", and my recommendation class being a toss between an MIT PhD or a eastern european highschool student
------
honestly i'm really glad that elon didn't buy 4chan
------
amusingly it would have been a much more capital efficient purchase
------
good morning
------
does anyone know if a standard home oven can get to 750c? Is there some limiter I can remove or something?
------
Has anyone reproduced? No one is going to reproduce, right?
------
This actually can't be real
------
We are releasing the Advanced Reasoning Benchmark dataset for LLMs (ARB)!

- Evaluates SotA LLMs on ARB, on which even GPT4 struggles
- Explores the feasibility of letting LLMs generate and use rubrics to evaluate generated solutions.

proj: https://arb.duckai.org
(1/N)
------
MY HYPERPRIORS ARE SIMPLY BETTER THAN YOURS. YOU DONT EVEN HAVE A HYPERPRIOR. YOUR PRIOR IS SIMPLY A DERIVATIVE OF THE HYPERPRIOR OF MY ETHOS. SIMPLE AS
------
Imagine being a leading quantum physics researcher, dedicating your life to the edges of reality. You have climbed the ranks to manage multiple labs. Then, some Koreans figure the thing out and you get 100 texts, at 8 am, with a poorly formatted arxiv link saying "cap or no cap?"
------
Journalism hahahahahahhaahahhaahhaaha ohhhh hahahahahah *breathes in* ahahahahahahhahaahahahahahah
------
This chart from Alex Karp’s op-ed in the NYT is insanely misleading. The y-axes are not comparable.
------
how many conference accepts does JohannesGaessler have, I wonder?
------
Is this the *minimum* requirement for a new grad in machine learning now? #NVIDIA
------
actually shocked at these midjourney outputs
------
precisely. the proof is in the pudding, kid
------
midjourney has gotten SO good. scrolling through the discord invokes more wonder in me than a museum ever has
------
Introducing Mentat - an open source, GPT-4 powered coding assistant!

Mentat runs in your command line, giving it the context of your projects and allowing it to coordinate edits across multiple files!

More videos and a link to github below:
------
why do pre-pmf ai startups have policy people and government relation teams
------
not now mom! i'm rehabilitating the model to fix the cyber-alzheimers that was caused by the tensor surgery
------
Fine tuned a code model to generate instructions. A process that can be seen in the paper "LongForm: Optimizing Instruction Tuning for Long Text Generation". Why is this interesting? Because this is one way to build a strong dataset without distilling code from proprietary LLMs
------
we're accelerating so fast that you can basically say that something big is coming whenever you want, and in a week you'll seem like an industry insider
------
forbes journalists, my email is lessismoreforalignmentballs at gmail dot com
------
this
get the fastest training loop possible
e.g. PEFT (LoRA)
on the smallest model
on the preconfigured machine humming next to you
it will help you find many many data bugs
start small
------
Yea I think there's a lot of ways to do it, main reason I am on replit-3B currently is because it's so much faster to train (smaller model) and see the result
------
Innocence is the child, and forgetfulness, and a new beginning, a game, a self-rolling wheel, a first movement

a holy "yes"
------
pictured: a man who knows how little he knows
------
i don't think people understand how much M bucks are worth to nerds
in about 3 years, I will be at a party where the biggest nerd legend will ask to see my score
and based on that, he will either respect me or not
------
this is a good app
------
When you ask backend developer to make frontend
------
also, this is kinda kino design & branding
jugaad branding
type of page to make nat friedman
------
I've worked and met a lot of talented people with 0 credentials

you used to actually need the credentials. of course, the credentials are useful as a filtering function. I'm hoping that we move to more "proof is in the pudding"
------
there is no correlation to academic distinguishment to applied CS skill

Measured by people asking me "hey, who has this very rare skillset that is hard to acquire", and my recommendation class being a toss between an MIT PhD or a eastern european highschool student
------
honestly i'm really glad that elon didn't buy 4chan
------
amusingly it would have been a much more capital efficient purchase
------
good morning
------
does anyone know if a standard home oven can get to 750c? Is there some limiter I can remove or something?
------
Has anyone reproduced? No one is going to reproduce, right?
------
This actually can't be real
------
We are releasing the Advanced Reasoning Benchmark dataset for LLMs (ARB)!

- Evaluates SotA LLMs on ARB, on which even GPT4 struggles
- Explores the feasibility of letting LLMs generate and use rubrics to evaluate generated solutions.

proj: https://arb.duckai.org
(1/N)
------
MY HYPERPRIORS ARE SIMPLY BETTER THAN YOURS. YOU DONT EVEN HAVE A HYPERPRIOR. YOUR PRIOR IS SIMPLY A DERIVATIVE OF THE HYPERPRIOR OF MY ETHOS. SIMPLE AS
------
Imagine being a leading quantum physics researcher, dedicating your life to the edges of reality. You have climbed the ranks to manage multiple labs. Then, some Koreans figure the thing out and you get 100 texts, at 8 am, with a poorly formatted arxiv link saying "cap or no cap?"
------
Journalism hahahahahahhaahahhaahhaaha ohhhh hahahahahah *breathes in* ahahahahahahhahaahahahahahah
------
This chart from Alex Karp’s op-ed in the NYT is insanely misleading. The y-axes are not comparable.
------
how many conference accepts does JohannesGaessler have, I wonder?
------
Is this the *minimum* requirement for a new grad in machine learning now? #NVIDIA
------
actually shocked at these midjourney outputs
------
precisely. the proof is in the pudding, kid
------
midjourney has gotten SO good. scrolling through the discord invokes more wonder in me than a museum ever has
------
Introducing Mentat - an open source, GPT-4 powered coding assistant!

Mentat runs in your command line, giving it the context of your projects and allowing it to coordinate edits across multiple files!

More videos and a link to github below:
------
why do pre-pmf ai startups have policy people and government relation teams
------
not now mom! i'm rehabilitating the model to fix the cyber-alzheimers that was caused by the tensor surgery
------
Fine tuned a code model to generate instructions. A process that can be seen in the paper "LongForm: Optimizing Instruction Tuning for Long Text Generation". Why is this interesting? Because this is one way to build a strong dataset without distilling code from proprietary LLMs
------
we're accelerating so fast that you can basically say that something big is coming whenever you want, and in a week you'll seem like an industry insider
------
forbes journalists, my email is lessismoreforalignmentballs at gmail dot com
------
this
get the fastest training loop possible
e.g. PEFT (LoRA)
on the smallest model
on the preconfigured machine humming next to you
it will help you find many many data bugs
start small
------
Yea I think there's a lot of ways to do it, main reason I am on replit-3B currently is because it's so much faster to train (smaller model) and see the result
------
GOOD MORNING
------
The incidentoooor. "Did you guys notice that? Seems like an incident" "hooohh boy I would not want to be one of their devs right now." "So, your auth standards don't meet the technical definition of oauth, if you're not calling this an incident you need to improve your standards"
------
from 
@finbarrtimbers
's latest s*bst*ck: 
there are two companies
- Ones that need massive capital, o(billion), manhattan style projects
- Companies whose operating cost is the founder's costco bill

bad news if you're an investor
go read it
------
(didn't link to dodge the algo deboosting)

this is something i am facing
the only way, as a founder, to compete with 1, is to hit a breakthrough, which is nigh but impossible to compete against that many sickos

OSS projects get around this, largely
but those aren't 
------
can investors get the same +E(X) without the b$ crits? most founders I know with an actual chance are actually trying to hit 1M ARR, not 100M

moonshots are fundamentally misaligned with most founders. why deal with the communication overhead when you can go full 
@dannypostmaa
?
------
god i miss going to the office https://twitter.com/HelenBevan/status/1683112995918958593…
------
if you can't have fun in team activities like this, you're not a team player and you're ngmi
------
despite all my ragie.. i'm still a wagie in a cagie
------
also, this is kinda kino design & branding
jugaad branding
type of page to make nat friedman
------
I've worked and met a lot of talented people with 0 credentials

you used to actually need the credentials. of course, the credentials are useful as a filtering function. I'm hoping that we move to more "proof is in the pudding"
------
there is no correlation to academic distinguishment to applied CS skill

Measured by people asking me "hey, who has this very rare skillset that is hard to acquire", and my recommendation class being a toss between an MIT PhD or a eastern european highschool student
------
honestly i'm really glad that elon didn't buy 4chan
------
amusingly it would have been a much more capital efficient purchase
------
good morning
------
does anyone know if a standard home oven can get to 750c? Is there some limiter I can remove or something?
------
Has anyone reproduced? No one is going to reproduce, right?
------
This actually can't be real
------
We are releasing the Advanced Reasoning Benchmark dataset for LLMs (ARB)!

- Evaluates SotA LLMs on ARB, on which even GPT4 struggles
- Explores the feasibility of letting LLMs generate and use rubrics to evaluate generated solutions.

proj: https://arb.duckai.org
(1/N)
------
MY HYPERPRIORS ARE SIMPLY BETTER THAN YOURS. YOU DONT EVEN HAVE A HYPERPRIOR. YOUR PRIOR IS SIMPLY A DERIVATIVE OF THE HYPERPRIOR OF MY ETHOS. SIMPLE AS
------
Imagine being a leading quantum physics researcher, dedicating your life to the edges of reality. You have climbed the ranks to manage multiple labs. Then, some Koreans figure the thing out and you get 100 texts, at 8 am, with a poorly formatted arxiv link saying "cap or no cap?"
------
Journalism hahahahahahhaahahhaahhaaha ohhhh hahahahahah *breathes in* ahahahahahahhahaahahahahahah
------
This chart from Alex Karp’s op-ed in the NYT is insanely misleading. The y-axes are not comparable.
------
how many conference accepts does JohannesGaessler have, I wonder?
------
Is this the *minimum* requirement for a new grad in machine learning now? #NVIDIA
------
actually shocked at these midjourney outputs
------
precisely. the proof is in the pudding, kid
------
midjourney has gotten SO good. scrolling through the discord invokes more wonder in me than a museum ever has
------
Introducing Mentat - an open source, GPT-4 powered coding assistant!

Mentat runs in your command line, giving it the context of your projects and allowing it to coordinate edits across multiple files!

More videos and a link to github below:
------
why do pre-pmf ai startups have policy people and government relation teams
------
not now mom! i'm rehabilitating the model to fix the cyber-alzheimers that was caused by the tensor surgery
------
Fine tuned a code model to generate instructions. A process that can be seen in the paper "LongForm: Optimizing Instruction Tuning for Long Text Generation". Why is this interesting? Because this is one way to build a strong dataset without distilling code from proprietary LLMs
------
we're accelerating so fast that you can basically say that something big is coming whenever you want, and in a week you'll seem like an industry insider
------
forbes journalists, my email is lessismoreforalignmentballs at gmail dot com
------
this
get the fastest training loop possible
e.g. PEFT (LoRA)
on the smallest model
on the preconfigured machine humming next to you
it will help you find many many data bugs
start small
------
Yea I think there's a lot of ways to do it, main reason I am on replit-3B currently is because it's so much faster to train (smaller model) and see the result
------
GOOD MORNING
------
The incidentoooor. "Did you guys notice that? Seems like an incident" "hooohh boy I would not want to be one of their devs right now." "So, your auth standards don't meet the technical definition of oauth, if you're not calling this an incident you need to improve your standards"
------
from 
@finbarrtimbers
's latest s*bst*ck: 
there are two companies
- Ones that need massive capital, o(billion), manhattan style projects
- Companies whose operating cost is the founder's costco bill

bad news if you're an investor
go read it
------
(didn't link to dodge the algo deboosting)

this is something i am facing
the only way, as a founder, to compete with 1, is to hit a breakthrough, which is nigh but impossible to compete against that many sickos

OSS projects get around this, largely
but those aren't 
------
can investors get the same +E(X) without the b$ crits? most founders I know with an actual chance are actually trying to hit 1M ARR, not 100M

moonshots are fundamentally misaligned with most founders. why deal with the communication overhead when you can go full 
@dannypostmaa
?
------
god i miss going to the office https://twitter.com/HelenBevan/status/1683112995918958593…
------
if you can't have fun in team activities like this, you're not a team player and you're ngmi
------
despite all my ragie.. i'm still a wagie in a cagie
------
hello sir.
i would like one shannon entropies that still fits the curve
------
finally.. crutches for bad coders is coming out
------
I'm super excited to release what I've been working on tomorrow!
------
cant wait to use it
------
I download separate 30gb weights for each one of my LLM projects. These guys are like babies https://twitter.com/kazzkiq/status/1683534809660833793…
------
I have broken into Japanese twitter

But actually, why can I tell the difference between Japanese anime pfps and westa anime pfps?
------
engineering is always the bottleneck. wtf? can't you guys work any faster? god. lazy ass mfs
------
good night
------
Lmao
Many such cases
------
We are releasing the Advanced Reasoning Benchmark dataset for LLMs (ARB)!

- Evaluates SotA LLMs on ARB, on which even GPT4 struggles
- Explores the feasibility of letting LLMs generate and use rubrics to evaluate generated solutions.

proj: https://arb.duckai.org
(1/N)
------
MY HYPERPRIORS ARE SIMPLY BETTER THAN YOURS. YOU DONT EVEN HAVE A HYPERPRIOR. YOUR PRIOR IS SIMPLY A DERIVATIVE OF THE HYPERPRIOR OF MY ETHOS. SIMPLE AS
------
Imagine being a leading quantum physics researcher, dedicating your life to the edges of reality. You have climbed the ranks to manage multiple labs. Then, some Koreans figure the thing out and you get 100 texts, at 8 am, with a poorly formatted arxiv link saying "cap or no cap?"
------
Journalism hahahahahahhaahahhaahhaaha ohhhh hahahahahah *breathes in* ahahahahahahhahaahahahahahah
------
This chart from Alex Karp’s op-ed in the NYT is insanely misleading. The y-axes are not comparable.
------
how many conference accepts does JohannesGaessler have, I wonder?
------
Is this the *minimum* requirement for a new grad in machine learning now? #NVIDIA
------
actually shocked at these midjourney outputs
------
precisely. the proof is in the pudding, kid
------
midjourney has gotten SO good. scrolling through the discord invokes more wonder in me than a museum ever has
------
Introducing Mentat - an open source, GPT-4 powered coding assistant!

Mentat runs in your command line, giving it the context of your projects and allowing it to coordinate edits across multiple files!

More videos and a link to github below:
------
why do pre-pmf ai startups have policy people and government relation teams
------
not now mom! i'm rehabilitating the model to fix the cyber-alzheimers that was caused by the tensor surgery
------
Fine tuned a code model to generate instructions. A process that can be seen in the paper "LongForm: Optimizing Instruction Tuning for Long Text Generation". Why is this interesting? Because this is one way to build a strong dataset without distilling code from proprietary LLMs
------
we're accelerating so fast that you can basically say that something big is coming whenever you want, and in a week you'll seem like an industry insider
------
forbes journalists, my email is lessismoreforalignmentballs at gmail dot com
------
this
get the fastest training loop possible
e.g. PEFT (LoRA)
on the smallest model
on the preconfigured machine humming next to you
it will help you find many many data bugs
start small
------
Yea I think there's a lot of ways to do it, main reason I am on replit-3B currently is because it's so much faster to train (smaller model) and see the result
------
GOOD MORNING
------
The incidentoooor. "Did you guys notice that? Seems like an incident" "hooohh boy I would not want to be one of their devs right now." "So, your auth standards don't meet the technical definition of oauth, if you're not calling this an incident you need to improve your standards"
------
from 
@finbarrtimbers
's latest s*bst*ck: 
there are two companies
- Ones that need massive capital, o(billion), manhattan style projects
- Companies whose operating cost is the founder's costco bill

bad news if you're an investor
go read it
------
(didn't link to dodge the algo deboosting)

this is something i am facing
the only way, as a founder, to compete with 1, is to hit a breakthrough, which is nigh but impossible to compete against that many sickos

OSS projects get around this, largely
but those aren't 
------
can investors get the same +E(X) without the b$ crits? most founders I know with an actual chance are actually trying to hit 1M ARR, not 100M

moonshots are fundamentally misaligned with most founders. why deal with the communication overhead when you can go full 
@dannypostmaa
?
------
god i miss going to the office https://twitter.com/HelenBevan/status/1683112995918958593…
------
if you can't have fun in team activities like this, you're not a team player and you're ngmi
------
despite all my ragie.. i'm still a wagie in a cagie
------
hello sir.
i would like one shannon entropies that still fits the curve
------
finally.. crutches for bad coders is coming out
------
I'm super excited to release what I've been working on tomorrow!
------
cant wait to use it
------
I download separate 30gb weights for each one of my LLM projects. These guys are like babies https://twitter.com/kazzkiq/status/1683534809660833793…
------
I have broken into Japanese twitter

But actually, why can I tell the difference between Japanese anime pfps and westa anime pfps?
------
engineering is always the bottleneck. wtf? can't you guys work any faster? god. lazy ass mfs
------
good night
------
Lmao
Many such cases
------
We did it! The LLM can now reliably detect the end of the conversation
------
I'm gonna learrrnnnnn
https://twitch.tv/beginanon
------
llama.cpp now has grammar based sampling
that means you can force it to follow a certain grammar, like JSON, (or, a symbolic predicate language)

Merged 19 hours ago - thanks 
@evanqjones
!!!
------
in case you don't know what CFG means
https://arxiv.org/abs/2305.13971
------
Journalism hahahahahahhaahahhaahhaaha ohhhh hahahahahah *breathes in* ahahahahahahhahaahahahahahah
------
This chart from Alex Karp’s op-ed in the NYT is insanely misleading. The y-axes are not comparable.
------
how many conference accepts does JohannesGaessler have, I wonder?
------
Is this the *minimum* requirement for a new grad in machine learning now? #NVIDIA
------
actually shocked at these midjourney outputs
------
precisely. the proof is in the pudding, kid
------
midjourney has gotten SO good. scrolling through the discord invokes more wonder in me than a museum ever has
------
Introducing Mentat - an open source, GPT-4 powered coding assistant!

Mentat runs in your command line, giving it the context of your projects and allowing it to coordinate edits across multiple files!

More videos and a link to github below:
------
why do pre-pmf ai startups have policy people and government relation teams
------
not now mom! i'm rehabilitating the model to fix the cyber-alzheimers that was caused by the tensor surgery
------
Fine tuned a code model to generate instructions. A process that can be seen in the paper "LongForm: Optimizing Instruction Tuning for Long Text Generation". Why is this interesting? Because this is one way to build a strong dataset without distilling code from proprietary LLMs
------
we're accelerating so fast that you can basically say that something big is coming whenever you want, and in a week you'll seem like an industry insider
------
forbes journalists, my email is lessismoreforalignmentballs at gmail dot com
------
this
get the fastest training loop possible
e.g. PEFT (LoRA)
on the smallest model
on the preconfigured machine humming next to you
it will help you find many many data bugs
start small
------
Yea I think there's a lot of ways to do it, main reason I am on replit-3B currently is because it's so much faster to train (smaller model) and see the result
------
from 
@finbarrtimbers
's latest s*bst*ck: 
there are two companies
- Ones that need massive capital, o(billion), manhattan style projects
- Companies whose operating cost is the founder's costco bill

bad news if you're an investor
go read it
------
(didn't link to dodge the algo deboosting)

this is something i am facing
the only way, as a founder, to compete with 1, is to hit a breakthrough, which is nigh but impossible to compete against that many sickos

OSS projects get around this, largely
but those aren't 
------
can investors get the same +E(X) without the b$ crits? most founders I know with an actual chance are actually trying to hit 1M ARR, not 100M

moonshots are fundamentally misaligned with most founders. why deal with the communication overhead when you can go full 
@dannypostmaa
?
------
god i miss going to the office https://twitter.com/HelenBevan/status/1683112995918958593…
------
if you can't have fun in team activities like this, you're not a team player and you're ngmi
------
despite all my ragie.. i'm still a wagie in a cagie
------
hello sir.
i would like one shannon entropies that still fits the curve
------
finally.. crutches for bad coders is coming out
------
I'm super excited to release what I've been working on tomorrow!
------
cant wait to use it
------
I download separate 30gb weights for each one of my LLM projects. These guys are like babies https://twitter.com/kazzkiq/status/1683534809660833793…
------
I have broken into Japanese twitter

But actually, why can I tell the difference between Japanese anime pfps and westa anime pfps?
------
engineering is always the bottleneck. wtf? can't you guys work any faster? god. lazy ass mfs
------
good night
------
Lmao
Many such cases
------
We did it! The LLM can now reliably detect the end of the conversation
------
I'm gonna learrrnnnnn
https://twitch.tv/beginanon
------
llama.cpp now has grammar based sampling
that means you can force it to follow a certain grammar, like JSON, (or, a symbolic predicate language)

Merged 19 hours ago - thanks 
@evanqjones
!!!
------
in case you don't know what CFG means
https://arxiv.org/abs/2305.13971
------
I spent 3 hours labeling data today
------
I was labeling my pre labeled data (labeled it myself)
About 20% has mistakes
------
there is no good code or bad code
only code that makes money
and code that doesn't make money
------
As much as I want to blame other people for bad implementations or design decisions within the software I own, I never do. I'm fundamentally responsible for every line of code I click approve for on a PR. It is my burden to carry. If I were better I wouldn't have approved it.
------
i need biden to fly me and my homies to new mexico and send us a year's supply worth of cigarettes and whiteboard markers STAT
------
midjourney has gotten SO good. scrolling through the discord invokes more wonder in me than a museum ever has
------
Introducing Mentat - an open source, GPT-4 powered coding assistant!

Mentat runs in your command line, giving it the context of your projects and allowing it to coordinate edits across multiple files!

More videos and a link to github below:
------
why do pre-pmf ai startups have policy people and government relation teams
------
not now mom! i'm rehabilitating the model to fix the cyber-alzheimers that was caused by the tensor surgery
------
Fine tuned a code model to generate instructions. A process that can be seen in the paper "LongForm: Optimizing Instruction Tuning for Long Text Generation". Why is this interesting? Because this is one way to build a strong dataset without distilling code from proprietary LLMs
------
we're accelerating so fast that you can basically say that something big is coming whenever you want, and in a week you'll seem like an industry insider
------
forbes journalists, my email is lessismoreforalignmentballs at gmail dot com
------
this
get the fastest training loop possible
e.g. PEFT (LoRA)
on the smallest model
on the preconfigured machine humming next to you
it will help you find many many data bugs
start small
------
Yea I think there's a lot of ways to do it, main reason I am on replit-3B currently is because it's so much faster to train (smaller model) and see the result
------
from 
@finbarrtimbers
's latest s*bst*ck: 
there are two companies
- Ones that need massive capital, o(billion), manhattan style projects
- Companies whose operating cost is the founder's costco bill

bad news if you're an investor
go read it
------
(didn't link to dodge the algo deboosting)

this is something i am facing
the only way, as a founder, to compete with 1, is to hit a breakthrough, which is nigh but impossible to compete against that many sickos

OSS projects get around this, largely
but those aren't 
------
can investors get the same +E(X) without the b$ crits? most founders I know with an actual chance are actually trying to hit 1M ARR, not 100M

moonshots are fundamentally misaligned with most founders. why deal with the communication overhead when you can go full 
@dannypostmaa
?
------
god i miss going to the office https://twitter.com/HelenBevan/status/1683112995918958593…
------
if you can't have fun in team activities like this, you're not a team player and you're ngmi
------
despite all my ragie.. i'm still a wagie in a cagie
------
hello sir.
i would like one shannon entropies that still fits the curve
------
finally.. crutches for bad coders is coming out
------
I'm super excited to release what I've been working on tomorrow!
------
cant wait to use it
------
I download separate 30gb weights for each one of my LLM projects. These guys are like babies https://twitter.com/kazzkiq/status/1683534809660833793…
------
I have broken into Japanese twitter

But actually, why can I tell the difference between Japanese anime pfps and westa anime pfps?
------
engineering is always the bottleneck. wtf? can't you guys work any faster? god. lazy ass mfs
------
good night
------
Lmao
Many such cases
------
We did it! The LLM can now reliably detect the end of the conversation
------
I'm gonna learrrnnnnn
https://twitch.tv/beginanon
------
llama.cpp now has grammar based sampling
that means you can force it to follow a certain grammar, like JSON, (or, a symbolic predicate language)

Merged 19 hours ago - thanks 
@evanqjones
!!!
------
in case you don't know what CFG means
https://arxiv.org/abs/2305.13971
------
I spent 3 hours labeling data today
------
I was labeling my pre labeled data (labeled it myself)
About 20% has mistakes
------
there is no good code or bad code
only code that makes money
and code that doesn't make money
------
As much as I want to blame other people for bad implementations or design decisions within the software I own, I never do. I'm fundamentally responsible for every line of code I click approve for on a PR. It is my burden to carry. If I were better I wouldn't have approved it.
------
i need biden to fly me and my homies to new mexico and send us a year's supply worth of cigarettes and whiteboard markers STAT
------
worldcoin's universal basic income already exists, it's called zero real interest rates
------
get a job and take out a loan, kid.
------
:DDD those are some crazy numbers for "only" 3B parameters
------
BTLM sets a new standard in 3B performance. Thanks to its high quality training data (SlimPajama-627B), it outperforms 3B models trained on almost 2x the data.

It’s also the first model trained on the Condor Galaxy 1 AI supercomputer thanks to the support of G42 Cloud & IIAI!
------
worldcoin exists, it's called your Gmail/apple id + your credit score and transaction history
------
twitter jira
[change bird app logo to x app logo] (done)
[unban 20% of twitter that got errantly banned] (in progress)
------
I never read the paper, but yeah it is pretty much what you can observer in LLaMA as well
------
it would be interesting to run some experiment with the API, giving instructions or examples in system/user/assistant roles and seeing the impact of each
------
I hypothesize that the RLHF to have GPT4 act "good" has harmed it's capability to follow instructions quite a bit

The way I've been getting around it is by putting words in its mouth: having it repeat the instructions back to me

Is this because its learned to ignore users?
------
Fine tuned a code model to generate instructions. A process that can be seen in the paper "LongForm: Optimizing Instruction Tuning for Long Text Generation". Why is this interesting? Because this is one way to build a strong dataset without distilling code from proprietary LLMs
------
we're accelerating so fast that you can basically say that something big is coming whenever you want, and in a week you'll seem like an industry insider
------
forbes journalists, my email is lessismoreforalignmentballs at gmail dot com
------
this
get the fastest training loop possible
e.g. PEFT (LoRA)
on the smallest model
on the preconfigured machine humming next to you
it will help you find many many data bugs
start small
------
Yea I think there's a lot of ways to do it, main reason I am on replit-3B currently is because it's so much faster to train (smaller model) and see the result
------
from 
@finbarrtimbers
's latest s*bst*ck: 
there are two companies
- Ones that need massive capital, o(billion), manhattan style projects
- Companies whose operating cost is the founder's costco bill

bad news if you're an investor
go read it
------
(didn't link to dodge the algo deboosting)

this is something i am facing
the only way, as a founder, to compete with 1, is to hit a breakthrough, which is nigh but impossible to compete against that many sickos

OSS projects get around this, largely
but those aren't 
------
can investors get the same +E(X) without the b$ crits? most founders I know with an actual chance are actually trying to hit 1M ARR, not 100M

moonshots are fundamentally misaligned with most founders. why deal with the communication overhead when you can go full 
@dannypostmaa
?
------
god i miss going to the office https://twitter.com/HelenBevan/status/1683112995918958593…
------
if you can't have fun in team activities like this, you're not a team player and you're ngmi
------
despite all my ragie.. i'm still a wagie in a cagie
------
hello sir.
i would like one shannon entropies that still fits the curve
------
finally.. crutches for bad coders is coming out
------
I'm super excited to release what I've been working on tomorrow!
------
cant wait to use it
------
I download separate 30gb weights for each one of my LLM projects. These guys are like babies https://twitter.com/kazzkiq/status/1683534809660833793…
------
I have broken into Japanese twitter

But actually, why can I tell the difference between Japanese anime pfps and westa anime pfps?
------
engineering is always the bottleneck. wtf? can't you guys work any faster? god. lazy ass mfs
------
good night
------
Lmao
Many such cases
------
We did it! The LLM can now reliably detect the end of the conversation
------
I'm gonna learrrnnnnn
https://twitch.tv/beginanon
------
llama.cpp now has grammar based sampling
that means you can force it to follow a certain grammar, like JSON, (or, a symbolic predicate language)

Merged 19 hours ago - thanks 
@evanqjones
!!!
------
in case you don't know what CFG means
https://arxiv.org/abs/2305.13971
------
I spent 3 hours labeling data today
------
I was labeling my pre labeled data (labeled it myself)
About 20% has mistakes
------
there is no good code or bad code
only code that makes money
and code that doesn't make money
------
As much as I want to blame other people for bad implementations or design decisions within the software I own, I never do. I'm fundamentally responsible for every line of code I click approve for on a PR. It is my burden to carry. If I were better I wouldn't have approved it.
------
i need biden to fly me and my homies to new mexico and send us a year's supply worth of cigarettes and whiteboard markers STAT
------
worldcoin's universal basic income already exists, it's called zero real interest rates
------
get a job and take out a loan, kid.
------
:DDD those are some crazy numbers for "only" 3B parameters
------
BTLM sets a new standard in 3B performance. Thanks to its high quality training data (SlimPajama-627B), it outperforms 3B models trained on almost 2x the data.

It’s also the first model trained on the Condor Galaxy 1 AI supercomputer thanks to the support of G42 Cloud & IIAI!
------
worldcoin exists, it's called your Gmail/apple id + your credit score and transaction history
------
twitter jira
[change bird app logo to x app logo] (done)
[unban 20% of twitter that got errantly banned] (in progress)
------
I never read the paper, but yeah it is pretty much what you can observer in LLaMA as well
------
it would be interesting to run some experiment with the API, giving instructions or examples in system/user/assistant roles and seeing the impact of each
------
I hypothesize that the RLHF to have GPT4 act "good" has harmed it's capability to follow instructions quite a bit

The way I've been getting around it is by putting words in its mouth: having it repeat the instructions back to me

Is this because its learned to ignore users?
------
disclaimer: i really just be sayin shit
------
POV: Someone entering /r/LocalLLaMA for the first time
------
Intelligence is the commodity we are in most dire need to improve human condition.
------
There are a lot of things wrong with this world…

but too much intelligence is not one of them.
------
"This isn't related to what the job requires?"
"Listen kid, just reverse the binary tree"
------
"you don't know what a binary tree is?"
------
finally.. crutches for bad coders is coming out
------
I'm super excited to release what I've been working on tomorrow!
------
cant wait to use it
------
I download separate 30gb weights for each one of my LLM projects. These guys are like babies https://twitter.com/kazzkiq/status/1683534809660833793…
------
I have broken into Japanese twitter

But actually, why can I tell the difference between Japanese anime pfps and westa anime pfps?
------
engineering is always the bottleneck. wtf? can't you guys work any faster? god. lazy ass mfs
------
good night
------
Lmao
Many such cases
------
We did it! The LLM can now reliably detect the end of the conversation
------
I'm gonna learrrnnnnn
https://twitch.tv/beginanon
------
llama.cpp now has grammar based sampling
that means you can force it to follow a certain grammar, like JSON, (or, a symbolic predicate language)

Merged 19 hours ago - thanks 
@evanqjones
!!!
------
in case you don't know what CFG means
https://arxiv.org/abs/2305.13971
------
I spent 3 hours labeling data today
------
I was labeling my pre labeled data (labeled it myself)
About 20% has mistakes
------
there is no good code or bad code
only code that makes money
and code that doesn't make money
------
As much as I want to blame other people for bad implementations or design decisions within the software I own, I never do. I'm fundamentally responsible for every line of code I click approve for on a PR. It is my burden to carry. If I were better I wouldn't have approved it.
------
i need biden to fly me and my homies to new mexico and send us a year's supply worth of cigarettes and whiteboard markers STAT
------
worldcoin's universal basic income already exists, it's called zero real interest rates
------
get a job and take out a loan, kid.
------
:DDD those are some crazy numbers for "only" 3B parameters
------
BTLM sets a new standard in 3B performance. Thanks to its high quality training data (SlimPajama-627B), it outperforms 3B models trained on almost 2x the data.

It’s also the first model trained on the Condor Galaxy 1 AI supercomputer thanks to the support of G42 Cloud & IIAI!
------
worldcoin exists, it's called your Gmail/apple id + your credit score and transaction history
------
twitter jira
[change bird app logo to x app logo] (done)
[unban 20% of twitter that got errantly banned] (in progress)
------
I never read the paper, but yeah it is pretty much what you can observer in LLaMA as well
------
it would be interesting to run some experiment with the API, giving instructions or examples in system/user/assistant roles and seeing the impact of each
------
I hypothesize that the RLHF to have GPT4 act "good" has harmed it's capability to follow instructions quite a bit

The way I've been getting around it is by putting words in its mouth: having it repeat the instructions back to me

Is this because its learned to ignore users?
------
disclaimer: i really just be sayin shit
------
POV: Someone entering /r/LocalLLaMA for the first time
------
Intelligence is the commodity we are in most dire need to improve human condition.
------
There are a lot of things wrong with this world…

but too much intelligence is not one of them.
------
"This isn't related to what the job requires?"
"Listen kid, just reverse the binary tree"
------
"you don't know what a binary tree is?"
------
Software engineers when the company they’re interviewing with asks them to spend 30 minutes solving a basic programming puzzle before they agree to pay them $380K per year to do 12 hours of work per week
------
a really good way to understand a movement is: what if you increase the saturation to the max?
e/a => you get sam bankman fried
------
that whole thing was honestly so crazy
but yeah
it was astral less wrong with saturation dialed in at 11
------
A bit of awful personal news. I overcooked my burgers and they are a little bit dry
------
Just got a call from the Canadian Government.. they want me to hand back my dad card..
------
computer scientists don't like to talk about it bc it's embarrassing but basically any decent math or physics person can pick up computing and excel at it within 18 months. we try to keep it quiet bc we want to keep making those sweet mid 6 figs for what we know is midwit work
------
lecun just victory lapping right now
------
Remember the Galactica screeching??? Hahaha 
------
Lmao
Many such cases
------
We did it! The LLM can now reliably detect the end of the conversation
------
I'm gonna learrrnnnnn
https://twitch.tv/beginanon
------
llama.cpp now has grammar based sampling
that means you can force it to follow a certain grammar, like JSON, (or, a symbolic predicate language)

Merged 19 hours ago - thanks 
@evanqjones
!!!
------
in case you don't know what CFG means
https://arxiv.org/abs/2305.13971
------
I spent 3 hours labeling data today
------
I was labeling my pre labeled data (labeled it myself)
About 20% has mistakes
------
there is no good code or bad code
only code that makes money
and code that doesn't make money
------
As much as I want to blame other people for bad implementations or design decisions within the software I own, I never do. I'm fundamentally responsible for every line of code I click approve for on a PR. It is my burden to carry. If I were better I wouldn't have approved it.
------
i need biden to fly me and my homies to new mexico and send us a year's supply worth of cigarettes and whiteboard markers STAT
------
worldcoin's universal basic income already exists, it's called zero real interest rates
------
get a job and take out a loan, kid.
------
:DDD those are some crazy numbers for "only" 3B parameters
------
BTLM sets a new standard in 3B performance. Thanks to its high quality training data (SlimPajama-627B), it outperforms 3B models trained on almost 2x the data.

It’s also the first model trained on the Condor Galaxy 1 AI supercomputer thanks to the support of G42 Cloud & IIAI!
------
worldcoin exists, it's called your Gmail/apple id + your credit score and transaction history
------
twitter jira
[change bird app logo to x app logo] (done)
[unban 20% of twitter that got errantly banned] (in progress)
------
I never read the paper, but yeah it is pretty much what you can observer in LLaMA as well
------
it would be interesting to run some experiment with the API, giving instructions or examples in system/user/assistant roles and seeing the impact of each
------
I hypothesize that the RLHF to have GPT4 act "good" has harmed it's capability to follow instructions quite a bit

The way I've been getting around it is by putting words in its mouth: having it repeat the instructions back to me

Is this because its learned to ignore users?
------
disclaimer: i really just be sayin shit
------
POV: Someone entering /r/LocalLLaMA for the first time
------
Intelligence is the commodity we are in most dire need to improve human condition.
------
There are a lot of things wrong with this world…

but too much intelligence is not one of them.
------
"This isn't related to what the job requires?"
"Listen kid, just reverse the binary tree"
------
"you don't know what a binary tree is?"
------
Software engineers when the company they’re interviewing with asks them to spend 30 minutes solving a basic programming puzzle before they agree to pay them $380K per year to do 12 hours of work per week
------
a really good way to understand a movement is: what if you increase the saturation to the max?
e/a => you get sam bankman fried
------
that whole thing was honestly so crazy
but yeah
it was astral less wrong with saturation dialed in at 11
------
A bit of awful personal news. I overcooked my burgers and they are a little bit dry
------
Just got a call from the Canadian Government.. they want me to hand back my dad card..
------
computer scientists don't like to talk about it bc it's embarrassing but basically any decent math or physics person can pick up computing and excel at it within 18 months. we try to keep it quiet bc we want to keep making those sweet mid 6 figs for what we know is midwit work
------
lecun just victory lapping right now
------
Remember the Galactica screeching??? Hahaha 
------
Qualcomm working with Meta to run Llama-2 on mobile devices.
------
Llama-2 is coming to your phone: https://qualcomm.com/news/releases/2023/07/qualcomm-works-with-meta-to-enable-on-device-ai-applications-usi…
------
Ilya is like your dad's mysterious friend from university that you see like once as a 6 year old and then again as a 17 year old
------
you cannot even begin to understand how early we are
------
I introduced my parents to ChatGPT today. They never heard about it, had trouble signing up, and were completely mindblown that such a thing exists or how it works or how to use it. Fun reminder that I live in a bubble.
------
today i will remind them
https://arxiv.org/abs/2305.11206
------
Lima balls
------
First attempt 8,800 samples scored 10.97% on code benchmark. Second attempt extensive cleaning 50% less data with 4400 samples, high quality score 11.8% on benchmark results in faster training and a better model
------
pic related it's me
------
Old school business types

Work horses

Pretty common in the minerals, real estate, jets, yachts and import/export industries

Lot to learn from them
------
in case you don't know what CFG means
https://arxiv.org/abs/2305.13971
------
I spent 3 hours labeling data today
------
I was labeling my pre labeled data (labeled it myself)
About 20% has mistakes
------
there is no good code or bad code
only code that makes money
and code that doesn't make money
------
As much as I want to blame other people for bad implementations or design decisions within the software I own, I never do. I'm fundamentally responsible for every line of code I click approve for on a PR. It is my burden to carry. If I were better I wouldn't have approved it.
------
i need biden to fly me and my homies to new mexico and send us a year's supply worth of cigarettes and whiteboard markers STAT
------
worldcoin's universal basic income already exists, it's called zero real interest rates
------
get a job and take out a loan, kid.
------
:DDD those are some crazy numbers for "only" 3B parameters
------
BTLM sets a new standard in 3B performance. Thanks to its high quality training data (SlimPajama-627B), it outperforms 3B models trained on almost 2x the data.

It’s also the first model trained on the Condor Galaxy 1 AI supercomputer thanks to the support of G42 Cloud & IIAI!
------
worldcoin exists, it's called your Gmail/apple id + your credit score and transaction history
------
twitter jira
[change bird app logo to x app logo] (done)
[unban 20% of twitter that got errantly banned] (in progress)
------
I never read the paper, but yeah it is pretty much what you can observer in LLaMA as well
------
it would be interesting to run some experiment with the API, giving instructions or examples in system/user/assistant roles and seeing the impact of each
------
I hypothesize that the RLHF to have GPT4 act "good" has harmed it's capability to follow instructions quite a bit

The way I've been getting around it is by putting words in its mouth: having it repeat the instructions back to me

Is this because its learned to ignore users?
------
disclaimer: i really just be sayin shit
------
POV: Someone entering /r/LocalLLaMA for the first time
------
Intelligence is the commodity we are in most dire need to improve human condition.
------
There are a lot of things wrong with this world…

but too much intelligence is not one of them.
------
"This isn't related to what the job requires?"
"Listen kid, just reverse the binary tree"
------
"you don't know what a binary tree is?"
------
Software engineers when the company they’re interviewing with asks them to spend 30 minutes solving a basic programming puzzle before they agree to pay them $380K per year to do 12 hours of work per week
------
a really good way to understand a movement is: what if you increase the saturation to the max?
e/a => you get sam bankman fried
------
that whole thing was honestly so crazy
but yeah
it was astral less wrong with saturation dialed in at 11
------
A bit of awful personal news. I overcooked my burgers and they are a little bit dry
------
Just got a call from the Canadian Government.. they want me to hand back my dad card..
------
computer scientists don't like to talk about it bc it's embarrassing but basically any decent math or physics person can pick up computing and excel at it within 18 months. we try to keep it quiet bc we want to keep making those sweet mid 6 figs for what we know is midwit work
------
lecun just victory lapping right now
------
Remember the Galactica screeching??? Hahaha 
------
Qualcomm working with Meta to run Llama-2 on mobile devices.
------
Llama-2 is coming to your phone: https://qualcomm.com/news/releases/2023/07/qualcomm-works-with-meta-to-enable-on-device-ai-applications-usi…
------
Ilya is like your dad's mysterious friend from university that you see like once as a 6 year old and then again as a 17 year old
------
you cannot even begin to understand how early we are
------
I introduced my parents to ChatGPT today. They never heard about it, had trouble signing up, and were completely mindblown that such a thing exists or how it works or how to use it. Fun reminder that I live in a bubble.
------
today i will remind them
https://arxiv.org/abs/2305.11206
------
Lima balls
------
First attempt 8,800 samples scored 10.97% on code benchmark. Second attempt extensive cleaning 50% less data with 4400 samples, high quality score 11.8% on benchmark results in faster training and a better model
------
pic related it's me
------
Old school business types

Work horses

Pretty common in the minerals, real estate, jets, yachts and import/export industries

Lot to learn from them
------
My fun weekend hack: llama2.c 
https://github.com/karpathy/llama2.c…
Lets you train a baby Llama 2 model in PyTorch, then inference it with one 500-line file with no dependencies, in pure C. My pretrained model (on TinyStories) samples stories in fp32 at 18 tok/s on my MacBook Air M1 CPU.
------
My life is in the hands of the machine now 
@anyma_eva
------
:DDD those are some crazy numbers for "only" 3B parameters
------
BTLM sets a new standard in 3B performance. Thanks to its high quality training data (SlimPajama-627B), it outperforms 3B models trained on almost 2x the data.

It’s also the first model trained on the Condor Galaxy 1 AI supercomputer thanks to the support of G42 Cloud & IIAI!
------
worldcoin exists, it's called your Gmail/apple id + your credit score and transaction history
------
twitter jira
[change bird app logo to x app logo] (done)
[unban 20% of twitter that got errantly banned] (in progress)
------
I never read the paper, but yeah it is pretty much what you can observer in LLaMA as well
------
it would be interesting to run some experiment with the API, giving instructions or examples in system/user/assistant roles and seeing the impact of each
------
I hypothesize that the RLHF to have GPT4 act "good" has harmed it's capability to follow instructions quite a bit

The way I've been getting around it is by putting words in its mouth: having it repeat the instructions back to me

Is this because its learned to ignore users?
------
disclaimer: i really just be sayin shit
------
POV: Someone entering /r/LocalLLaMA for the first time
------
Intelligence is the commodity we are in most dire need to improve human condition.
------
There are a lot of things wrong with this world…

but too much intelligence is not one of them.
------
"This isn't related to what the job requires?"
"Listen kid, just reverse the binary tree"
------
"you don't know what a binary tree is?"
------
Software engineers when the company they’re interviewing with asks them to spend 30 minutes solving a basic programming puzzle before they agree to pay them $380K per year to do 12 hours of work per week
------
a really good way to understand a movement is: what if you increase the saturation to the max?
e/a => you get sam bankman fried
------
that whole thing was honestly so crazy
but yeah
it was astral less wrong with saturation dialed in at 11
------
A bit of awful personal news. I overcooked my burgers and they are a little bit dry
------
Just got a call from the Canadian Government.. they want me to hand back my dad card..
------
computer scientists don't like to talk about it bc it's embarrassing but basically any decent math or physics person can pick up computing and excel at it within 18 months. we try to keep it quiet bc we want to keep making those sweet mid 6 figs for what we know is midwit work
------
lecun just victory lapping right now
------
Remember the Galactica screeching??? Hahaha 
------
Qualcomm working with Meta to run Llama-2 on mobile devices.
------
Llama-2 is coming to your phone: https://qualcomm.com/news/releases/2023/07/qualcomm-works-with-meta-to-enable-on-device-ai-applications-usi…
------
Ilya is like your dad's mysterious friend from university that you see like once as a 6 year old and then again as a 17 year old
------
you cannot even begin to understand how early we are
------
I introduced my parents to ChatGPT today. They never heard about it, had trouble signing up, and were completely mindblown that such a thing exists or how it works or how to use it. Fun reminder that I live in a bubble.
------
today i will remind them
https://arxiv.org/abs/2305.11206
------
Lima balls
------
First attempt 8,800 samples scored 10.97% on code benchmark. Second attempt extensive cleaning 50% less data with 4400 samples, high quality score 11.8% on benchmark results in faster training and a better model
------
pic related it's me
------
Old school business types

Work horses

Pretty common in the minerals, real estate, jets, yachts and import/export industries

Lot to learn from them
------
My fun weekend hack: llama2.c 
https://github.com/karpathy/llama2.c…
Lets you train a baby Llama 2 model in PyTorch, then inference it with one 500-line file with no dependencies, in pure C. My pretrained model (on TinyStories) samples stories in fp32 at 18 tok/s on my MacBook Air M1 CPU.
------
My life is in the hands of the machine now 
@anyma_eva
------
The latest release of bitsandbytes has an improved CUDA setup and A100 4-bit inference.

I thought that A40 and A100 GPUs were close enough, and optimized for A40s, but they are very different. A100 performance is now 40% faster with a small hit for other GPUs.
------
i love the aesthetic of the anon twitter kids who shitpost about ai all day

they are bringing sf energy back
------
AI is nothing like nuclear bombs. It’s either like the printing press (which is why journalists are afraid) or it’s like photosynthesis.
------
im thinkin.................... e acc is based
------
if you spoke spoke then what grass fed beef?
------
how have we not invented a battery that doesn't run out yet?
------
POV: Someone entering /r/LocalLLaMA for the first time
------
Intelligence is the commodity we are in most dire need to improve human condition.
------
There are a lot of things wrong with this world…

but too much intelligence is not one of them.
------
"This isn't related to what the job requires?"
"Listen kid, just reverse the binary tree"
------
"you don't know what a binary tree is?"
------
Software engineers when the company they’re interviewing with asks them to spend 30 minutes solving a basic programming puzzle before they agree to pay them $380K per year to do 12 hours of work per week
------
a really good way to understand a movement is: what if you increase the saturation to the max?
e/a => you get sam bankman fried
------
that whole thing was honestly so crazy
but yeah
it was astral less wrong with saturation dialed in at 11
------
A bit of awful personal news. I overcooked my burgers and they are a little bit dry
------
Just got a call from the Canadian Government.. they want me to hand back my dad card..
------
computer scientists don't like to talk about it bc it's embarrassing but basically any decent math or physics person can pick up computing and excel at it within 18 months. we try to keep it quiet bc we want to keep making those sweet mid 6 figs for what we know is midwit work
------
lecun just victory lapping right now
------
Remember the Galactica screeching??? Hahaha 
------
Qualcomm working with Meta to run Llama-2 on mobile devices.
------
Llama-2 is coming to your phone: https://qualcomm.com/news/releases/2023/07/qualcomm-works-with-meta-to-enable-on-device-ai-applications-usi…
------
Ilya is like your dad's mysterious friend from university that you see like once as a 6 year old and then again as a 17 year old
------
you cannot even begin to understand how early we are
------
I introduced my parents to ChatGPT today. They never heard about it, had trouble signing up, and were completely mindblown that such a thing exists or how it works or how to use it. Fun reminder that I live in a bubble.
------
today i will remind them
https://arxiv.org/abs/2305.11206
------
Lima balls
------
First attempt 8,800 samples scored 10.97% on code benchmark. Second attempt extensive cleaning 50% less data with 4400 samples, high quality score 11.8% on benchmark results in faster training and a better model
------
pic related it's me
------
Old school business types

Work horses

Pretty common in the minerals, real estate, jets, yachts and import/export industries

Lot to learn from them
------
My fun weekend hack: llama2.c 
https://github.com/karpathy/llama2.c…
Lets you train a baby Llama 2 model in PyTorch, then inference it with one 500-line file with no dependencies, in pure C. My pretrained model (on TinyStories) samples stories in fp32 at 18 tok/s on my MacBook Air M1 CPU.
------
My life is in the hands of the machine now 
@anyma_eva
------
The latest release of bitsandbytes has an improved CUDA setup and A100 4-bit inference.

I thought that A40 and A100 GPUs were close enough, and optimized for A40s, but they are very different. A100 performance is now 40% faster with a small hit for other GPUs.
------
i love the aesthetic of the anon twitter kids who shitpost about ai all day

they are bringing sf energy back
------
AI is nothing like nuclear bombs. It’s either like the printing press (which is why journalists are afraid) or it’s like photosynthesis.
------
im thinkin.................... e acc is based
------
if you spoke spoke then what grass fed beef?
------
how have we not invented a battery that doesn't run out yet?
------
our culture's reverence for experts is a mistake
------
Just had an awful twitter space and talked shit for three hours. Met 
@josh_rees
 he is a Sydney VC that runs underground raves but has the local gang colluding with the police to crash his parties because he is taking too much of their business
------
I hear people, and sometimes myself, come up with these little stories about why they’re not as talented as they could be. Meanwhile the extremely talented people I know are like, “Ya I did heroin for 4 years”
------
damn i love custom instructions
------
watch barbie? or watch ben shapiro's review at 2x speed?
------
Software engineers when the company they’re interviewing with asks them to spend 30 minutes solving a basic programming puzzle before they agree to pay them $380K per year to do 12 hours of work per week
------
a really good way to understand a movement is: what if you increase the saturation to the max?
e/a => you get sam bankman fried
------
that whole thing was honestly so crazy
but yeah
it was astral less wrong with saturation dialed in at 11
------
A bit of awful personal news. I overcooked my burgers and they are a little bit dry
------
Just got a call from the Canadian Government.. they want me to hand back my dad card..
------
computer scientists don't like to talk about it bc it's embarrassing but basically any decent math or physics person can pick up computing and excel at it within 18 months. we try to keep it quiet bc we want to keep making those sweet mid 6 figs for what we know is midwit work
------
lecun just victory lapping right now
------
Remember the Galactica screeching??? Hahaha 
------
Qualcomm working with Meta to run Llama-2 on mobile devices.
------
Llama-2 is coming to your phone: https://qualcomm.com/news/releases/2023/07/qualcomm-works-with-meta-to-enable-on-device-ai-applications-usi…
------
Ilya is like your dad's mysterious friend from university that you see like once as a 6 year old and then again as a 17 year old
------
you cannot even begin to understand how early we are
------
I introduced my parents to ChatGPT today. They never heard about it, had trouble signing up, and were completely mindblown that such a thing exists or how it works or how to use it. Fun reminder that I live in a bubble.
------
today i will remind them
https://arxiv.org/abs/2305.11206
------
Lima balls
------
First attempt 8,800 samples scored 10.97% on code benchmark. Second attempt extensive cleaning 50% less data with 4400 samples, high quality score 11.8% on benchmark results in faster training and a better model
------
pic related it's me
------
Old school business types

Work horses

Pretty common in the minerals, real estate, jets, yachts and import/export industries

Lot to learn from them
------
My fun weekend hack: llama2.c 
https://github.com/karpathy/llama2.c…
Lets you train a baby Llama 2 model in PyTorch, then inference it with one 500-line file with no dependencies, in pure C. My pretrained model (on TinyStories) samples stories in fp32 at 18 tok/s on my MacBook Air M1 CPU.
------
My life is in the hands of the machine now 
@anyma_eva
------
The latest release of bitsandbytes has an improved CUDA setup and A100 4-bit inference.

I thought that A40 and A100 GPUs were close enough, and optimized for A40s, but they are very different. A100 performance is now 40% faster with a small hit for other GPUs.
------
i love the aesthetic of the anon twitter kids who shitpost about ai all day

they are bringing sf energy back
------
AI is nothing like nuclear bombs. It’s either like the printing press (which is why journalists are afraid) or it’s like photosynthesis.
------
im thinkin.................... e acc is based
------
if you spoke spoke then what grass fed beef?
------
how have we not invented a battery that doesn't run out yet?
------
our culture's reverence for experts is a mistake
------
Just had an awful twitter space and talked shit for three hours. Met 
@josh_rees
 he is a Sydney VC that runs underground raves but has the local gang colluding with the police to crash his parties because he is taking too much of their business
------
I hear people, and sometimes myself, come up with these little stories about why they’re not as talented as they could be. Meanwhile the extremely talented people I know are like, “Ya I did heroin for 4 years”
------
damn i love custom instructions
------
watch barbie? or watch ben shapiro's review at 2x speed?
------
This guy wrote custom software that takes a 3D scan and then creates a 3D printable design of a custom face mask
------
This is one of the coolest projects I’ve ever seen in VR

GO GIVE THEM SOME LOVE!

https://youtu.be/B_Q0SbGr7wE
------
3D scan software really has come a long way
------
Early Torvalds email announcing his hobby project, “just a hobby, won’t be big and professional like gnu”
------
Introducing DialogStudio, the largest and most diverse dialogue dataset collection with diverse goals （e.g. task-oriented, open-domain, NLU, etc.) and different domains (e.g. finance, insurance software, movie, etc.)
http://github.com/salesforce/DialogStudio… 
https://huggingface.co/datasets/Salesforce/dialogstudio…
#NLP #AI
------
Ilya is like your dad's mysterious friend from university that you see like once as a 6 year old and then again as a 17 year old
------
you cannot even begin to understand how early we are
------
I introduced my parents to ChatGPT today. They never heard about it, had trouble signing up, and were completely mindblown that such a thing exists or how it works or how to use it. Fun reminder that I live in a bubble.
------
today i will remind them
https://arxiv.org/abs/2305.11206
------
Lima balls
------
First attempt 8,800 samples scored 10.97% on code benchmark. Second attempt extensive cleaning 50% less data with 4400 samples, high quality score 11.8% on benchmark results in faster training and a better model
------
pic related it's me
------
Old school business types

Work horses

Pretty common in the minerals, real estate, jets, yachts and import/export industries

Lot to learn from them
------
My fun weekend hack: llama2.c 
https://github.com/karpathy/llama2.c…
Lets you train a baby Llama 2 model in PyTorch, then inference it with one 500-line file with no dependencies, in pure C. My pretrained model (on TinyStories) samples stories in fp32 at 18 tok/s on my MacBook Air M1 CPU.
------
My life is in the hands of the machine now 
@anyma_eva
------
The latest release of bitsandbytes has an improved CUDA setup and A100 4-bit inference.

I thought that A40 and A100 GPUs were close enough, and optimized for A40s, but they are very different. A100 performance is now 40% faster with a small hit for other GPUs.
------
i love the aesthetic of the anon twitter kids who shitpost about ai all day

they are bringing sf energy back
------
AI is nothing like nuclear bombs. It’s either like the printing press (which is why journalists are afraid) or it’s like photosynthesis.
------
im thinkin.................... e acc is based
------
if you spoke spoke then what grass fed beef?
------
how have we not invented a battery that doesn't run out yet?
------
our culture's reverence for experts is a mistake
------
Just had an awful twitter space and talked shit for three hours. Met 
@josh_rees
 he is a Sydney VC that runs underground raves but has the local gang colluding with the police to crash his parties because he is taking too much of their business
------
I hear people, and sometimes myself, come up with these little stories about why they’re not as talented as they could be. Meanwhile the extremely talented people I know are like, “Ya I did heroin for 4 years”
------
damn i love custom instructions
------
watch barbie? or watch ben shapiro's review at 2x speed?
------
This guy wrote custom software that takes a 3D scan and then creates a 3D printable design of a custom face mask
------
This is one of the coolest projects I’ve ever seen in VR

GO GIVE THEM SOME LOVE!

https://youtu.be/B_Q0SbGr7wE
------
3D scan software really has come a long way
------
Early Torvalds email announcing his hobby project, “just a hobby, won’t be big and professional like gnu”
------
Introducing DialogStudio, the largest and most diverse dialogue dataset collection with diverse goals （e.g. task-oriented, open-domain, NLU, etc.) and different domains (e.g. finance, insurance software, movie, etc.)
http://github.com/salesforce/DialogStudio… 
https://huggingface.co/datasets/Salesforce/dialogstudio…
#NLP #AI
------
Sample enough times and you’ll get the model to produce coherent/correct code. Using n=10 here for 10 samples and picking the best one as the final instruction/response
------
i am become normie, goer of movies
------
pic related it's me
------
Old school business types

Work horses

Pretty common in the minerals, real estate, jets, yachts and import/export industries

Lot to learn from them
------
My fun weekend hack: llama2.c 
https://github.com/karpathy/llama2.c…
Lets you train a baby Llama 2 model in PyTorch, then inference it with one 500-line file with no dependencies, in pure C. My pretrained model (on TinyStories) samples stories in fp32 at 18 tok/s on my MacBook Air M1 CPU.
------
My life is in the hands of the machine now 
@anyma_eva
------
The latest release of bitsandbytes has an improved CUDA setup and A100 4-bit inference.

I thought that A40 and A100 GPUs were close enough, and optimized for A40s, but they are very different. A100 performance is now 40% faster with a small hit for other GPUs.
------
i love the aesthetic of the anon twitter kids who shitpost about ai all day

they are bringing sf energy back
------
AI is nothing like nuclear bombs. It’s either like the printing press (which is why journalists are afraid) or it’s like photosynthesis.
------
im thinkin.................... e acc is based
------
if you spoke spoke then what grass fed beef?
------
how have we not invented a battery that doesn't run out yet?
------
our culture's reverence for experts is a mistake
------
Just had an awful twitter space and talked shit for three hours. Met 
@josh_rees
 he is a Sydney VC that runs underground raves but has the local gang colluding with the police to crash his parties because he is taking too much of their business
------
I hear people, and sometimes myself, come up with these little stories about why they’re not as talented as they could be. Meanwhile the extremely talented people I know are like, “Ya I did heroin for 4 years”
------
damn i love custom instructions
------
watch barbie? or watch ben shapiro's review at 2x speed?
------
This guy wrote custom software that takes a 3D scan and then creates a 3D printable design of a custom face mask
------
This is one of the coolest projects I’ve ever seen in VR

GO GIVE THEM SOME LOVE!

https://youtu.be/B_Q0SbGr7wE
------
3D scan software really has come a long way
------
Early Torvalds email announcing his hobby project, “just a hobby, won’t be big and professional like gnu”
------
Introducing DialogStudio, the largest and most diverse dialogue dataset collection with diverse goals （e.g. task-oriented, open-domain, NLU, etc.) and different domains (e.g. finance, insurance software, movie, etc.)
http://github.com/salesforce/DialogStudio… 
https://huggingface.co/datasets/Salesforce/dialogstudio…
#NLP #AI
------
Sample enough times and you’ll get the model to produce coherent/correct code. Using n=10 here for 10 samples and picking the best one as the final instruction/response
------
i am become normie, goer of movies
------
sometimes i need to just like
look at my empty email inbox
and reaffirm to myself that yes, in fact, i have nothing urgent to attend to
------
Happy about the announcement of another awesome project I got the privilege to work on, but was mostly made by the great work of 
@dmayhem93
, FreeWilly 1 & 2! 

The dataset is 10% Orca paper replication! 

Thanks to 
@StabilityAI
 this dataset was able to be trained on Llama2-70b!!
------
Introducing FreeWilly1 and FreeWilly2 - The latest groundbreaking LLMs from Stability AI's and @carperai  lab! 
Open access and remarkable versatility. 
#AI #LLMs #OpenAccess  #StabilityAI 
https://bit.ly/3q1cCx2
------
finally getting what indiepreneur folks are saying about not being able to balance marketing & shipping. marketing really does take a lot of time. for example, i use twitter about 5 hours a day because i do it for marketing
------
not because it's extremely fun haha
------
Postman is the worst product on the face of the planet and its not even performant on top of that
------
can't believe they solved the car jacking problem by making every car just drive itself so you don't have to park anywhere
------
KEYWORD VOLUNTARY
------
THE WEST IS THE BEST BECAUSE IT IS FREE TO BEE
------
Announcing Nous Hermes Llama 2 13b!

This new version of Hermes, trained on Llama 2, has 4k context, and beats the benchmarks of original Hermes on every one we tested, including GPT4All benchmarks, BigBench, and AGIEval.

We have FP16, GGML, and GPTQ weights available!
------
Nous-Hermes-Llama2-13b weights have been released.

The model beats its previous benchmarks across the board, with the same dataset + hyperparameters.
4k context.  GGML + GPTQ versions ready.

Thanks to our sponsor @RedmondAI for the generous compute.  

https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b…
------
Learning about LoRAs | Talk
------
Hermes has completed training, and benchmarks across all our recent models are running. Will likely have an announcement tomorrow 
------
Reminder than LLaMa (v1) got released in February 2023 and already has 653 citations. Many works simply wouldn't have existed if this technology wasn't made available in the open.

So huge kudos to 
@MetaAI
 for releasing these models, enabling people to hack on them, improve them
------
The latest release of bitsandbytes has an improved CUDA setup and A100 4-bit inference.

I thought that A40 and A100 GPUs were close enough, and optimized for A40s, but they are very different. A100 performance is now 40% faster with a small hit for other GPUs.
------
i love the aesthetic of the anon twitter kids who shitpost about ai all day

they are bringing sf energy back
------
AI is nothing like nuclear bombs. It’s either like the printing press (which is why journalists are afraid) or it’s like photosynthesis.
------
im thinkin.................... e acc is based
------
if you spoke spoke then what grass fed beef?
------
how have we not invented a battery that doesn't run out yet?
------
our culture's reverence for experts is a mistake
------
Just had an awful twitter space and talked shit for three hours. Met 
@josh_rees
 he is a Sydney VC that runs underground raves but has the local gang colluding with the police to crash his parties because he is taking too much of their business
------
I hear people, and sometimes myself, come up with these little stories about why they’re not as talented as they could be. Meanwhile the extremely talented people I know are like, “Ya I did heroin for 4 years”
------
damn i love custom instructions
------
watch barbie? or watch ben shapiro's review at 2x speed?
------
This guy wrote custom software that takes a 3D scan and then creates a 3D printable design of a custom face mask
------
This is one of the coolest projects I’ve ever seen in VR

GO GIVE THEM SOME LOVE!

https://youtu.be/B_Q0SbGr7wE
------
3D scan software really has come a long way
------
Early Torvalds email announcing his hobby project, “just a hobby, won’t be big and professional like gnu”
------
Introducing DialogStudio, the largest and most diverse dialogue dataset collection with diverse goals （e.g. task-oriented, open-domain, NLU, etc.) and different domains (e.g. finance, insurance software, movie, etc.)
http://github.com/salesforce/DialogStudio… 
https://huggingface.co/datasets/Salesforce/dialogstudio…
#NLP #AI
------
Sample enough times and you’ll get the model to produce coherent/correct code. Using n=10 here for 10 samples and picking the best one as the final instruction/response
------
i am become normie, goer of movies
------
sometimes i need to just like
look at my empty email inbox
and reaffirm to myself that yes, in fact, i have nothing urgent to attend to
------
Happy about the announcement of another awesome project I got the privilege to work on, but was mostly made by the great work of 
@dmayhem93
, FreeWilly 1 & 2! 

The dataset is 10% Orca paper replication! 

Thanks to 
@StabilityAI
 this dataset was able to be trained on Llama2-70b!!
------
Introducing FreeWilly1 and FreeWilly2 - The latest groundbreaking LLMs from Stability AI's and @carperai  lab! 
Open access and remarkable versatility. 
#AI #LLMs #OpenAccess  #StabilityAI 
https://bit.ly/3q1cCx2
------
finally getting what indiepreneur folks are saying about not being able to balance marketing & shipping. marketing really does take a lot of time. for example, i use twitter about 5 hours a day because i do it for marketing
------
not because it's extremely fun haha
------
Postman is the worst product on the face of the planet and its not even performant on top of that
------
can't believe they solved the car jacking problem by making every car just drive itself so you don't have to park anywhere
------
KEYWORD VOLUNTARY
------
THE WEST IS THE BEST BECAUSE IT IS FREE TO BEE
------
Announcing Nous Hermes Llama 2 13b!

This new version of Hermes, trained on Llama 2, has 4k context, and beats the benchmarks of original Hermes on every one we tested, including GPT4All benchmarks, BigBench, and AGIEval.

We have FP16, GGML, and GPTQ weights available!
------
Nous-Hermes-Llama2-13b weights have been released.

The model beats its previous benchmarks across the board, with the same dataset + hyperparameters.
4k context.  GGML + GPTQ versions ready.

Thanks to our sponsor @RedmondAI for the generous compute.  

https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b…
------
Learning about LoRAs | Talk
------
Hermes has completed training, and benchmarks across all our recent models are running. Will likely have an announcement tomorrow 
------
Reminder than LLaMa (v1) got released in February 2023 and already has 653 citations. Many works simply wouldn't have existed if this technology wasn't made available in the open.

So huge kudos to 
@MetaAI
 for releasing these models, enabling people to hack on them, improve them
------
knowledge work chat assistant is a local minimum that will last a few years tops. companies racing to make their own language model chat assistant are missing the point
------
basically it's hard to teach humans new tricks. it turns out it's very simple to teach machines new tricks
------
the year of the virtual linux desktop is upon us
https://youtube.com/watch?v=sXqtO3tWIaw…
------
Give Joe feedback on the chatgpt custom instructions feature 
------
I built this. Let me know what you think.
------
instead of using GPT4 and setting up all the infra, with the plan of using that data to fine tune a model, you should just mechanical turk it and wrap yourself in a function call. /v1/kache/completions

after running 5 jobs mturk style you'll have enough training data for a LoRA
------
this will actually take you less time than prompt engineering & evaling
------
(small task specific things)
------
I hear people, and sometimes myself, come up with these little stories about why they’re not as talented as they could be. Meanwhile the extremely talented people I know are like, “Ya I did heroin for 4 years”
------
damn i love custom instructions
------
watch barbie? or watch ben shapiro's review at 2x speed?
------
This guy wrote custom software that takes a 3D scan and then creates a 3D printable design of a custom face mask
------
This is one of the coolest projects I’ve ever seen in VR

GO GIVE THEM SOME LOVE!

https://youtu.be/B_Q0SbGr7wE
------
3D scan software really has come a long way
------
Early Torvalds email announcing his hobby project, “just a hobby, won’t be big and professional like gnu”
------
Introducing DialogStudio, the largest and most diverse dialogue dataset collection with diverse goals （e.g. task-oriented, open-domain, NLU, etc.) and different domains (e.g. finance, insurance software, movie, etc.)
http://github.com/salesforce/DialogStudio… 
https://huggingface.co/datasets/Salesforce/dialogstudio…
#NLP #AI
------
Sample enough times and you’ll get the model to produce coherent/correct code. Using n=10 here for 10 samples and picking the best one as the final instruction/response
------
i am become normie, goer of movies
------
sometimes i need to just like
look at my empty email inbox
and reaffirm to myself that yes, in fact, i have nothing urgent to attend to
------
Happy about the announcement of another awesome project I got the privilege to work on, but was mostly made by the great work of 
@dmayhem93
, FreeWilly 1 & 2! 

The dataset is 10% Orca paper replication! 

Thanks to 
@StabilityAI
 this dataset was able to be trained on Llama2-70b!!
------
Introducing FreeWilly1 and FreeWilly2 - The latest groundbreaking LLMs from Stability AI's and @carperai  lab! 
Open access and remarkable versatility. 
#AI #LLMs #OpenAccess  #StabilityAI 
https://bit.ly/3q1cCx2
------
finally getting what indiepreneur folks are saying about not being able to balance marketing & shipping. marketing really does take a lot of time. for example, i use twitter about 5 hours a day because i do it for marketing
------
not because it's extremely fun haha
------
Postman is the worst product on the face of the planet and its not even performant on top of that
------
can't believe they solved the car jacking problem by making every car just drive itself so you don't have to park anywhere
------
KEYWORD VOLUNTARY
------
THE WEST IS THE BEST BECAUSE IT IS FREE TO BEE
------
Announcing Nous Hermes Llama 2 13b!

This new version of Hermes, trained on Llama 2, has 4k context, and beats the benchmarks of original Hermes on every one we tested, including GPT4All benchmarks, BigBench, and AGIEval.

We have FP16, GGML, and GPTQ weights available!
------
Nous-Hermes-Llama2-13b weights have been released.

The model beats its previous benchmarks across the board, with the same dataset + hyperparameters.
4k context.  GGML + GPTQ versions ready.

Thanks to our sponsor @RedmondAI for the generous compute.  

https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b…
------
Learning about LoRAs | Talk
------
Hermes has completed training, and benchmarks across all our recent models are running. Will likely have an announcement tomorrow 
------
Reminder than LLaMa (v1) got released in February 2023 and already has 653 citations. Many works simply wouldn't have existed if this technology wasn't made available in the open.

So huge kudos to 
@MetaAI
 for releasing these models, enabling people to hack on them, improve them
------
knowledge work chat assistant is a local minimum that will last a few years tops. companies racing to make their own language model chat assistant are missing the point
------
basically it's hard to teach humans new tricks. it turns out it's very simple to teach machines new tricks
------
the year of the virtual linux desktop is upon us
https://youtube.com/watch?v=sXqtO3tWIaw…
------
Give Joe feedback on the chatgpt custom instructions feature 
------
I built this. Let me know what you think.
------
instead of using GPT4 and setting up all the infra, with the plan of using that data to fine tune a model, you should just mechanical turk it and wrap yourself in a function call. /v1/kache/completions

after running 5 jobs mturk style you'll have enough training data for a LoRA
------
this will actually take you less time than prompt engineering & evaling
------
(small task specific things)
------
gm
------
SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models

Contains many college-level free response problems in math, chemistry, physics and CS.

repo: https://github.com/mandyyyyii/scibench…
abs: https://arxiv.org/abs/2307.10635
------
intelligence is an emergent property of physics
------
Flash Attention 2 increases inference speed 4x and halves memory usage. It's quite phenomenal. We're working on general modeling code that any model can use 
------
Llama 2 70B-chat running on the Petals network at ~6 tokens per second https://chat.petals.dev
------
the government is held up by a tiny portion of heros that keep it running out of nothing but responsibility
------
canada
------
watch barbie? or watch ben shapiro's review at 2x speed?
------
This guy wrote custom software that takes a 3D scan and then creates a 3D printable design of a custom face mask
------
This is one of the coolest projects I’ve ever seen in VR

GO GIVE THEM SOME LOVE!

https://youtu.be/B_Q0SbGr7wE
------
3D scan software really has come a long way
------
Early Torvalds email announcing his hobby project, “just a hobby, won’t be big and professional like gnu”
------
Introducing DialogStudio, the largest and most diverse dialogue dataset collection with diverse goals （e.g. task-oriented, open-domain, NLU, etc.) and different domains (e.g. finance, insurance software, movie, etc.)
http://github.com/salesforce/DialogStudio… 
https://huggingface.co/datasets/Salesforce/dialogstudio…
#NLP #AI
------
Sample enough times and you’ll get the model to produce coherent/correct code. Using n=10 here for 10 samples and picking the best one as the final instruction/response
------
i am become normie, goer of movies
------
sometimes i need to just like
look at my empty email inbox
and reaffirm to myself that yes, in fact, i have nothing urgent to attend to
------
Happy about the announcement of another awesome project I got the privilege to work on, but was mostly made by the great work of 
@dmayhem93
, FreeWilly 1 & 2! 

The dataset is 10% Orca paper replication! 

Thanks to 
@StabilityAI
 this dataset was able to be trained on Llama2-70b!!
------
Introducing FreeWilly1 and FreeWilly2 - The latest groundbreaking LLMs from Stability AI's and @carperai  lab! 
Open access and remarkable versatility. 
#AI #LLMs #OpenAccess  #StabilityAI 
https://bit.ly/3q1cCx2
------
finally getting what indiepreneur folks are saying about not being able to balance marketing & shipping. marketing really does take a lot of time. for example, i use twitter about 5 hours a day because i do it for marketing
------
not because it's extremely fun haha
------
Postman is the worst product on the face of the planet and its not even performant on top of that
------
can't believe they solved the car jacking problem by making every car just drive itself so you don't have to park anywhere
------
KEYWORD VOLUNTARY
------
THE WEST IS THE BEST BECAUSE IT IS FREE TO BEE
------
Announcing Nous Hermes Llama 2 13b!

This new version of Hermes, trained on Llama 2, has 4k context, and beats the benchmarks of original Hermes on every one we tested, including GPT4All benchmarks, BigBench, and AGIEval.

We have FP16, GGML, and GPTQ weights available!
------
Nous-Hermes-Llama2-13b weights have been released.

The model beats its previous benchmarks across the board, with the same dataset + hyperparameters.
4k context.  GGML + GPTQ versions ready.

Thanks to our sponsor @RedmondAI for the generous compute.  

https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b…
------
Learning about LoRAs | Talk
------
Hermes has completed training, and benchmarks across all our recent models are running. Will likely have an announcement tomorrow 
------
Reminder than LLaMa (v1) got released in February 2023 and already has 653 citations. Many works simply wouldn't have existed if this technology wasn't made available in the open.

So huge kudos to 
@MetaAI
 for releasing these models, enabling people to hack on them, improve them
------
knowledge work chat assistant is a local minimum that will last a few years tops. companies racing to make their own language model chat assistant are missing the point
------
basically it's hard to teach humans new tricks. it turns out it's very simple to teach machines new tricks
------
the year of the virtual linux desktop is upon us
https://youtube.com/watch?v=sXqtO3tWIaw…
------
Give Joe feedback on the chatgpt custom instructions feature 
------
I built this. Let me know what you think.
------
instead of using GPT4 and setting up all the infra, with the plan of using that data to fine tune a model, you should just mechanical turk it and wrap yourself in a function call. /v1/kache/completions

after running 5 jobs mturk style you'll have enough training data for a LoRA
------
this will actually take you less time than prompt engineering & evaling
------
(small task specific things)
------
gm
------
SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models

Contains many college-level free response problems in math, chemistry, physics and CS.

repo: https://github.com/mandyyyyii/scibench…
abs: https://arxiv.org/abs/2307.10635
------
intelligence is an emergent property of physics
------
Flash Attention 2 increases inference speed 4x and halves memory usage. It's quite phenomenal. We're working on general modeling code that any model can use 
------
Llama 2 70B-chat running on the Petals network at ~6 tokens per second https://chat.petals.dev
------
the government is held up by a tiny portion of heros that keep it running out of nothing but responsibility
------
canada
------
very good system prompt s/o essen
------
Sometimes ChatGPT is so incredibly useful that it still gives me a pause. E.g. today around data science, pandas, matplotlib, I just asked for some analysis in 3 sentences and the code just streamed out. It's too easy and feels like cheating. It should have taken hours.
------
test time
If you're using custom instructions; show me what the following query generates for you
"With unix, how do I take a file and then find unique entries?"
------
NEW VIDEO! One of the biggest leaks related to Valve's Deckard HMD was the reliance on the Steam Deck compositor known as Gamescope

We finally got it running in Steam VR! This is a big deal and I wanted to explain in full detail why that is
------
Introducing DialogStudio, the largest and most diverse dialogue dataset collection with diverse goals （e.g. task-oriented, open-domain, NLU, etc.) and different domains (e.g. finance, insurance software, movie, etc.)
http://github.com/salesforce/DialogStudio… 
https://huggingface.co/datasets/Salesforce/dialogstudio…
#NLP #AI
------
Sample enough times and you’ll get the model to produce coherent/correct code. Using n=10 here for 10 samples and picking the best one as the final instruction/response
------
i am become normie, goer of movies
------
sometimes i need to just like
look at my empty email inbox
and reaffirm to myself that yes, in fact, i have nothing urgent to attend to
------
Happy about the announcement of another awesome project I got the privilege to work on, but was mostly made by the great work of 
@dmayhem93
, FreeWilly 1 & 2! 

The dataset is 10% Orca paper replication! 

Thanks to 
@StabilityAI
 this dataset was able to be trained on Llama2-70b!!
------
Introducing FreeWilly1 and FreeWilly2 - The latest groundbreaking LLMs from Stability AI's and @carperai  lab! 
Open access and remarkable versatility. 
#AI #LLMs #OpenAccess  #StabilityAI 
https://bit.ly/3q1cCx2
------
finally getting what indiepreneur folks are saying about not being able to balance marketing & shipping. marketing really does take a lot of time. for example, i use twitter about 5 hours a day because i do it for marketing
------
not because it's extremely fun haha
------
Postman is the worst product on the face of the planet and its not even performant on top of that
------
can't believe they solved the car jacking problem by making every car just drive itself so you don't have to park anywhere
------
KEYWORD VOLUNTARY
------
THE WEST IS THE BEST BECAUSE IT IS FREE TO BEE
------
Announcing Nous Hermes Llama 2 13b!

This new version of Hermes, trained on Llama 2, has 4k context, and beats the benchmarks of original Hermes on every one we tested, including GPT4All benchmarks, BigBench, and AGIEval.

We have FP16, GGML, and GPTQ weights available!
------
Nous-Hermes-Llama2-13b weights have been released.

The model beats its previous benchmarks across the board, with the same dataset + hyperparameters.
4k context.  GGML + GPTQ versions ready.

Thanks to our sponsor @RedmondAI for the generous compute.  

https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b…
------
Learning about LoRAs | Talk
------
Hermes has completed training, and benchmarks across all our recent models are running. Will likely have an announcement tomorrow 
------
Reminder than LLaMa (v1) got released in February 2023 and already has 653 citations. Many works simply wouldn't have existed if this technology wasn't made available in the open.

So huge kudos to 
@MetaAI
 for releasing these models, enabling people to hack on them, improve them
------
knowledge work chat assistant is a local minimum that will last a few years tops. companies racing to make their own language model chat assistant are missing the point
------
basically it's hard to teach humans new tricks. it turns out it's very simple to teach machines new tricks
------
the year of the virtual linux desktop is upon us
https://youtube.com/watch?v=sXqtO3tWIaw…
------
Give Joe feedback on the chatgpt custom instructions feature 
------
I built this. Let me know what you think.
------
instead of using GPT4 and setting up all the infra, with the plan of using that data to fine tune a model, you should just mechanical turk it and wrap yourself in a function call. /v1/kache/completions

after running 5 jobs mturk style you'll have enough training data for a LoRA
------
this will actually take you less time than prompt engineering & evaling
------
(small task specific things)
------
gm
------
SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models

Contains many college-level free response problems in math, chemistry, physics and CS.

repo: https://github.com/mandyyyyii/scibench…
abs: https://arxiv.org/abs/2307.10635
------
intelligence is an emergent property of physics
------
Flash Attention 2 increases inference speed 4x and halves memory usage. It's quite phenomenal. We're working on general modeling code that any model can use 
------
Llama 2 70B-chat running on the Petals network at ~6 tokens per second https://chat.petals.dev
------
the government is held up by a tiny portion of heros that keep it running out of nothing but responsibility
------
canada
------
very good system prompt s/o essen
------
Sometimes ChatGPT is so incredibly useful that it still gives me a pause. E.g. today around data science, pandas, matplotlib, I just asked for some analysis in 3 sentences and the code just streamed out. It's too easy and feels like cheating. It should have taken hours.
------
test time
If you're using custom instructions; show me what the following query generates for you
"With unix, how do I take a file and then find unique entries?"
------
NEW VIDEO! One of the biggest leaks related to Valve's Deckard HMD was the reliance on the Steam Deck compositor known as Gamescope

We finally got it running in Steam VR! This is a big deal and I wanted to explain in full detail why that is
------
I don't see how any closed source provider catches up with openai. they've won
------
every co has deep staff ratio
openai's got more deep staff than not (from impressions)
e.g. compare google. how much of google are deep google, ratio wise? not much
and then theres quality
deep oai staff are on a war path
an actual war path
i am so astounded at what they've done
------
even if someone scoops, do you think deep oai staff are going to give up?
they are on a war path
------
KEYWORD VOLUNTARY
------
THE WEST IS THE BEST BECAUSE IT IS FREE TO BEE
------
Announcing Nous Hermes Llama 2 13b!

This new version of Hermes, trained on Llama 2, has 4k context, and beats the benchmarks of original Hermes on every one we tested, including GPT4All benchmarks, BigBench, and AGIEval.

We have FP16, GGML, and GPTQ weights available!
------
Nous-Hermes-Llama2-13b weights have been released.

The model beats its previous benchmarks across the board, with the same dataset + hyperparameters.
4k context.  GGML + GPTQ versions ready.

Thanks to our sponsor @RedmondAI for the generous compute.  

https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b…
------
Learning about LoRAs | Talk
------
Hermes has completed training, and benchmarks across all our recent models are running. Will likely have an announcement tomorrow 
------
Reminder than LLaMa (v1) got released in February 2023 and already has 653 citations. Many works simply wouldn't have existed if this technology wasn't made available in the open.

So huge kudos to 
@MetaAI
 for releasing these models, enabling people to hack on them, improve them
------
knowledge work chat assistant is a local minimum that will last a few years tops. companies racing to make their own language model chat assistant are missing the point
------
basically it's hard to teach humans new tricks. it turns out it's very simple to teach machines new tricks
------
the year of the virtual linux desktop is upon us
https://youtube.com/watch?v=sXqtO3tWIaw…
------
Give Joe feedback on the chatgpt custom instructions feature 
------
I built this. Let me know what you think.
------
instead of using GPT4 and setting up all the infra, with the plan of using that data to fine tune a model, you should just mechanical turk it and wrap yourself in a function call. /v1/kache/completions

after running 5 jobs mturk style you'll have enough training data for a LoRA
------
this will actually take you less time than prompt engineering & evaling
------
(small task specific things)
------
gm
------
SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models

Contains many college-level free response problems in math, chemistry, physics and CS.

repo: https://github.com/mandyyyyii/scibench…
abs: https://arxiv.org/abs/2307.10635
------
intelligence is an emergent property of physics
------
Flash Attention 2 increases inference speed 4x and halves memory usage. It's quite phenomenal. We're working on general modeling code that any model can use 
------
Llama 2 70B-chat running on the Petals network at ~6 tokens per second https://chat.petals.dev
------
the government is held up by a tiny portion of heros that keep it running out of nothing but responsibility
------
canada
------
very good system prompt s/o essen
------
Sometimes ChatGPT is so incredibly useful that it still gives me a pause. E.g. today around data science, pandas, matplotlib, I just asked for some analysis in 3 sentences and the code just streamed out. It's too easy and feels like cheating. It should have taken hours.
------
test time
If you're using custom instructions; show me what the following query generates for you
"With unix, how do I take a file and then find unique entries?"
------
NEW VIDEO! One of the biggest leaks related to Valve's Deckard HMD was the reliance on the Steam Deck compositor known as Gamescope

We finally got it running in Steam VR! This is a big deal and I wanted to explain in full detail why that is
------
I don't see how any closed source provider catches up with openai. they've won
------
every co has deep staff ratio
openai's got more deep staff than not (from impressions)
e.g. compare google. how much of google are deep google, ratio wise? not much
and then theres quality
deep oai staff are on a war path
an actual war path
i am so astounded at what they've done
------
even if someone scoops, do you think deep oai staff are going to give up?
they are on a war path
------
an elaboration on why i think this
every company has deep staff
e.g. there is deep facebook
deep google
you can tell who's deep staff just by talking to them
their incentives are aligned, they are top percentile, and they have been giving it
------
the greatest sin of silicon valley hubris sufferers is thinking that we can control technology
------
Remember, Marcus Aurelius has already absolved you of the duty of having a take
------
re what i was saying yesterday about vision models being easier to evaluate and steer than text models
------
 Announcing FABRIC, a training-free method for using iterative feedback to improve the results of any Stable Diffusion model.

Instead of spending hours to find the right prompt, just click / to tell the model what exactly you want.

 Demo: https://huggingface.co/spaces/dvruette/fabric…
------
thread:
Prompting text to speech systems works
If you want them to be excited; start with "I'm excited! <your text here>"
that will always be amusing to me

cite: jbetker https://nonint.com/2022/04/25/tortoise-architectural-design-doc/…
------
Whats the most enthusiastic AI voice system?
------
if you guys get a chance, you should read james betker's blog
https://nonint.com
he has some new posts up
he is our type of guy
------
you probably don't actually want a chat model
------
leaving your house is an infohazard
------
basically it's hard to teach humans new tricks. it turns out it's very simple to teach machines new tricks
------
the year of the virtual linux desktop is upon us
https://youtube.com/watch?v=sXqtO3tWIaw…
------
Give Joe feedback on the chatgpt custom instructions feature 
------
I built this. Let me know what you think.
------
instead of using GPT4 and setting up all the infra, with the plan of using that data to fine tune a model, you should just mechanical turk it and wrap yourself in a function call. /v1/kache/completions

after running 5 jobs mturk style you'll have enough training data for a LoRA
------
this will actually take you less time than prompt engineering & evaling
------
(small task specific things)
------
gm
------
SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models

Contains many college-level free response problems in math, chemistry, physics and CS.

repo: https://github.com/mandyyyyii/scibench…
abs: https://arxiv.org/abs/2307.10635
------
intelligence is an emergent property of physics
------
Flash Attention 2 increases inference speed 4x and halves memory usage. It's quite phenomenal. We're working on general modeling code that any model can use 
------
Llama 2 70B-chat running on the Petals network at ~6 tokens per second https://chat.petals.dev
------
the government is held up by a tiny portion of heros that keep it running out of nothing but responsibility
------
canada
------
very good system prompt s/o essen
------
Sometimes ChatGPT is so incredibly useful that it still gives me a pause. E.g. today around data science, pandas, matplotlib, I just asked for some analysis in 3 sentences and the code just streamed out. It's too easy and feels like cheating. It should have taken hours.
------
test time
If you're using custom instructions; show me what the following query generates for you
"With unix, how do I take a file and then find unique entries?"
------
NEW VIDEO! One of the biggest leaks related to Valve's Deckard HMD was the reliance on the Steam Deck compositor known as Gamescope

We finally got it running in Steam VR! This is a big deal and I wanted to explain in full detail why that is
------
I don't see how any closed source provider catches up with openai. they've won
------
every co has deep staff ratio
openai's got more deep staff than not (from impressions)
e.g. compare google. how much of google are deep google, ratio wise? not much
and then theres quality
deep oai staff are on a war path
an actual war path
i am so astounded at what they've done
------
even if someone scoops, do you think deep oai staff are going to give up?
they are on a war path
------
an elaboration on why i think this
every company has deep staff
e.g. there is deep facebook
deep google
you can tell who's deep staff just by talking to them
their incentives are aligned, they are top percentile, and they have been giving it
------
the greatest sin of silicon valley hubris sufferers is thinking that we can control technology
------
Remember, Marcus Aurelius has already absolved you of the duty of having a take
------
re what i was saying yesterday about vision models being easier to evaluate and steer than text models
------
 Announcing FABRIC, a training-free method for using iterative feedback to improve the results of any Stable Diffusion model.

Instead of spending hours to find the right prompt, just click / to tell the model what exactly you want.

 Demo: https://huggingface.co/spaces/dvruette/fabric…
------
thread:
Prompting text to speech systems works
If you want them to be excited; start with "I'm excited! <your text here>"
that will always be amusing to me

cite: jbetker https://nonint.com/2022/04/25/tortoise-architectural-design-doc/…
------
Whats the most enthusiastic AI voice system?
------
if you guys get a chance, you should read james betker's blog
https://nonint.com
he has some new posts up
he is our type of guy
------
you probably don't actually want a chat model
------
leaving your house is an infohazard
------
broke: i can build this in a weekend
woke: i already built this in a weekend
------
i am become influencer, poster of noise
------
T_T
------
Guessing it's some combination of GPU allocation to this project (these days the raw number of GPUs in your cluster is more the "cost" than $ "cost"; very scarce, hence also NVIDIA ), and wanting to release earlier than later. (no first-hand knowledge though)
------
the higher layers are responsible for textures, the lower layers are responsible for higher-order features such as subject.
so you could do a model-merge with high layers from a style model and low layers from a subject model.
https://prompt-plus.github.io
------
SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models

Contains many college-level free response problems in math, chemistry, physics and CS.

repo: https://github.com/mandyyyyii/scibench…
abs: https://arxiv.org/abs/2307.10635
------
intelligence is an emergent property of physics
------
Flash Attention 2 increases inference speed 4x and halves memory usage. It's quite phenomenal. We're working on general modeling code that any model can use 
------
Llama 2 70B-chat running on the Petals network at ~6 tokens per second https://chat.petals.dev
------
the government is held up by a tiny portion of heros that keep it running out of nothing but responsibility
------
canada
------
very good system prompt s/o essen
------
Sometimes ChatGPT is so incredibly useful that it still gives me a pause. E.g. today around data science, pandas, matplotlib, I just asked for some analysis in 3 sentences and the code just streamed out. It's too easy and feels like cheating. It should have taken hours.
------
test time
If you're using custom instructions; show me what the following query generates for you
"With unix, how do I take a file and then find unique entries?"
------
NEW VIDEO! One of the biggest leaks related to Valve's Deckard HMD was the reliance on the Steam Deck compositor known as Gamescope

We finally got it running in Steam VR! This is a big deal and I wanted to explain in full detail why that is
------
I don't see how any closed source provider catches up with openai. they've won
------
every co has deep staff ratio
openai's got more deep staff than not (from impressions)
e.g. compare google. how much of google are deep google, ratio wise? not much
and then theres quality
deep oai staff are on a war path
an actual war path
i am so astounded at what they've done
------
even if someone scoops, do you think deep oai staff are going to give up?
they are on a war path
------
an elaboration on why i think this
every company has deep staff
e.g. there is deep facebook
deep google
you can tell who's deep staff just by talking to them
their incentives are aligned, they are top percentile, and they have been giving it
------
the greatest sin of silicon valley hubris sufferers is thinking that we can control technology
------
Remember, Marcus Aurelius has already absolved you of the duty of having a take
------
re what i was saying yesterday about vision models being easier to evaluate and steer than text models
------
 Announcing FABRIC, a training-free method for using iterative feedback to improve the results of any Stable Diffusion model.

Instead of spending hours to find the right prompt, just click / to tell the model what exactly you want.

 Demo: https://huggingface.co/spaces/dvruette/fabric…
------
thread:
Prompting text to speech systems works
If you want them to be excited; start with "I'm excited! <your text here>"
that will always be amusing to me

cite: jbetker https://nonint.com/2022/04/25/tortoise-architectural-design-doc/…
------
Whats the most enthusiastic AI voice system?
------
if you guys get a chance, you should read james betker's blog
https://nonint.com
he has some new posts up
he is our type of guy
------
you probably don't actually want a chat model
------
leaving your house is an infohazard
------
broke: i can build this in a weekend
woke: i already built this in a weekend
------
i am become influencer, poster of noise
------
T_T
------
Guessing it's some combination of GPU allocation to this project (these days the raw number of GPUs in your cluster is more the "cost" than $ "cost"; very scarce, hence also NVIDIA ), and wanting to release earlier than later. (no first-hand knowledge though)
------
the higher layers are responsible for textures, the lower layers are responsible for higher-order features such as subject.
so you could do a model-merge with high layers from a style model and low layers from a subject model.
https://prompt-plus.github.io
------
RLHF is just capabilities research in disguise, with the added benefit of reducing brand risk
------
helpful, and "safe" are two separate annotations
it's capabilities research
it's abstractly figuring out the best way to turn the entropic bag of beans into something that acts a certain way
any way
------
very good system prompt s/o essen
------
Sometimes ChatGPT is so incredibly useful that it still gives me a pause. E.g. today around data science, pandas, matplotlib, I just asked for some analysis in 3 sentences and the code just streamed out. It's too easy and feels like cheating. It should have taken hours.
------
test time
If you're using custom instructions; show me what the following query generates for you
"With unix, how do I take a file and then find unique entries?"
------
NEW VIDEO! One of the biggest leaks related to Valve's Deckard HMD was the reliance on the Steam Deck compositor known as Gamescope

We finally got it running in Steam VR! This is a big deal and I wanted to explain in full detail why that is
------
I don't see how any closed source provider catches up with openai. they've won
------
every co has deep staff ratio
openai's got more deep staff than not (from impressions)
e.g. compare google. how much of google are deep google, ratio wise? not much
and then theres quality
deep oai staff are on a war path
an actual war path
i am so astounded at what they've done
------
even if someone scoops, do you think deep oai staff are going to give up?
they are on a war path
------
an elaboration on why i think this
every company has deep staff
e.g. there is deep facebook
deep google
you can tell who's deep staff just by talking to them
their incentives are aligned, they are top percentile, and they have been giving it
------
the greatest sin of silicon valley hubris sufferers is thinking that we can control technology
------
Remember, Marcus Aurelius has already absolved you of the duty of having a take
------
re what i was saying yesterday about vision models being easier to evaluate and steer than text models
------
 Announcing FABRIC, a training-free method for using iterative feedback to improve the results of any Stable Diffusion model.

Instead of spending hours to find the right prompt, just click / to tell the model what exactly you want.

 Demo: https://huggingface.co/spaces/dvruette/fabric…
------
thread:
Prompting text to speech systems works
If you want them to be excited; start with "I'm excited! <your text here>"
that will always be amusing to me

cite: jbetker https://nonint.com/2022/04/25/tortoise-architectural-design-doc/…
------
Whats the most enthusiastic AI voice system?
------
if you guys get a chance, you should read james betker's blog
https://nonint.com
he has some new posts up
he is our type of guy
------
you probably don't actually want a chat model
------
leaving your house is an infohazard
------
broke: i can build this in a weekend
woke: i already built this in a weekend
------
i am become influencer, poster of noise
------
T_T
------
Guessing it's some combination of GPU allocation to this project (these days the raw number of GPUs in your cluster is more the "cost" than $ "cost"; very scarce, hence also NVIDIA ), and wanting to release earlier than later. (no first-hand knowledge though)
------
the higher layers are responsible for textures, the lower layers are responsible for higher-order features such as subject.
so you could do a model-merge with high layers from a style model and low layers from a subject model.
https://prompt-plus.github.io
------
RLHF is just capabilities research in disguise, with the added benefit of reducing brand risk
------
helpful, and "safe" are two separate annotations
it's capabilities research
it's abstractly figuring out the best way to turn the entropic bag of beans into something that acts a certain way
any way
------
>block merging
people selectively pick blocks out the stable diffusion UNets, and mix them together with custom recepies
from an anonymous rentry user, citing kohya_ss

"you can change the ratio between models on a per-layer (or 'block') basis, giving you much more fine control"
------
probably the reason image models seem to get so much more public experimentation is because it's so much easier to evaluate visual outputs

SD hackers merge models different ways, and then observing which specific recipe turns out the best

that loop doesn't work with text
picrel
------
cat
------
Really need to add some caveats here around:

1) having access to blocks of 2-4k GPUs that can sustain the throughput quoted in llama2

2) deriving cost based on the lowest end of pricing for on-demand / spot capacity from providers (generally where there won't be pools of… Show more
------
EDIT: thanks to @appenz for pointing out a mistake on the 34B and 70B parameter models 

Costs below, assuming $1.50 / A100 from @LambdaAPI:
- the 7B model cost $276,480
- the 13B model cost $552,960
- the 34B model cost $1.56M
- the 70B model cost $2.6M twitter.com/moinnadeem/sta…
------
being canadian is the worst
------
elon can i get a misinfo badge on this bad boy thanks
------
Android in the Wild: A Large-Scale Dataset for Android Device Control

Presents a massive dataset for device-control research, which contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. 

repo:… Show more
------
Absolutely dope #AIart transformation of 
@marcrebillet
  with 
@devdef
's #stablediffusion #warpfusion + a few other tools (see OP for workflow and links to tutorials), by redditor AthleteEducational63: https://reddit.com/r/StableDiffusion/comments/153juu9/marc_rebillet_diffused/…
------
I don't see how any closed source provider catches up with openai. they've won
------
every co has deep staff ratio
openai's got more deep staff than not (from impressions)
e.g. compare google. how much of google are deep google, ratio wise? not much
and then theres quality
deep oai staff are on a war path
an actual war path
i am so astounded at what they've done
------
even if someone scoops, do you think deep oai staff are going to give up?
they are on a war path
------
an elaboration on why i think this
every company has deep staff
e.g. there is deep facebook
deep google
you can tell who's deep staff just by talking to them
their incentives are aligned, they are top percentile, and they have been giving it
------
the greatest sin of silicon valley hubris sufferers is thinking that we can control technology
------
Remember, Marcus Aurelius has already absolved you of the duty of having a take
------
re what i was saying yesterday about vision models being easier to evaluate and steer than text models
------
 Announcing FABRIC, a training-free method for using iterative feedback to improve the results of any Stable Diffusion model.

Instead of spending hours to find the right prompt, just click / to tell the model what exactly you want.

 Demo: https://huggingface.co/spaces/dvruette/fabric…
------
thread:
Prompting text to speech systems works
If you want them to be excited; start with "I'm excited! <your text here>"
that will always be amusing to me

cite: jbetker https://nonint.com/2022/04/25/tortoise-architectural-design-doc/…
------
Whats the most enthusiastic AI voice system?
------
if you guys get a chance, you should read james betker's blog
https://nonint.com
he has some new posts up
he is our type of guy
------
you probably don't actually want a chat model
------
leaving your house is an infohazard
------
broke: i can build this in a weekend
woke: i already built this in a weekend
------
i am become influencer, poster of noise
------
T_T
------
Guessing it's some combination of GPU allocation to this project (these days the raw number of GPUs in your cluster is more the "cost" than $ "cost"; very scarce, hence also NVIDIA ), and wanting to release earlier than later. (no first-hand knowledge though)
------
the higher layers are responsible for textures, the lower layers are responsible for higher-order features such as subject.
so you could do a model-merge with high layers from a style model and low layers from a subject model.
https://prompt-plus.github.io
------
RLHF is just capabilities research in disguise, with the added benefit of reducing brand risk
------
helpful, and "safe" are two separate annotations
it's capabilities research
it's abstractly figuring out the best way to turn the entropic bag of beans into something that acts a certain way
any way
------
>block merging
people selectively pick blocks out the stable diffusion UNets, and mix them together with custom recepies
from an anonymous rentry user, citing kohya_ss

"you can change the ratio between models on a per-layer (or 'block') basis, giving you much more fine control"
------
probably the reason image models seem to get so much more public experimentation is because it's so much easier to evaluate visual outputs

SD hackers merge models different ways, and then observing which specific recipe turns out the best

that loop doesn't work with text
picrel
------
cat
------
Really need to add some caveats here around:

1) having access to blocks of 2-4k GPUs that can sustain the throughput quoted in llama2

2) deriving cost based on the lowest end of pricing for on-demand / spot capacity from providers (generally where there won't be pools of… Show more
------
EDIT: thanks to @appenz for pointing out a mistake on the 34B and 70B parameter models 

Costs below, assuming $1.50 / A100 from @LambdaAPI:
- the 7B model cost $276,480
- the 13B model cost $552,960
- the 34B model cost $1.56M
- the 70B model cost $2.6M twitter.com/moinnadeem/sta…
------
being canadian is the worst
------
elon can i get a misinfo badge on this bad boy thanks
------
Android in the Wild: A Large-Scale Dataset for Android Device Control

Presents a massive dataset for device-control research, which contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. 

repo:… Show more
------
Absolutely dope #AIart transformation of 
@marcrebillet
  with 
@devdef
's #stablediffusion #warpfusion + a few other tools (see OP for workflow and links to tutorials), by redditor AthleteEducational63: https://reddit.com/r/StableDiffusion/comments/153juu9/marc_rebillet_diffused/…
------
The startup shit is easy as fuck if you have something people actually want lmao.
------
conceptually, it should be possible to "debloat" a forward pass from python library bloat, right?
what i really want to see is a 40 line sequence of pytorch tensor smashes
not a 500 deep stack trace of python classes calling eachother with mixins
------
ahahaha okay let's go
------
after learning how transformers work (adding a forward hook and logging the forward source), it has come to my attention that making the training & inference processes share the same logic and state modulations was a terrible mistake and affront to god
------
Modern adult tech life is just picking which billionaire feudal empire you like best and commiting
------
ahahahahahahahah his godlike QA closed source model performance degraded from 200% above the average human to only 100% above the average human
------
meanwhile my 7B parameter IRC toy bot's performance hasn't degraded at all. it is state of the art in not degrading in performance (0% degradation (30% winograd to 30% winograd))
------
re what i was saying yesterday about vision models being easier to evaluate and steer than text models
------
 Announcing FABRIC, a training-free method for using iterative feedback to improve the results of any Stable Diffusion model.

Instead of spending hours to find the right prompt, just click / to tell the model what exactly you want.

 Demo: https://huggingface.co/spaces/dvruette/fabric…
------
thread:
Prompting text to speech systems works
If you want them to be excited; start with "I'm excited! <your text here>"
that will always be amusing to me

cite: jbetker https://nonint.com/2022/04/25/tortoise-architectural-design-doc/…
------
Whats the most enthusiastic AI voice system?
------
if you guys get a chance, you should read james betker's blog
https://nonint.com
he has some new posts up
he is our type of guy
------
you probably don't actually want a chat model
------
leaving your house is an infohazard
------
broke: i can build this in a weekend
woke: i already built this in a weekend
------
i am become influencer, poster of noise
------
T_T
------
Guessing it's some combination of GPU allocation to this project (these days the raw number of GPUs in your cluster is more the "cost" than $ "cost"; very scarce, hence also NVIDIA ), and wanting to release earlier than later. (no first-hand knowledge though)
------
the higher layers are responsible for textures, the lower layers are responsible for higher-order features such as subject.
so you could do a model-merge with high layers from a style model and low layers from a subject model.
https://prompt-plus.github.io
------
RLHF is just capabilities research in disguise, with the added benefit of reducing brand risk
------
helpful, and "safe" are two separate annotations
it's capabilities research
it's abstractly figuring out the best way to turn the entropic bag of beans into something that acts a certain way
any way
------
>block merging
people selectively pick blocks out the stable diffusion UNets, and mix them together with custom recepies
from an anonymous rentry user, citing kohya_ss

"you can change the ratio between models on a per-layer (or 'block') basis, giving you much more fine control"
------
probably the reason image models seem to get so much more public experimentation is because it's so much easier to evaluate visual outputs

SD hackers merge models different ways, and then observing which specific recipe turns out the best

that loop doesn't work with text
picrel
------
cat
------
Really need to add some caveats here around:

1) having access to blocks of 2-4k GPUs that can sustain the throughput quoted in llama2

2) deriving cost based on the lowest end of pricing for on-demand / spot capacity from providers (generally where there won't be pools of… Show more
------
EDIT: thanks to @appenz for pointing out a mistake on the 34B and 70B parameter models 

Costs below, assuming $1.50 / A100 from @LambdaAPI:
- the 7B model cost $276,480
- the 13B model cost $552,960
- the 34B model cost $1.56M
- the 70B model cost $2.6M twitter.com/moinnadeem/sta…
------
being canadian is the worst
------
elon can i get a misinfo badge on this bad boy thanks
------
Android in the Wild: A Large-Scale Dataset for Android Device Control

Presents a massive dataset for device-control research, which contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. 

repo:… Show more
------
Absolutely dope #AIart transformation of 
@marcrebillet
  with 
@devdef
's #stablediffusion #warpfusion + a few other tools (see OP for workflow and links to tutorials), by redditor AthleteEducational63: https://reddit.com/r/StableDiffusion/comments/153juu9/marc_rebillet_diffused/…
------
The startup shit is easy as fuck if you have something people actually want lmao.
------
conceptually, it should be possible to "debloat" a forward pass from python library bloat, right?
what i really want to see is a 40 line sequence of pytorch tensor smashes
not a 500 deep stack trace of python classes calling eachother with mixins
------
ahahaha okay let's go
------
after learning how transformers work (adding a forward hook and logging the forward source), it has come to my attention that making the training & inference processes share the same logic and state modulations was a terrible mistake and affront to god
------
Modern adult tech life is just picking which billionaire feudal empire you like best and commiting
------
ahahahahahahahah his godlike QA closed source model performance degraded from 200% above the average human to only 100% above the average human
------
meanwhile my 7B parameter IRC toy bot's performance hasn't degraded at all. it is state of the art in not degrading in performance (0% degradation (30% winograd to 30% winograd))
------
"why does gpt-4 seem like it has less gpt than it did a few months ago?"

everyone is short on inference gpus; degraded perf is better than showing users the door
------
gm its time to get actual real work done
------
the amount of advertising value alone for meta probably comps out all of the resource costs they had for LLaMa2
------
not to mention, becoming a talent attractor
it's a win win
------
broke: i can build this in a weekend
woke: i already built this in a weekend
------
i am become influencer, poster of noise
------
T_T
------
Guessing it's some combination of GPU allocation to this project (these days the raw number of GPUs in your cluster is more the "cost" than $ "cost"; very scarce, hence also NVIDIA ), and wanting to release earlier than later. (no first-hand knowledge though)
------
the higher layers are responsible for textures, the lower layers are responsible for higher-order features such as subject.
so you could do a model-merge with high layers from a style model and low layers from a subject model.
https://prompt-plus.github.io
------
RLHF is just capabilities research in disguise, with the added benefit of reducing brand risk
------
helpful, and "safe" are two separate annotations
it's capabilities research
it's abstractly figuring out the best way to turn the entropic bag of beans into something that acts a certain way
any way
------
>block merging
people selectively pick blocks out the stable diffusion UNets, and mix them together with custom recepies
from an anonymous rentry user, citing kohya_ss

"you can change the ratio between models on a per-layer (or 'block') basis, giving you much more fine control"
------
probably the reason image models seem to get so much more public experimentation is because it's so much easier to evaluate visual outputs

SD hackers merge models different ways, and then observing which specific recipe turns out the best

that loop doesn't work with text
picrel
------
cat
------
Really need to add some caveats here around:

1) having access to blocks of 2-4k GPUs that can sustain the throughput quoted in llama2

2) deriving cost based on the lowest end of pricing for on-demand / spot capacity from providers (generally where there won't be pools of… Show more
------
EDIT: thanks to @appenz for pointing out a mistake on the 34B and 70B parameter models 

Costs below, assuming $1.50 / A100 from @LambdaAPI:
- the 7B model cost $276,480
- the 13B model cost $552,960
- the 34B model cost $1.56M
- the 70B model cost $2.6M twitter.com/moinnadeem/sta…
------
being canadian is the worst
------
elon can i get a misinfo badge on this bad boy thanks
------
Android in the Wild: A Large-Scale Dataset for Android Device Control

Presents a massive dataset for device-control research, which contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. 

repo:… Show more
------
Absolutely dope #AIart transformation of 
@marcrebillet
  with 
@devdef
's #stablediffusion #warpfusion + a few other tools (see OP for workflow and links to tutorials), by redditor AthleteEducational63: https://reddit.com/r/StableDiffusion/comments/153juu9/marc_rebillet_diffused/…
------
The startup shit is easy as fuck if you have something people actually want lmao.
------
conceptually, it should be possible to "debloat" a forward pass from python library bloat, right?
what i really want to see is a 40 line sequence of pytorch tensor smashes
not a 500 deep stack trace of python classes calling eachother with mixins
------
ahahaha okay let's go
------
after learning how transformers work (adding a forward hook and logging the forward source), it has come to my attention that making the training & inference processes share the same logic and state modulations was a terrible mistake and affront to god
------
Modern adult tech life is just picking which billionaire feudal empire you like best and commiting
------
ahahahahahahahah his godlike QA closed source model performance degraded from 200% above the average human to only 100% above the average human
------
meanwhile my 7B parameter IRC toy bot's performance hasn't degraded at all. it is state of the art in not degrading in performance (0% degradation (30% winograd to 30% winograd))
------
"why does gpt-4 seem like it has less gpt than it did a few months ago?"

everyone is short on inference gpus; degraded perf is better than showing users the door
------
gm its time to get actual real work done
------
the amount of advertising value alone for meta probably comps out all of the resource costs they had for LLaMa2
------
not to mention, becoming a talent attractor
it's a win win
------
my wife kept on telling me this for the past like month. woman's intuition
------
Lots of people are wondering whether #GPT4 and #ChatGPT's performance has been changing over time, so Lingjiao Chen, @james_y_zou and I measured it. We found big changes including some large decreases in some problem-solving tasks: https://arxiv.org/pdf/2307.09009.pdf…
------
this is also not the actual chat interface, which is likely the lion's share of GPT-4 usage
which is also annoying to have change
a diff would be nice
------
(i am not saying changing software often is bad, it's a good thing)
just more transparency would be good
like
I need to know how much the CALM slider is being put at
i'm paying!!!!
------
"it's open source and available for commercial use by everyone except FAANG" is a good standard and good open source.
------
Ah yes, the well known "except you, FAANG" clause that's so common in *open source* licenses like GPL, MIT, BSD, Apache2, ...

Here I go again, this can't be for real lol  twitter.com/ylecun/status/…
------
There is no reason not to use LoRA
------
I'm thinkin....... we are still early
------
Confirmed.

70B LLaMA 2 easily training on a single GPU with 48GB

Green light on 70B 4-bit QLoRA & A6000.

Go wild.
------
MiDaS 3.1 vs Leia Inc. You be the judge! 

5% difference form their best result on δ1 metric over all datasets while having much more details on high resolution images. 

More to come! Sharper, better, bigger!
 4k and 8k images w/ full details, not limited to 512x512
------
the higher layers are responsible for textures, the lower layers are responsible for higher-order features such as subject.
so you could do a model-merge with high layers from a style model and low layers from a subject model.
https://prompt-plus.github.io
------
RLHF is just capabilities research in disguise, with the added benefit of reducing brand risk
------
helpful, and "safe" are two separate annotations
it's capabilities research
it's abstractly figuring out the best way to turn the entropic bag of beans into something that acts a certain way
any way
------
>block merging
people selectively pick blocks out the stable diffusion UNets, and mix them together with custom recepies
from an anonymous rentry user, citing kohya_ss

"you can change the ratio between models on a per-layer (or 'block') basis, giving you much more fine control"
------
probably the reason image models seem to get so much more public experimentation is because it's so much easier to evaluate visual outputs

SD hackers merge models different ways, and then observing which specific recipe turns out the best

that loop doesn't work with text
picrel
------
cat
------
Really need to add some caveats here around:

1) having access to blocks of 2-4k GPUs that can sustain the throughput quoted in llama2

2) deriving cost based on the lowest end of pricing for on-demand / spot capacity from providers (generally where there won't be pools of… Show more
------
EDIT: thanks to @appenz for pointing out a mistake on the 34B and 70B parameter models 

Costs below, assuming $1.50 / A100 from @LambdaAPI:
- the 7B model cost $276,480
- the 13B model cost $552,960
- the 34B model cost $1.56M
- the 70B model cost $2.6M twitter.com/moinnadeem/sta…
------
being canadian is the worst
------
elon can i get a misinfo badge on this bad boy thanks
------
Android in the Wild: A Large-Scale Dataset for Android Device Control

Presents a massive dataset for device-control research, which contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. 

repo:… Show more
------
Absolutely dope #AIart transformation of 
@marcrebillet
  with 
@devdef
's #stablediffusion #warpfusion + a few other tools (see OP for workflow and links to tutorials), by redditor AthleteEducational63: https://reddit.com/r/StableDiffusion/comments/153juu9/marc_rebillet_diffused/…
------
The startup shit is easy as fuck if you have something people actually want lmao.
------
conceptually, it should be possible to "debloat" a forward pass from python library bloat, right?
what i really want to see is a 40 line sequence of pytorch tensor smashes
not a 500 deep stack trace of python classes calling eachother with mixins
------
ahahaha okay let's go
------
after learning how transformers work (adding a forward hook and logging the forward source), it has come to my attention that making the training & inference processes share the same logic and state modulations was a terrible mistake and affront to god
------
Modern adult tech life is just picking which billionaire feudal empire you like best and commiting
------
ahahahahahahahah his godlike QA closed source model performance degraded from 200% above the average human to only 100% above the average human
------
meanwhile my 7B parameter IRC toy bot's performance hasn't degraded at all. it is state of the art in not degrading in performance (0% degradation (30% winograd to 30% winograd))
------
"why does gpt-4 seem like it has less gpt than it did a few months ago?"

everyone is short on inference gpus; degraded perf is better than showing users the door
------
gm its time to get actual real work done
------
the amount of advertising value alone for meta probably comps out all of the resource costs they had for LLaMa2
------
not to mention, becoming a talent attractor
it's a win win
------
my wife kept on telling me this for the past like month. woman's intuition
------
Lots of people are wondering whether #GPT4 and #ChatGPT's performance has been changing over time, so Lingjiao Chen, @james_y_zou and I measured it. We found big changes including some large decreases in some problem-solving tasks: https://arxiv.org/pdf/2307.09009.pdf…
------
this is also not the actual chat interface, which is likely the lion's share of GPT-4 usage
which is also annoying to have change
a diff would be nice
------
(i am not saying changing software often is bad, it's a good thing)
just more transparency would be good
like
I need to know how much the CALM slider is being put at
i'm paying!!!!
------
"it's open source and available for commercial use by everyone except FAANG" is a good standard and good open source.
------
Ah yes, the well known "except you, FAANG" clause that's so common in *open source* licenses like GPL, MIT, BSD, Apache2, ...

Here I go again, this can't be for real lol  twitter.com/ylecun/status/…
------
There is no reason not to use LoRA
------
I'm thinkin....... we are still early
------
Confirmed.

70B LLaMA 2 easily training on a single GPU with 48GB

Green light on 70B 4-bit QLoRA & A6000.

Go wild.
------
MiDaS 3.1 vs Leia Inc. You be the judge! 

5% difference form their best result on δ1 metric over all datasets while having much more details on high resolution images. 

More to come! Sharper, better, bigger!
 4k and 8k images w/ full details, not limited to 512x512
------
alignment overdose
------
Huge day indeed for AI and LLMs, congrats to Meta 
This is now the most capable LLM available directly as weights to anyone from researchers to companies.

The models look quite strong, e.g. Table 4 in the paper: MMLU is good to look at, the 70B model is just below GPT-3.5. But… Show more
------
This is huge: Llama-v2 is open source, with a license that authorizes commercial use!

This is going to change the landscape of the LLM market.
Llama-v2 is available on Microsoft Azure and will be available on AWS, Hugging Face and other providers

Pretrained and fine-tuned… Show more
------
llama 2 is likely the most important FOSS model release since stable diffusion: https://ai.meta.com/llama/
------
thread to dump reacctions in
------
cat
------
Really need to add some caveats here around:

1) having access to blocks of 2-4k GPUs that can sustain the throughput quoted in llama2

2) deriving cost based on the lowest end of pricing for on-demand / spot capacity from providers (generally where there won't be pools of… Show more
------
EDIT: thanks to @appenz for pointing out a mistake on the 34B and 70B parameter models 

Costs below, assuming $1.50 / A100 from @LambdaAPI:
- the 7B model cost $276,480
- the 13B model cost $552,960
- the 34B model cost $1.56M
- the 70B model cost $2.6M twitter.com/moinnadeem/sta…
------
being canadian is the worst
------
elon can i get a misinfo badge on this bad boy thanks
------
Android in the Wild: A Large-Scale Dataset for Android Device Control

Presents a massive dataset for device-control research, which contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. 

repo:… Show more
------
Absolutely dope #AIart transformation of 
@marcrebillet
  with 
@devdef
's #stablediffusion #warpfusion + a few other tools (see OP for workflow and links to tutorials), by redditor AthleteEducational63: https://reddit.com/r/StableDiffusion/comments/153juu9/marc_rebillet_diffused/…
------
The startup shit is easy as fuck if you have something people actually want lmao.
------
conceptually, it should be possible to "debloat" a forward pass from python library bloat, right?
what i really want to see is a 40 line sequence of pytorch tensor smashes
not a 500 deep stack trace of python classes calling eachother with mixins
------
ahahaha okay let's go
------
after learning how transformers work (adding a forward hook and logging the forward source), it has come to my attention that making the training & inference processes share the same logic and state modulations was a terrible mistake and affront to god
------
Modern adult tech life is just picking which billionaire feudal empire you like best and commiting
------
ahahahahahahahah his godlike QA closed source model performance degraded from 200% above the average human to only 100% above the average human
------
meanwhile my 7B parameter IRC toy bot's performance hasn't degraded at all. it is state of the art in not degrading in performance (0% degradation (30% winograd to 30% winograd))
------
"why does gpt-4 seem like it has less gpt than it did a few months ago?"

everyone is short on inference gpus; degraded perf is better than showing users the door
------
gm its time to get actual real work done
------
the amount of advertising value alone for meta probably comps out all of the resource costs they had for LLaMa2
------
not to mention, becoming a talent attractor
it's a win win
------
my wife kept on telling me this for the past like month. woman's intuition
------
Lots of people are wondering whether #GPT4 and #ChatGPT's performance has been changing over time, so Lingjiao Chen, @james_y_zou and I measured it. We found big changes including some large decreases in some problem-solving tasks: https://arxiv.org/pdf/2307.09009.pdf…
------
this is also not the actual chat interface, which is likely the lion's share of GPT-4 usage
which is also annoying to have change
a diff would be nice
------
(i am not saying changing software often is bad, it's a good thing)
just more transparency would be good
like
I need to know how much the CALM slider is being put at
i'm paying!!!!
------
"it's open source and available for commercial use by everyone except FAANG" is a good standard and good open source.
------
Ah yes, the well known "except you, FAANG" clause that's so common in *open source* licenses like GPL, MIT, BSD, Apache2, ...

Here I go again, this can't be for real lol  twitter.com/ylecun/status/…
------
There is no reason not to use LoRA
------
I'm thinkin....... we are still early
------
Confirmed.

70B LLaMA 2 easily training on a single GPU with 48GB

Green light on 70B 4-bit QLoRA & A6000.

Go wild.
------
MiDaS 3.1 vs Leia Inc. You be the judge! 

5% difference form their best result on δ1 metric over all datasets while having much more details on high resolution images. 

More to come! Sharper, better, bigger!
 4k and 8k images w/ full details, not limited to 512x512
------
alignment overdose
------
Huge day indeed for AI and LLMs, congrats to Meta 
This is now the most capable LLM available directly as weights to anyone from researchers to companies.

The models look quite strong, e.g. Table 4 in the paper: MMLU is good to look at, the 70B model is just below GPT-3.5. But… Show more
------
This is huge: Llama-v2 is open source, with a license that authorizes commercial use!

This is going to change the landscape of the LLM market.
Llama-v2 is available on Microsoft Azure and will be available on AWS, Hugging Face and other providers

Pretrained and fine-tuned… Show more
------
llama 2 is likely the most important FOSS model release since stable diffusion: https://ai.meta.com/llama/
------
thread to dump reacctions in
------
it can use tools its just like me.. its just like me...

(they really shouldn't have compared it with non fine tuned models, but good results)
------
lots of goodies in the appendix
go read the paper
i might have misunderstood some things in this thread
please correct me if you notice!
------
Absolutely dope #AIart transformation of 
@marcrebillet
  with 
@devdef
's #stablediffusion #warpfusion + a few other tools (see OP for workflow and links to tutorials), by redditor AthleteEducational63: https://reddit.com/r/StableDiffusion/comments/153juu9/marc_rebillet_diffused/…
------
The startup shit is easy as fuck if you have something people actually want lmao.
------
conceptually, it should be possible to "debloat" a forward pass from python library bloat, right?
what i really want to see is a 40 line sequence of pytorch tensor smashes
not a 500 deep stack trace of python classes calling eachother with mixins
------
ahahaha okay let's go
------
after learning how transformers work (adding a forward hook and logging the forward source), it has come to my attention that making the training & inference processes share the same logic and state modulations was a terrible mistake and affront to god
------
Modern adult tech life is just picking which billionaire feudal empire you like best and commiting
------
ahahahahahahahah his godlike QA closed source model performance degraded from 200% above the average human to only 100% above the average human
------
meanwhile my 7B parameter IRC toy bot's performance hasn't degraded at all. it is state of the art in not degrading in performance (0% degradation (30% winograd to 30% winograd))
------
"why does gpt-4 seem like it has less gpt than it did a few months ago?"

everyone is short on inference gpus; degraded perf is better than showing users the door
------
gm its time to get actual real work done
------
the amount of advertising value alone for meta probably comps out all of the resource costs they had for LLaMa2
------
not to mention, becoming a talent attractor
it's a win win
------
my wife kept on telling me this for the past like month. woman's intuition
------
Lots of people are wondering whether #GPT4 and #ChatGPT's performance has been changing over time, so Lingjiao Chen, @james_y_zou and I measured it. We found big changes including some large decreases in some problem-solving tasks: https://arxiv.org/pdf/2307.09009.pdf…
------
this is also not the actual chat interface, which is likely the lion's share of GPT-4 usage
which is also annoying to have change
a diff would be nice
------
(i am not saying changing software often is bad, it's a good thing)
just more transparency would be good
like
I need to know how much the CALM slider is being put at
i'm paying!!!!
------
"it's open source and available for commercial use by everyone except FAANG" is a good standard and good open source.
------
Ah yes, the well known "except you, FAANG" clause that's so common in *open source* licenses like GPL, MIT, BSD, Apache2, ...

Here I go again, this can't be for real lol  twitter.com/ylecun/status/…
------
There is no reason not to use LoRA
------
I'm thinkin....... we are still early
------
Confirmed.

70B LLaMA 2 easily training on a single GPU with 48GB

Green light on 70B 4-bit QLoRA & A6000.

Go wild.
------
MiDaS 3.1 vs Leia Inc. You be the judge! 

5% difference form their best result on δ1 metric over all datasets while having much more details on high resolution images. 

More to come! Sharper, better, bigger!
 4k and 8k images w/ full details, not limited to 512x512
------
alignment overdose
------
Huge day indeed for AI and LLMs, congrats to Meta 
This is now the most capable LLM available directly as weights to anyone from researchers to companies.

The models look quite strong, e.g. Table 4 in the paper: MMLU is good to look at, the 70B model is just below GPT-3.5. But… Show more
------
This is huge: Llama-v2 is open source, with a license that authorizes commercial use!

This is going to change the landscape of the LLM market.
Llama-v2 is available on Microsoft Azure and will be available on AWS, Hugging Face and other providers

Pretrained and fine-tuned… Show more
------
llama 2 is likely the most important FOSS model release since stable diffusion: https://ai.meta.com/llama/
------
thread to dump reacctions in
------
it can use tools its just like me.. its just like me...

(they really shouldn't have compared it with non fine tuned models, but good results)
------
lots of goodies in the appendix
go read the paper
i might have misunderstood some things in this thread
please correct me if you notice!
------
here's how you teach a model what time it is
1000 (only) SFT examples
------
Wish more details on the dataset side could've been released...

Looks like there was some form of PII redaction, and knowledge cutoff was sometime before November 2022.
------
@Gradio demo: https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI…
------
 
don't turn it off!!!!
------
that's a lot of linear algebra done fast
------
free my homie 34b, he's in LLM jail
------
ML builders on the way to filling out the LLaMa2 form
------
receipt
------
let the records show that I never doubted zuck, not even for a single moment
he was right. he is right. he continues to be right.
------
"why does gpt-4 seem like it has less gpt than it did a few months ago?"

everyone is short on inference gpus; degraded perf is better than showing users the door
------
gm its time to get actual real work done
------
the amount of advertising value alone for meta probably comps out all of the resource costs they had for LLaMa2
------
not to mention, becoming a talent attractor
it's a win win
------
my wife kept on telling me this for the past like month. woman's intuition
------
Lots of people are wondering whether #GPT4 and #ChatGPT's performance has been changing over time, so Lingjiao Chen, @james_y_zou and I measured it. We found big changes including some large decreases in some problem-solving tasks: https://arxiv.org/pdf/2307.09009.pdf…
------
this is also not the actual chat interface, which is likely the lion's share of GPT-4 usage
which is also annoying to have change
a diff would be nice
------
(i am not saying changing software often is bad, it's a good thing)
just more transparency would be good
like
I need to know how much the CALM slider is being put at
i'm paying!!!!
------
"it's open source and available for commercial use by everyone except FAANG" is a good standard and good open source.
------
Ah yes, the well known "except you, FAANG" clause that's so common in *open source* licenses like GPL, MIT, BSD, Apache2, ...

Here I go again, this can't be for real lol  twitter.com/ylecun/status/…
------
There is no reason not to use LoRA
------
I'm thinkin....... we are still early
------
Confirmed.

70B LLaMA 2 easily training on a single GPU with 48GB

Green light on 70B 4-bit QLoRA & A6000.

Go wild.
------
MiDaS 3.1 vs Leia Inc. You be the judge! 

5% difference form their best result on δ1 metric over all datasets while having much more details on high resolution images. 

More to come! Sharper, better, bigger!
 4k and 8k images w/ full details, not limited to 512x512
------
alignment overdose
------
Huge day indeed for AI and LLMs, congrats to Meta 
This is now the most capable LLM available directly as weights to anyone from researchers to companies.

The models look quite strong, e.g. Table 4 in the paper: MMLU is good to look at, the 70B model is just below GPT-3.5. But… Show more
------
This is huge: Llama-v2 is open source, with a license that authorizes commercial use!

This is going to change the landscape of the LLM market.
Llama-v2 is available on Microsoft Azure and will be available on AWS, Hugging Face and other providers

Pretrained and fine-tuned… Show more
------
llama 2 is likely the most important FOSS model release since stable diffusion: https://ai.meta.com/llama/
------
thread to dump reacctions in
------
it can use tools its just like me.. its just like me...

(they really shouldn't have compared it with non fine tuned models, but good results)
------
lots of goodies in the appendix
go read the paper
i might have misunderstood some things in this thread
please correct me if you notice!
------
here's how you teach a model what time it is
1000 (only) SFT examples
------
Wish more details on the dataset side could've been released...

Looks like there was some form of PII redaction, and knowledge cutoff was sometime before November 2022.
------
@Gradio demo: https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI…
------
 
don't turn it off!!!!
------
that's a lot of linear algebra done fast
------
free my homie 34b, he's in LLM jail
------
ML builders on the way to filling out the LLaMa2 form
------
receipt
------
let the records show that I never doubted zuck, not even for a single moment
he was right. he is right. he continues to be right.
------
Apologize to him right now
------
This is huge: Llama-v2 is open source, with a license that authorizes commercial use!

This is going to change the landscape of the LLM market.
Llama-v2 is available on Microsoft Azure and will be available on AWS, Hugging Face and other providers

Pretrained and fine-tuned… Show more
------
"hey yacine, hope you're having a good morning. I'd just like to call and thank you for making that meme of me talking about open source yesterday. yeah it like, changed my mind and i decided to make it open source."
------
Really great twitter space, thanks for helping me kill time while we waited for llama2 to come out! 

I'll try to do these every Tuesday, probably
------
claude's been the best speaker by far, but I gotta snooze
------
This is not really what it looks like and there is an important & nuanced backstory here — I made a video to explain: https://youtu.be/dQw4w9WgXcQ
------
Going through the YC directory for some inspiration, and no way @Replit did this...
------
"it's open source and available for commercial use by everyone except FAANG" is a good standard and good open source.
------
Ah yes, the well known "except you, FAANG" clause that's so common in *open source* licenses like GPL, MIT, BSD, Apache2, ...

Here I go again, this can't be for real lol  twitter.com/ylecun/status/…
------
There is no reason not to use LoRA
------
I'm thinkin....... we are still early
------
Confirmed.

70B LLaMA 2 easily training on a single GPU with 48GB

Green light on 70B 4-bit QLoRA & A6000.

Go wild.
------
MiDaS 3.1 vs Leia Inc. You be the judge! 

5% difference form their best result on δ1 metric over all datasets while having much more details on high resolution images. 

More to come! Sharper, better, bigger!
 4k and 8k images w/ full details, not limited to 512x512
------
alignment overdose
------
Huge day indeed for AI and LLMs, congrats to Meta 
This is now the most capable LLM available directly as weights to anyone from researchers to companies.

The models look quite strong, e.g. Table 4 in the paper: MMLU is good to look at, the 70B model is just below GPT-3.5. But… Show more
------
This is huge: Llama-v2 is open source, with a license that authorizes commercial use!

This is going to change the landscape of the LLM market.
Llama-v2 is available on Microsoft Azure and will be available on AWS, Hugging Face and other providers

Pretrained and fine-tuned… Show more
------
llama 2 is likely the most important FOSS model release since stable diffusion: https://ai.meta.com/llama/
------
thread to dump reacctions in
------
it can use tools its just like me.. its just like me...

(they really shouldn't have compared it with non fine tuned models, but good results)
------
lots of goodies in the appendix
go read the paper
i might have misunderstood some things in this thread
please correct me if you notice!
------
here's how you teach a model what time it is
1000 (only) SFT examples
------
Wish more details on the dataset side could've been released...

Looks like there was some form of PII redaction, and knowledge cutoff was sometime before November 2022.
------
@Gradio demo: https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI…
------
 
don't turn it off!!!!
------
that's a lot of linear algebra done fast
------
free my homie 34b, he's in LLM jail
------
ML builders on the way to filling out the LLaMa2 form
------
receipt
------
let the records show that I never doubted zuck, not even for a single moment
he was right. he is right. he continues to be right.
------
Apologize to him right now
------
This is huge: Llama-v2 is open source, with a license that authorizes commercial use!

This is going to change the landscape of the LLM market.
Llama-v2 is available on Microsoft Azure and will be available on AWS, Hugging Face and other providers

Pretrained and fine-tuned… Show more
------
"hey yacine, hope you're having a good morning. I'd just like to call and thank you for making that meme of me talking about open source yesterday. yeah it like, changed my mind and i decided to make it open source."
------
Really great twitter space, thanks for helping me kill time while we waited for llama2 to come out! 

I'll try to do these every Tuesday, probably
------
claude's been the best speaker by far, but I gotta snooze
------
This is not really what it looks like and there is an important & nuanced backstory here — I made a video to explain: https://youtu.be/dQw4w9WgXcQ
------
Going through the YC directory for some inspiration, and no way @Replit did this...
------
This is a sin
------
had dinner with two google employees and they got into a bragging competition about who works fewer hours, the guy @ 2 hrs/day for 500k tc won
------
Confirmed.

70B LLaMA 2 easily training on a single GPU with 48GB

Green light on 70B 4-bit QLoRA & A6000.

Go wild.
------
MiDaS 3.1 vs Leia Inc. You be the judge! 

5% difference form their best result on δ1 metric over all datasets while having much more details on high resolution images. 

More to come! Sharper, better, bigger!
 4k and 8k images w/ full details, not limited to 512x512
------
alignment overdose
------
Huge day indeed for AI and LLMs, congrats to Meta 
This is now the most capable LLM available directly as weights to anyone from researchers to companies.

The models look quite strong, e.g. Table 4 in the paper: MMLU is good to look at, the 70B model is just below GPT-3.5. But… Show more
------
This is huge: Llama-v2 is open source, with a license that authorizes commercial use!

This is going to change the landscape of the LLM market.
Llama-v2 is available on Microsoft Azure and will be available on AWS, Hugging Face and other providers

Pretrained and fine-tuned… Show more
------
llama 2 is likely the most important FOSS model release since stable diffusion: https://ai.meta.com/llama/
------
thread to dump reacctions in
------
it can use tools its just like me.. its just like me...

(they really shouldn't have compared it with non fine tuned models, but good results)
------
lots of goodies in the appendix
go read the paper
i might have misunderstood some things in this thread
please correct me if you notice!
------
here's how you teach a model what time it is
1000 (only) SFT examples
------
Wish more details on the dataset side could've been released...

Looks like there was some form of PII redaction, and knowledge cutoff was sometime before November 2022.
------
@Gradio demo: https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI…
------
free my homie 34b, he's in LLM jail
------
ML builders on the way to filling out the LLaMa2 form
------
receipt
------
let the records show that I never doubted zuck, not even for a single moment
he was right. he is right. he continues to be right.
------
Apologize to him right now
------
This is huge: Llama-v2 is open source, with a license that authorizes commercial use!

This is going to change the landscape of the LLM market.
Llama-v2 is available on Microsoft Azure and will be available on AWS, Hugging Face and other providers

Pretrained and fine-tuned… Show more
------
"hey yacine, hope you're having a good morning. I'd just like to call and thank you for making that meme of me talking about open source yesterday. yeah it like, changed my mind and i decided to make it open source."
------
Really great twitter space, thanks for helping me kill time while we waited for llama2 to come out! 

I'll try to do these every Tuesday, probably
------
claude's been the best speaker by far, but I gotta snooze
------
This is not really what it looks like and there is an important & nuanced backstory here — I made a video to explain: https://youtu.be/dQw4w9WgXcQ
------
Going through the YC directory for some inspiration, and no way @Replit did this...
------
This is a sin
------
had dinner with two google employees and they got into a bragging competition about who works fewer hours, the guy @ 2 hrs/day for 500k tc won
------
good morning let's make our beds
------
Retentive Network: A Successor to Transformer
for Large Language Models

Proposes RetNet as a foundation architecture for LLMs, simultaneously achieving training parallelism, low-cost inference, and good performance.

https://arxiv.org/abs/2307.08621
------
I love going to google images, searching for images of jesus for some midnight inspiration, and then getting nothing but ads for shitty printed art
------
a fly has somehow got into my computer. should i kick off a training run?
------
khajiit has gpt wrapper
if you have coin
------
LESS DATA
look
at
your data
50k samples? easily doable in a week
------
AlpaGasus: Training A Better Alpaca with Fewer Data

Significantly outperforms the original Alpaca and reaches 90% of davinci-003 w/ 5.7x faster training.

proj: https://lichang-chen.github.io/AlpaGasus/
abs: https://arxiv.org/abs/2307.08701
------
look!!!!!!!!!! look with your eyes
------
broke: prompt engineering
woke: parameter efficient fine tuning (better and cheaper)
here is a paper that confirms my bias
https://arxiv.org/abs/2205.05638
------
thread to dump reacctions in
------
it can use tools its just like me.. its just like me...

(they really shouldn't have compared it with non fine tuned models, but good results)
------
lots of goodies in the appendix
go read the paper
i might have misunderstood some things in this thread
please correct me if you notice!
------
here's how you teach a model what time it is
1000 (only) SFT examples
------
Wish more details on the dataset side could've been released...

Looks like there was some form of PII redaction, and knowledge cutoff was sometime before November 2022.
------
@Gradio demo: https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI…
------
free my homie 34b, he's in LLM jail
------
ML builders on the way to filling out the LLaMa2 form
------
receipt
------
let the records show that I never doubted zuck, not even for a single moment
he was right. he is right. he continues to be right.
------
Apologize to him right now
------
This is huge: Llama-v2 is open source, with a license that authorizes commercial use!

This is going to change the landscape of the LLM market.
Llama-v2 is available on Microsoft Azure and will be available on AWS, Hugging Face and other providers

Pretrained and fine-tuned… Show more
------
"hey yacine, hope you're having a good morning. I'd just like to call and thank you for making that meme of me talking about open source yesterday. yeah it like, changed my mind and i decided to make it open source."
------
Really great twitter space, thanks for helping me kill time while we waited for llama2 to come out! 

I'll try to do these every Tuesday, probably
------
claude's been the best speaker by far, but I gotta snooze
------
This is not really what it looks like and there is an important & nuanced backstory here — I made a video to explain: https://youtu.be/dQw4w9WgXcQ
------
Going through the YC directory for some inspiration, and no way @Replit did this...
------
This is a sin
------
had dinner with two google employees and they got into a bragging competition about who works fewer hours, the guy @ 2 hrs/day for 500k tc won
------
good morning let's make our beds
------
Retentive Network: A Successor to Transformer
for Large Language Models

Proposes RetNet as a foundation architecture for LLMs, simultaneously achieving training parallelism, low-cost inference, and good performance.

https://arxiv.org/abs/2307.08621
------
I love going to google images, searching for images of jesus for some midnight inspiration, and then getting nothing but ads for shitty printed art
------
a fly has somehow got into my computer. should i kick off a training run?
------
khajiit has gpt wrapper
if you have coin
------
LESS DATA
look
at
your data
50k samples? easily doable in a week
------
AlpaGasus: Training A Better Alpaca with Fewer Data

Significantly outperforms the original Alpaca and reaches 90% of davinci-003 w/ 5.7x faster training.

proj: https://lichang-chen.github.io/AlpaGasus/
abs: https://arxiv.org/abs/2307.08701
------
look!!!!!!!!!! look with your eyes
------
broke: prompt engineering
woke: parameter efficient fine tuning (better and cheaper)
here is a paper that confirms my bias
https://arxiv.org/abs/2205.05638
------
paper is 2022 btw (10 years old in ML time)
also get your loop down this is a PSA
get your training loop down 
get your labeling loop down
get it down
LoRA on 7b params takes 30 minutes
------
YES!!!! We are all going to make it!!! YES!!!!!!!!!!!
------
yessssssssssssssssss!!!!!!!!!!!!!!!!!!!!!!!!!!
------
expectation: kache steals OC from 4chan
reality: i am the OC on 4chan
------
"yeah coding is basically like blue collar work"
------
Government dooms citizenry to toothpaste and detergent ads and small businesses to bankruptcy.
------
It’s wild it’s considered a bad thing that small businesses should be able to find targeted customers for their products
------
here's how you teach a model what time it is
1000 (only) SFT examples
------
Wish more details on the dataset side could've been released...

Looks like there was some form of PII redaction, and knowledge cutoff was sometime before November 2022.
------
@Gradio demo: https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI…
------
free my homie 34b, he's in LLM jail
------
ML builders on the way to filling out the LLaMa2 form
------
receipt
------
let the records show that I never doubted zuck, not even for a single moment
he was right. he is right. he continues to be right.
------
Apologize to him right now
------
This is huge: Llama-v2 is open source, with a license that authorizes commercial use!

This is going to change the landscape of the LLM market.
Llama-v2 is available on Microsoft Azure and will be available on AWS, Hugging Face and other providers

Pretrained and fine-tuned… Show more
------
"hey yacine, hope you're having a good morning. I'd just like to call and thank you for making that meme of me talking about open source yesterday. yeah it like, changed my mind and i decided to make it open source."
------
Really great twitter space, thanks for helping me kill time while we waited for llama2 to come out! 

I'll try to do these every Tuesday, probably
------
claude's been the best speaker by far, but I gotta snooze
------
This is not really what it looks like and there is an important & nuanced backstory here — I made a video to explain: https://youtu.be/dQw4w9WgXcQ
------
Going through the YC directory for some inspiration, and no way @Replit did this...
------
This is a sin
------
had dinner with two google employees and they got into a bragging competition about who works fewer hours, the guy @ 2 hrs/day for 500k tc won
------
good morning let's make our beds
------
Retentive Network: A Successor to Transformer
for Large Language Models

Proposes RetNet as a foundation architecture for LLMs, simultaneously achieving training parallelism, low-cost inference, and good performance.

https://arxiv.org/abs/2307.08621
------
I love going to google images, searching for images of jesus for some midnight inspiration, and then getting nothing but ads for shitty printed art
------
a fly has somehow got into my computer. should i kick off a training run?
------
khajiit has gpt wrapper
if you have coin
------
LESS DATA
look
at
your data
50k samples? easily doable in a week
------
AlpaGasus: Training A Better Alpaca with Fewer Data

Significantly outperforms the original Alpaca and reaches 90% of davinci-003 w/ 5.7x faster training.

proj: https://lichang-chen.github.io/AlpaGasus/
abs: https://arxiv.org/abs/2307.08701
------
look!!!!!!!!!! look with your eyes
------
broke: prompt engineering
woke: parameter efficient fine tuning (better and cheaper)
here is a paper that confirms my bias
https://arxiv.org/abs/2205.05638
------
paper is 2022 btw (10 years old in ML time)
also get your loop down this is a PSA
get your training loop down 
get your labeling loop down
get it down
LoRA on 7b params takes 30 minutes
------
YES!!!! We are all going to make it!!! YES!!!!!!!!!!!
------
yessssssssssssssssss!!!!!!!!!!!!!!!!!!!!!!!!!!
------
expectation: kache steals OC from 4chan
reality: i am the OC on 4chan
------
"yeah coding is basically like blue collar work"
------
Government dooms citizenry to toothpaste and detergent ads and small businesses to bankruptcy.
------
It’s wild it’s considered a bad thing that small businesses should be able to find targeted customers for their products
------
you didn’t earn the right to be annoying on the internet. yann lecun has. if he wants to claim his dog is smarter than GPT-4 or say autoregressive models are fundamentally flawed - that’s totally fine with me - he’s done enough for the field shilling neural networks since the 80s
------
NOOOOOOOOOOOOOOOOOOOOO I DONT HAVE THE RUNWAY FOR THIS NOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO
------
i would like 24 thousand dollars in free GPT4 tokens, please
------
consider it done
------
Apologize to him right now
------
This is huge: Llama-v2 is open source, with a license that authorizes commercial use!

This is going to change the landscape of the LLM market.
Llama-v2 is available on Microsoft Azure and will be available on AWS, Hugging Face and other providers

Pretrained and fine-tuned… Show more
------
"hey yacine, hope you're having a good morning. I'd just like to call and thank you for making that meme of me talking about open source yesterday. yeah it like, changed my mind and i decided to make it open source."
------
Really great twitter space, thanks for helping me kill time while we waited for llama2 to come out! 

I'll try to do these every Tuesday, probably
------
claude's been the best speaker by far, but I gotta snooze
------
This is not really what it looks like and there is an important & nuanced backstory here — I made a video to explain: https://youtu.be/dQw4w9WgXcQ
------
Going through the YC directory for some inspiration, and no way @Replit did this...
------
This is a sin
------
had dinner with two google employees and they got into a bragging competition about who works fewer hours, the guy @ 2 hrs/day for 500k tc won
------
good morning let's make our beds
------
Retentive Network: A Successor to Transformer
for Large Language Models

Proposes RetNet as a foundation architecture for LLMs, simultaneously achieving training parallelism, low-cost inference, and good performance.

https://arxiv.org/abs/2307.08621
------
I love going to google images, searching for images of jesus for some midnight inspiration, and then getting nothing but ads for shitty printed art
------
a fly has somehow got into my computer. should i kick off a training run?
------
khajiit has gpt wrapper
if you have coin
------
LESS DATA
look
at
your data
50k samples? easily doable in a week
------
AlpaGasus: Training A Better Alpaca with Fewer Data

Significantly outperforms the original Alpaca and reaches 90% of davinci-003 w/ 5.7x faster training.

proj: https://lichang-chen.github.io/AlpaGasus/
abs: https://arxiv.org/abs/2307.08701
------
look!!!!!!!!!! look with your eyes
------
broke: prompt engineering
woke: parameter efficient fine tuning (better and cheaper)
here is a paper that confirms my bias
https://arxiv.org/abs/2205.05638
------
paper is 2022 btw (10 years old in ML time)
also get your loop down this is a PSA
get your training loop down 
get your labeling loop down
get it down
LoRA on 7b params takes 30 minutes
------
YES!!!! We are all going to make it!!! YES!!!!!!!!!!!
------
yessssssssssssssssss!!!!!!!!!!!!!!!!!!!!!!!!!!
------
expectation: kache steals OC from 4chan
reality: i am the OC on 4chan
------
"yeah coding is basically like blue collar work"
------
Government dooms citizenry to toothpaste and detergent ads and small businesses to bankruptcy.
------
It’s wild it’s considered a bad thing that small businesses should be able to find targeted customers for their products
------
you didn’t earn the right to be annoying on the internet. yann lecun has. if he wants to claim his dog is smarter than GPT-4 or say autoregressive models are fundamentally flawed - that’s totally fine with me - he’s done enough for the field shilling neural networks since the 80s
------
NOOOOOOOOOOOOOOOOOOOOO I DONT HAVE THE RUNWAY FOR THIS NOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO
------
i would like 24 thousand dollars in free GPT4 tokens, please
------
consider it done
------
okay guys
hear me out
minecraft redstone sorting systems
but in real life
------
3D printed conveyer belts, 3D printed shulker boxes and computer vision raspberry pis 
that's all you need, right?
------
This is not really what it looks like and there is an important & nuanced backstory here — I made a video to explain: https://youtu.be/dQw4w9WgXcQ
------
Going through the YC directory for some inspiration, and no way @Replit did this...
------
This is a sin
------
had dinner with two google employees and they got into a bragging competition about who works fewer hours, the guy @ 2 hrs/day for 500k tc won
------
good morning let's make our beds
------
Retentive Network: A Successor to Transformer
for Large Language Models

Proposes RetNet as a foundation architecture for LLMs, simultaneously achieving training parallelism, low-cost inference, and good performance.

https://arxiv.org/abs/2307.08621
------
I love going to google images, searching for images of jesus for some midnight inspiration, and then getting nothing but ads for shitty printed art
------
a fly has somehow got into my computer. should i kick off a training run?
------
khajiit has gpt wrapper
if you have coin
------
LESS DATA
look
at
your data
50k samples? easily doable in a week
------
AlpaGasus: Training A Better Alpaca with Fewer Data

Significantly outperforms the original Alpaca and reaches 90% of davinci-003 w/ 5.7x faster training.

proj: https://lichang-chen.github.io/AlpaGasus/
abs: https://arxiv.org/abs/2307.08701
------
look!!!!!!!!!! look with your eyes
------
broke: prompt engineering
woke: parameter efficient fine tuning (better and cheaper)
here is a paper that confirms my bias
https://arxiv.org/abs/2205.05638
------
paper is 2022 btw (10 years old in ML time)
also get your loop down this is a PSA
get your training loop down 
get your labeling loop down
get it down
LoRA on 7b params takes 30 minutes
------
YES!!!! We are all going to make it!!! YES!!!!!!!!!!!
------
yessssssssssssssssss!!!!!!!!!!!!!!!!!!!!!!!!!!
------
expectation: kache steals OC from 4chan
reality: i am the OC on 4chan
------
"yeah coding is basically like blue collar work"
------
Government dooms citizenry to toothpaste and detergent ads and small businesses to bankruptcy.
------
It’s wild it’s considered a bad thing that small businesses should be able to find targeted customers for their products
------
you didn’t earn the right to be annoying on the internet. yann lecun has. if he wants to claim his dog is smarter than GPT-4 or say autoregressive models are fundamentally flawed - that’s totally fine with me - he’s done enough for the field shilling neural networks since the 80s
------
NOOOOOOOOOOOOOOOOOOOOO I DONT HAVE THE RUNWAY FOR THIS NOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO
------
i would like 24 thousand dollars in free GPT4 tokens, please
------
consider it done
------
okay guys
hear me out
minecraft redstone sorting systems
but in real life
------
3D printed conveyer belts, 3D printed shulker boxes and computer vision raspberry pis 
that's all you need, right?
------
with zero++, ReLoRA, Flash Attn 2, and other advances, I'm feeling optimistic for the future of training on low-vram low-bandwidth consumer hardware.
------
DeepSpeed v0.10.0 release! Includes our ZeRO++ release, H100 support, and many bug fixes/updates. Special thanks to our wonderful community of contributors!

ZeRO++ paper: https://arxiv.org/pdf/2306.10209.pdf…

ZeRO++ blog: https://microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/… 

v0.10.0 details: https://github.com/microsoft/DeepSpeed/releases/tag/v0.10.0…
------
The new fourth edition of Linear Algebra Done Right will not have an abridged version because the entire electronic version will be legally free to the world. Still on schedule for publication in late November 2023.
------
if your software can't fit into a gpt4 context window to get rewritten you are 100% not going to make it, i am sorry to say
------
Embrace the cron, .sh files are good. Beware of the premature docker compose yamls, these are sirens that have taken down many a ship that passed their shores
------
muh class
muh clean principled abstraction
hahahahahaahahahahahahahah
that shit'll leak faster than you can say OOPs
literally
delete all of your code
it is harmful
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
khajiit has gpt wrapper
if you have coin
------
LESS DATA
look
at
your data
50k samples? easily doable in a week
------
AlpaGasus: Training A Better Alpaca with Fewer Data

Significantly outperforms the original Alpaca and reaches 90% of davinci-003 w/ 5.7x faster training.

proj: https://lichang-chen.github.io/AlpaGasus/
abs: https://arxiv.org/abs/2307.08701
------
look!!!!!!!!!! look with your eyes
------
broke: prompt engineering
woke: parameter efficient fine tuning (better and cheaper)
here is a paper that confirms my bias
https://arxiv.org/abs/2205.05638
------
paper is 2022 btw (10 years old in ML time)
also get your loop down this is a PSA
get your training loop down 
get your labeling loop down
get it down
LoRA on 7b params takes 30 minutes
------
YES!!!! We are all going to make it!!! YES!!!!!!!!!!!
------
yessssssssssssssssss!!!!!!!!!!!!!!!!!!!!!!!!!!
------
expectation: kache steals OC from 4chan
reality: i am the OC on 4chan
------
"yeah coding is basically like blue collar work"
------
Government dooms citizenry to toothpaste and detergent ads and small businesses to bankruptcy.
------
It’s wild it’s considered a bad thing that small businesses should be able to find targeted customers for their products
------
you didn’t earn the right to be annoying on the internet. yann lecun has. if he wants to claim his dog is smarter than GPT-4 or say autoregressive models are fundamentally flawed - that’s totally fine with me - he’s done enough for the field shilling neural networks since the 80s
------
NOOOOOOOOOOOOOOOOOOOOO I DONT HAVE THE RUNWAY FOR THIS NOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO
------
i would like 24 thousand dollars in free GPT4 tokens, please
------
consider it done
------
okay guys
hear me out
minecraft redstone sorting systems
but in real life
------
3D printed conveyer belts, 3D printed shulker boxes and computer vision raspberry pis 
that's all you need, right?
------
with zero++, ReLoRA, Flash Attn 2, and other advances, I'm feeling optimistic for the future of training on low-vram low-bandwidth consumer hardware.
------
DeepSpeed v0.10.0 release! Includes our ZeRO++ release, H100 support, and many bug fixes/updates. Special thanks to our wonderful community of contributors!

ZeRO++ paper: https://arxiv.org/pdf/2306.10209.pdf…

ZeRO++ blog: https://microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/… 

v0.10.0 details: https://github.com/microsoft/DeepSpeed/releases/tag/v0.10.0…
------
The new fourth edition of Linear Algebra Done Right will not have an abridged version because the entire electronic version will be legally free to the world. Still on schedule for publication in late November 2023.
------
if your software can't fit into a gpt4 context window to get rewritten you are 100% not going to make it, i am sorry to say
------
Embrace the cron, .sh files are good. Beware of the premature docker compose yamls, these are sirens that have taken down many a ship that passed their shores
------
muh class
muh clean principled abstraction
hahahahahaahahahahahahahah
that shit'll leak faster than you can say OOPs
literally
delete all of your code
it is harmful
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
broke: prompt engineering
woke: parameter efficient fine tuning (better and cheaper)
here is a paper that confirms my bias
https://arxiv.org/abs/2205.05638
------
paper is 2022 btw (10 years old in ML time)
also get your loop down this is a PSA
get your training loop down 
get your labeling loop down
get it down
LoRA on 7b params takes 30 minutes
------
YES!!!! We are all going to make it!!! YES!!!!!!!!!!!
------
yessssssssssssssssss!!!!!!!!!!!!!!!!!!!!!!!!!!
------
expectation: kache steals OC from 4chan
reality: i am the OC on 4chan
------
"yeah coding is basically like blue collar work"
------
Government dooms citizenry to toothpaste and detergent ads and small businesses to bankruptcy.
------
It’s wild it’s considered a bad thing that small businesses should be able to find targeted customers for their products
------
you didn’t earn the right to be annoying on the internet. yann lecun has. if he wants to claim his dog is smarter than GPT-4 or say autoregressive models are fundamentally flawed - that’s totally fine with me - he’s done enough for the field shilling neural networks since the 80s
------
NOOOOOOOOOOOOOOOOOOOOO I DONT HAVE THE RUNWAY FOR THIS NOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO
------
i would like 24 thousand dollars in free GPT4 tokens, please
------
consider it done
------
okay guys
hear me out
minecraft redstone sorting systems
but in real life
------
3D printed conveyer belts, 3D printed shulker boxes and computer vision raspberry pis 
that's all you need, right?
------
with zero++, ReLoRA, Flash Attn 2, and other advances, I'm feeling optimistic for the future of training on low-vram low-bandwidth consumer hardware.
------
DeepSpeed v0.10.0 release! Includes our ZeRO++ release, H100 support, and many bug fixes/updates. Special thanks to our wonderful community of contributors!

ZeRO++ paper: https://arxiv.org/pdf/2306.10209.pdf…

ZeRO++ blog: https://microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/… 

v0.10.0 details: https://github.com/microsoft/DeepSpeed/releases/tag/v0.10.0…
------
The new fourth edition of Linear Algebra Done Right will not have an abridged version because the entire electronic version will be legally free to the world. Still on schedule for publication in late November 2023.
------
if your software can't fit into a gpt4 context window to get rewritten you are 100% not going to make it, i am sorry to say
------
Embrace the cron, .sh files are good. Beware of the premature docker compose yamls, these are sirens that have taken down many a ship that passed their shores
------
muh class
muh clean principled abstraction
hahahahahaahahahahahahahah
that shit'll leak faster than you can say OOPs
literally
delete all of your code
it is harmful
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
Government dooms citizenry to toothpaste and detergent ads and small businesses to bankruptcy.
------
It’s wild it’s considered a bad thing that small businesses should be able to find targeted customers for their products
------
you didn’t earn the right to be annoying on the internet. yann lecun has. if he wants to claim his dog is smarter than GPT-4 or say autoregressive models are fundamentally flawed - that’s totally fine with me - he’s done enough for the field shilling neural networks since the 80s
------
NOOOOOOOOOOOOOOOOOOOOO I DONT HAVE THE RUNWAY FOR THIS NOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO
------
i would like 24 thousand dollars in free GPT4 tokens, please
------
consider it done
------
okay guys
hear me out
minecraft redstone sorting systems
but in real life
------
3D printed conveyer belts, 3D printed shulker boxes and computer vision raspberry pis 
that's all you need, right?
------
with zero++, ReLoRA, Flash Attn 2, and other advances, I'm feeling optimistic for the future of training on low-vram low-bandwidth consumer hardware.
------
DeepSpeed v0.10.0 release! Includes our ZeRO++ release, H100 support, and many bug fixes/updates. Special thanks to our wonderful community of contributors!

ZeRO++ paper: https://arxiv.org/pdf/2306.10209.pdf…

ZeRO++ blog: https://microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/… 

v0.10.0 details: https://github.com/microsoft/DeepSpeed/releases/tag/v0.10.0…
------
The new fourth edition of Linear Algebra Done Right will not have an abridged version because the entire electronic version will be legally free to the world. Still on schedule for publication in late November 2023.
------
if your software can't fit into a gpt4 context window to get rewritten you are 100% not going to make it, i am sorry to say
------
Embrace the cron, .sh files are good. Beware of the premature docker compose yamls, these are sirens that have taken down many a ship that passed their shores
------
muh class
muh clean principled abstraction
hahahahahaahahahahahahahah
that shit'll leak faster than you can say OOPs
literally
delete all of your code
it is harmful
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
consider it done
------
okay guys
hear me out
minecraft redstone sorting systems
but in real life
------
3D printed conveyer belts, 3D printed shulker boxes and computer vision raspberry pis 
that's all you need, right?
------
with zero++, ReLoRA, Flash Attn 2, and other advances, I'm feeling optimistic for the future of training on low-vram low-bandwidth consumer hardware.
------
DeepSpeed v0.10.0 release! Includes our ZeRO++ release, H100 support, and many bug fixes/updates. Special thanks to our wonderful community of contributors!

ZeRO++ paper: https://arxiv.org/pdf/2306.10209.pdf…

ZeRO++ blog: https://microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/… 

v0.10.0 details: https://github.com/microsoft/DeepSpeed/releases/tag/v0.10.0…
------
The new fourth edition of Linear Algebra Done Right will not have an abridged version because the entire electronic version will be legally free to the world. Still on schedule for publication in late November 2023.
------
if your software can't fit into a gpt4 context window to get rewritten you are 100% not going to make it, i am sorry to say
------
Embrace the cron, .sh files are good. Beware of the premature docker compose yamls, these are sirens that have taken down many a ship that passed their shores
------
muh class
muh clean principled abstraction
hahahahahaahahahahahahahah
that shit'll leak faster than you can say OOPs
literally
delete all of your code
it is harmful
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul
------
"look i avoided code duplication!!!"
------
Announcing FlashAttention-2! We released FlashAttention a year ago, making attn 2-4 faster and is now widely used in most LLM libraries. Recently I’ve been working on the next version: 2x faster than v1, 5-9x vs standard attn, reaching 225 TFLOPs/s training speed on A100. 1/
------
so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it
------
someone came into the interview and they didn't have dead skin and grease on where their palm would be resting on their macbook if they were working... big red flag tbh
------
"so like, open source is just better. you know? Like, why fight against all of the creativity of the internet? yeah man, like, we can just like work together with everyone. its probably safer too, dude"
------
amusingly anthropic's models are one of the most popular for AIGF sims
------
it’s *so* over
------
my least libertarian view
------
I am NOT saying you can't do whatever you want with your your models, chooms

I am saying we should probably stop megacorporations from braindancing our kids with game theoretic optimized miku mikus
------
owo whats this
------
good morning
it's a new week
------
many don't know this but automatic1111 has an entire API you can reverse engineer
------
and by reverse engineer i mean all you have to do is look at swagger and attach a debugger to figure out what the API really is

and yes, even controlnet works
------
Plinz is the most misunderstood heroes of our generation
------
morality redditors simply cannot understand his pure, unsullied childlike soul